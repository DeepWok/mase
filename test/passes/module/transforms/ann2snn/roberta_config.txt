=== Model Configuration ===
RobertaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "XianYiyk/roberta-relu-pretrained-sst2",
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "relu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "problem_type": "single_label_classification",
  "torch_dtype": "float32",
  "transformers_version": "4.48.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}


=== Model Architecture ===
RobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0-11): 12 x RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSdpaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): ReLU()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=2, bias=True)
  )
)

=== Model Parameters ===
roberta.embeddings.word_embeddings.weight: torch.Size([50265, 768])
roberta.embeddings.position_embeddings.weight: torch.Size([514, 768])
roberta.embeddings.token_type_embeddings.weight: torch.Size([1, 768])
roberta.embeddings.LayerNorm.weight: torch.Size([768])
roberta.embeddings.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.0.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.0.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.0.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.0.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.0.output.dense.bias: torch.Size([768])
roberta.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.1.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.1.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.1.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.1.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.1.output.dense.bias: torch.Size([768])
roberta.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.2.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.2.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.2.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.2.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.2.output.dense.bias: torch.Size([768])
roberta.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.3.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.3.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.3.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.3.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.3.output.dense.bias: torch.Size([768])
roberta.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.4.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.4.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.4.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.4.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.4.output.dense.bias: torch.Size([768])
roberta.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.5.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.5.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.5.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.5.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.5.output.dense.bias: torch.Size([768])
roberta.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.6.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.6.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.6.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.6.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.6.output.dense.bias: torch.Size([768])
roberta.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.7.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.7.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.7.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.7.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.7.output.dense.bias: torch.Size([768])
roberta.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.8.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.8.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.8.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.8.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.8.output.dense.bias: torch.Size([768])
roberta.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.9.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.9.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.9.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.9.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.9.output.dense.bias: torch.Size([768])
roberta.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.10.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.10.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.10.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.10.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.10.output.dense.bias: torch.Size([768])
roberta.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])
roberta.encoder.layer.11.attention.self.query.bias: torch.Size([768])
roberta.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])
roberta.encoder.layer.11.attention.self.key.bias: torch.Size([768])
roberta.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])
roberta.encoder.layer.11.attention.self.value.bias: torch.Size([768])
roberta.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])
roberta.encoder.layer.11.attention.output.dense.bias: torch.Size([768])
roberta.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])
roberta.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])
roberta.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])
roberta.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])
roberta.encoder.layer.11.output.dense.bias: torch.Size([768])
roberta.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])
roberta.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])
classifier.dense.weight: torch.Size([768, 768])
classifier.dense.bias: torch.Size([768])
classifier.out_proj.weight: torch.Size([2, 768])
classifier.out_proj.bias: torch.Size([2])
