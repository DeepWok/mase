# model = "toy"
# dataset = "toy-tiny"
accelerator = "cpu"
[passes.quantize]
by = "name"
report = true

# baseline_weight_path = "../mase_output/jsc-s_classification_jsc_2023-10-03/software/transform/transformed_ckpt_bl/transformed_ckpt/graph_module.mz" # This is the baseline model which we use to find out if there is any activation functions followed by targeted layer
baseline_weight_path = "../mase_output/jsc-tiny_classification_jsc_2023-10-04/software/transform/transformed_ckpt_bl/transformed_ckpt/graph_module.mz" # This is the baseline model which we use to find out if there is any activation functions followed by targeted layer
# baseline_weight_path = "/workspace/mase_output/jsc-s_classification_jsc_2023-09-25/software/transform/transformed_ckpt_bl/transformed_ckpt/graph_module.mz" # This is the baseline model which we use to find out if there is any activation functions followed by targeted layer
load_type = "mz"

[passes.quantize.default.config]
name = "NA"

# [passes.quantize.seq_blocks_4.config]
# name = "integer"
# data_in_width = 2
# data_in_frac_width = 1

[passes.quantize.seq_blocks_2.config]
name = "logicnets"
data_in_width = 2
data_in_frac_width = 1 # This will be the output bit of the followed activation function
data_out_width = 2 # This will be the output bit of the followed activation function
data_out_frac_width = 1
weight_width = "NA"
weight_frac_width = "NA"
bias_width = "NA"
bias_frac_width = "NA"
additional_layers_inputs = ["seq_blocks_0", "seq_blocks_1"] # merge these preceding layers into the same LogicNets layer
additional_layers_outputs = ["seq_blocks_3", "seq_blocks_4"] # merge these succeeding layers into the same LogicNets layer
# seq_blocks_0: BatchNorm1d
# seq_blocks_1: ReLU
# seq_blocks_2: Linear
# seq_blocks_3: BatchNorm1d
# seq_blocks_4: ReLU