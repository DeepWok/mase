# NOTE: Take a look at pruning/methods.py and pruning/criteria.py for available values
# This config shows an example weight pruning pass (L1 norm - 10% sparsity) with the
# activation pruning add-on (observe strategy). This strategy doesn't enforce activation
# sparsity; it reports the sparsity in activation induced by the sparsity in weights.
# Though, the presence of batch normalisation layers does hinder this.
# --------------------------------------------------------------------------------------
# Feel free to swap the model and dataset out to experiment :)
# Use this configuration with machop/test/passes/transforms/prune/prune.py
model = "resnet18"
dataset = "imagenet"

[passes.prune.weight]
method = "level-pruner"
criterion = "l1"
sparsity = 0.1

[passes.prune.activation]
strategy = "observe"
