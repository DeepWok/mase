{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "pip install --no-cache-dir --index-url https://pypi.nvidia.com pytorch-quantization !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    " ./ch transform --config configs/examples/toy_uniform_tensorRT.toml --load /root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-03/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch train --config configs/examples/toy_uniform_tensorRT.toml\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt update && sudo apt upgrade -y \n",
    "\n",
    "wget https://repo.continuum.io/archive/Anaconda3-2023.09-0-Linux-x86_64.sh \n",
    "\n",
    "bash Anaconda3-2023.09-0-Linux-x86_64.sh \n",
    "\n",
    "Close and reopen terminal\n",
    "\n",
    "source /root/.bashrc\n",
    "\n",
    "conda config --set auto_activate_base false\n",
    "\n",
    "git clone https://github.com/mau-mar/mase/\n",
    "\n",
    "cd mase\n",
    "\n",
    "bash scripts/init-conda.sh\n",
    "\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "\n",
    "conda activate mase\n",
    "\n",
    "conda config --add channels conda-forge\n",
    "\n",
    "git checkout origin/mauro-tensorRT-integration\n",
    "\n",
    "cuda-python\n",
    "absl-py\n",
    "scipy\n",
    "prettytable\n",
    "sphinx-glpi-theme\n",
    "\n",
    "./ch train --config configs/examples/toy_uniform_tensorRT.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-16 11:20:23,406] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 11:20:25.071521 140497372997440 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# # figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch_tensorrt\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "\n",
    "from chop.passes.graph import (\n",
    "    save_node_meta_param_interface_pass,\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    tensorrt_analysis_pass,\n",
    "    )\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\u001b[0m\n",
      "I0316 11:20:31.062528 140497372997440 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\n"
     ]
    }
   ],
   "source": [
    "# üìùÔ∏è change this CHECKPOINT_PATH to the one you trained in Lab1\n",
    "#CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "# CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-trt_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "# CHECKPOINT_PATH = \"/root/mase/TensorRTDev/vgg7-test-accu-0.9332.ckpt\"\n",
    "\n",
    "model_name = \"jsc-toy\"\n",
    "dataset_name = \"jsc\"\n",
    "\n",
    "# model_name = \"vgg7\"\n",
    "# dataset_name = \"cifar10\"\n",
    "\n",
    "max_epochs = 1\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "accelerator = \"gpu\"\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Args: {'model': 'jsc-toy', 'dataset': 'jsc', 'max_epochs': 10, 'batch_size': 256, 'learning_rate': 0.001, 'accelerator': 'gpu', 'passes': {'tensorrt_quantize': {'by': 'type', 'calibrator': ['percentile', 'mse', 'entropy'], 'num_calibration_batches': 10, 'percentiles': [99], 'report': True, 'default': {'config': {'quantize': True, 'calibrator': 'histogram', 'precision': 'FP16', 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}}, 'linear': {'config': {'calibrator': 'histogram', 'quantize': True, 'precision': 'FP16'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'epochs': 1}}, 'tensorrt_analysis': {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}}}\n",
      "TensorRT Quantize Config: {'by': 'type', 'calibrator': ['percentile', 'mse', 'entropy'], 'num_calibration_batches': 10, 'percentiles': [99], 'report': True, 'default': {'config': {'quantize': True, 'calibrator': 'histogram', 'precision': 'FP16', 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}}, 'linear': {'config': {'calibrator': 'histogram', 'quantize': True, 'precision': 'FP16'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'fine_tune': {'epochs': 1}}\n",
      "TensorRT Analysis Config: {'num_batches': 500, 'num_GPU_warmup_batches': 5, 'test': True}\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "\n",
    "# Path to your TOML file\n",
    "toml_file_path = '/root/mase/machop/configs/examples/jsc_trt_quantization.toml'\n",
    "# toml_file_path = '/root/mase/machop/configs/examples/vgg7_quantization.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(toml_file_path, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt_quantize' section and its children\n",
    "tensorrt_quantize_config = pass_args.get('passes', {}).get('tensorrt_quantize', {})\n",
    "# Extract the 'passes.tensorrt_fine_tune' section and its children\n",
    "tensorrt_train_config = pass_args.get('passes', {}).get('tensorrt_fine_tune', {})\n",
    "# Extract the 'passes.tensorrt_analysis' section and its children\n",
    "tensorrt_analysis_config = pass_args.get('passes', {}).get('tensorrt_analysis', {})\n",
    "\n",
    "# Print or return the extracted section\n",
    "print(\"Pass Args:\", pass_args)\n",
    "print(\"TensorRT Quantize Config:\", tensorrt_quantize_config)\n",
    "print(\"TensorRT Analysis Config:\", tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [tensorrt_quantize_config, tensorrt_train_config, tensorrt_analysis_config]\n",
    "for config in configs:\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0316 11:20:32.720956 140497372997440 utils.py:121] Applying fake quantization to PyTorch model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0316 11:20:34.096790 140497372997440 utils.py:137] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0316 11:20:34.099369 140497372997440 calibrate.py:63] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 11:20:34.104685 140497372997440 calibrate.py:73] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 11:20:34.106192 140497372997440 calibrate.py:73] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 11:20:34.107707 140497372997440 calibrate.py:73] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 11:20:34.109252 140497372997440 calibrate.py:73] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 11:20:34.110726 140497372997440 calibrate.py:73] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 11:20:34.114143 140497372997440 calibrate.py:73] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 11:20:35.518891 140497372997440 calibrate.py:89] Enabling Quantization and Disabling Calibration\n",
      "W0316 11:20:35.521587 140497372997440 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 11:20:35.522630 140497372997440 calibrate.py:89] Enabling Quantization and Disabling Calibration\n",
      "W0316 11:20:35.524272 140497372997440 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 11:20:35.525218 140497372997440 calibrate.py:89] Enabling Quantization and Disabling Calibration\n",
      "W0316 11:20:35.526888 140497372997440 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 11:20:35.527748 140497372997440 calibrate.py:89] Enabling Quantization and Disabling Calibration\n",
      "W0316 11:20:35.529389 140497372997440 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 11:20:35.530272 140497372997440 calibrate.py:89] Enabling Quantization and Disabling Calibration\n",
      "W0316 11:20:35.531834 140497372997440 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 11:20:35.532635 140497372997440 calibrate.py:89] Enabling Quantization and Disabling Calibration\n",
      "W0316 11:20:35.535462 140497372997440 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0316 11:20:35.539027 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 11:20:35.539773 140497372997440 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.9622 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:35.541192 140497372997440 calibrate.py:58] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.9622 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:35.543186 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:35.544036 140497372997440 calibrate.py:58] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:35.546260 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.7688 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:35.547351 140497372997440 calibrate.py:58] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.7688 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:35.549312 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:35.550145 140497372997440 calibrate.py:58] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:35.552307 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.1351 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:35.553367 140497372997440 calibrate.py:58] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.1351 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:35.555395 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:35.556425 140497372997440 calibrate.py:58] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:37.370743 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.6396 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:37.372225 140497372997440 calibrate.py:58] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.6396 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:39.147745 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:39.149273 140497372997440 calibrate.py:58] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:40.951640 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.9804 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:40.953457 140497372997440 calibrate.py:58] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.9804 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:42.736238 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:42.737799 140497372997440 calibrate.py:58] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:44.727879 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.1210 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:44.729384 140497372997440 calibrate.py:58] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.1210 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:46.531515 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:46.532798 140497372997440 calibrate.py:58] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:20:56.309772 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.2097 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:20:56.311349 140497372997440 calibrate.py:58] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.2097 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:21:03.340211 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:21:03.341887 140497372997440 calibrate.py:58] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:21:12.644536 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.6378 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:21:12.646140 140497372997440 calibrate.py:58] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.6378 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:21:19.437114 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:21:19.438566 140497372997440 calibrate.py:58] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:21:30.846262 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.1366 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:21:30.848879 140497372997440 calibrate.py:58] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.1366 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 11:21:37.465611 140497372997440 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 11:21:37.468061 140497372997440 calibrate.py:58] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0316 11:21:37.471837 140497372997440 calibrate.py:108] Succeeded in calibrating the model in PyTorch!\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "I0316 11:21:37.483541 140497372997440 quantize.py:129] Converting PyTorch model to ONNX...\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_16/version_3/model.onnx\u001b[0m\n",
      "I0316 11:21:37.854310 140497372997440 quantize.py:152] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_16/version_3/model.onnx\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "I0316 11:21:37.856722 140497372997440 quantize.py:55] Converting PyTorch model to TensorRT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-11:21:47] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_1/model.trt\u001b[0m\n",
      "I0316 11:23:44.385425 140497372997440 quantize.py:124] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_1/model.trt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/TensorRT/Quantization/JSON/2024_03_16/version_1/model.json\u001b[0m\n",
      "I0316 11:23:44.693626 140497372997440 quantize.py:168] TensorRT Model Summary Exported to /root/mase/mase_output/TensorRT/Quantization/JSON/2024_03_16/version_1/model.json\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-trt_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/vgg7-test-accu-0.9332.ckpt --load-type pl\n",
    "'''\n",
    "# Restart kernel and re-run previous cells to avoid any issues\n",
    "# mg_og = deepcopy_mase_graph(mg)\n",
    "mg_original = copy.deepcopy(mg.model)\n",
    "\n",
    "# Calibrate model (passing data samples to the quantizer and deciding the best amax for activations)\n",
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Conduct QAT\n",
    "# mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Convert and store to ONNX and then TensorRT\n",
    "mg, trt_meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\u001b[0m\n",
      "I0316 11:19:28.639614 140089548523328 analysis.py:128] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:19:28.641288 140089548523328 analysis.py:202] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-11:19:28] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73309    |\n",
      "|      Average Precision       |    0.74935    |\n",
      "|        Average Recall        |    0.73329    |\n",
      "|       Average F1 Score       |    0.73688    |\n",
      "|         Average Loss         |    0.7531     |\n",
      "|       Average Latency        |  0.18148 ms   |\n",
      "|   Average GPU Power Usage    |   64.879 W    |\n",
      "| Inference Energy Consumption | 0.0032707 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "I0316 11:19:32.303160 140089548523328 analysis.py:306] \n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73309    |\n",
      "|      Average Precision       |    0.74935    |\n",
      "|        Average Recall        |    0.73329    |\n",
      "|       Average F1 Score       |    0.73688    |\n",
      "|         Average Loss         |    0.7531     |\n",
      "|       Average Latency        |  0.18148 ms   |\n",
      "|   Average GPU Power Usage    |   64.879 W    |\n",
      "| Inference Energy Consumption | 0.0032707 mWh |\n",
      "+------------------------------+---------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:19:32.310961 140089548523328 analysis.py:202] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73168    |\n",
      "|      Average Precision       |   0.74472    |\n",
      "|        Average Recall        |   0.73037    |\n",
      "|       Average F1 Score       |   0.73369    |\n",
      "|         Average Loss         |   0.76315    |\n",
      "|       Average Latency        |  0.94611 ms  |\n",
      "|   Average GPU Power Usage    |   65.397 W   |\n",
      "| Inference Energy Consumption | 0.017187 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0316 11:19:36.270318 140089548523328 analysis.py:306] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73168    |\n",
      "|      Average Precision       |   0.74472    |\n",
      "|        Average Recall        |   0.73037    |\n",
      "|       Average F1 Score       |   0.73369    |\n",
      "|         Average Loss         |   0.76315    |\n",
      "|       Average Latency        |  0.94611 ms  |\n",
      "|   Average GPU Power Usage    |   65.397 W   |\n",
      "| Inference Energy Consumption | 0.017187 mWh |\n",
      "+------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "trt_meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy INT8 only quantization \n",
    "trt_meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_0/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(trt_meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\u001b[0m\n",
      "I0316 11:24:17.414859 140497372997440 analysis.py:128] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:24:17.417420 140497372997440 analysis.py:202] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-11:24:17] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73325    |\n",
      "|      Average Precision       |    0.74924    |\n",
      "|        Average Recall        |    0.73356    |\n",
      "|       Average F1 Score       |    0.73732    |\n",
      "|         Average Loss         |    0.75403    |\n",
      "|       Average Latency        |   0.2609 ms   |\n",
      "|   Average GPU Power Usage    |   68.244 W    |\n",
      "| Inference Energy Consumption | 0.0049457 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "I0316 11:24:23.903356 140497372997440 analysis.py:306] \n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73325    |\n",
      "|      Average Precision       |    0.74924    |\n",
      "|        Average Recall        |    0.73356    |\n",
      "|       Average F1 Score       |    0.73732    |\n",
      "|         Average Loss         |    0.75403    |\n",
      "|       Average Latency        |   0.2609 ms   |\n",
      "|   Average GPU Power Usage    |   68.244 W    |\n",
      "| Inference Energy Consumption | 0.0049457 mWh |\n",
      "+------------------------------+---------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:24:23.913443 140497372997440 analysis.py:202] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73071    |\n",
      "|      Average Precision       |   0.74392    |\n",
      "|        Average Recall        |   0.72935    |\n",
      "|       Average F1 Score       |   0.73267    |\n",
      "|         Average Loss         |   0.76629    |\n",
      "|       Average Latency        |  5.0403 ms   |\n",
      "|   Average GPU Power Usage    |   68.492 W   |\n",
      "| Inference Energy Consumption | 0.095894 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0316 11:24:32.237802 140497372997440 analysis.py:306] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73071    |\n",
      "|      Average Precision       |   0.74392    |\n",
      "|        Average Recall        |   0.72935    |\n",
      "|       Average F1 Score       |   0.73267    |\n",
      "|         Average Loss         |   0.76629    |\n",
      "|       Average Latency        |  5.0403 ms   |\n",
      "|   Average GPU Power Usage    |   68.492 W   |\n",
      "| Inference Energy Consumption | 0.095894 mWh |\n",
      "+------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "trt_meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy FP16 only quantization \n",
    "trt_meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_1/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(trt_meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
