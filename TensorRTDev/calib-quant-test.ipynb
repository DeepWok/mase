{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "pip install --no-cache-dir --index-url https://pypi.nvidia.com pytorch-quantization !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    " ./ch transform --config configs/examples/toy_uniform_tensorRT.toml --load /root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-03/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch train --config configs/examples/toy_uniform_tensorRT.toml\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt update && sudo apt upgrade -y \n",
    "\n",
    "wget https://repo.continuum.io/archive/Anaconda3-2023.09-0-Linux-x86_64.sh \n",
    "\n",
    "bash Anaconda3-2023.09-0-Linux-x86_64.sh \n",
    "\n",
    "Close and reopen terminal\n",
    "\n",
    "source /root/.bashrc\n",
    "\n",
    "conda config --set auto_activate_base false\n",
    "\n",
    "git clone https://github.com/mau-mar/mase/\n",
    "\n",
    "cd mase\n",
    "\n",
    "bash scripts/init-conda.sh\n",
    "\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "\n",
    "conda activate mase\n",
    "\n",
    "conda config --add channels conda-forge\n",
    "\n",
    "git checkout origin/mauro-tensorRT-integration\n",
    "\n",
    "cuda-python\n",
    "absl-py\n",
    "scipy\n",
    "prettytable\n",
    "sphinx-glpi-theme\n",
    "\n",
    "./ch train --config configs/examples/toy_uniform_tensorRT.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0311 18:40:58.892867 140358832170816 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# # figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch_tensorrt\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(pytorch_quantization.__version__)\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "\n",
    "from chop.passes.graph import (\n",
    "    save_node_meta_param_interface_pass,\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_train_transform_pass,\n",
    "    tensorrt_quantize_transform_pass,\n",
    "    tensorrt_analysis_pass,\n",
    "    )\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\u001b[0m\n",
      "I0311 18:41:03.255064 140358832170816 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jsc-tiny\"\n",
    "dataset_name = \"jsc\"\n",
    "max_epochs = 1\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "accelerator = \"gpu\"\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# üìùÔ∏è change this CHECKPOINT_PATH to the one you trained in Lab1\n",
    "CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "#CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-trt_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "#CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "\n",
    "for inputs in data_module.train_dataloader():\n",
    "    xs, ys = inputs\n",
    "    preds = mg.model(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT Quantize Config: {'by': 'type', 'calibrator': ['percentile', 'max', 'mse', 'entropy'], 'num_calibration_batches': 100, 'percentiles': [99], 'report': True, 'linear': {'config': {'quantize': True, 'precision': 'INT8'}, 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}, 'default': {'config': {'quantize': True, 'precision': 'INT8', 'input': {'calibrator': 'histogram', 'quantize_axis': False}, 'weight': {'calibrator': 'histogram', 'quantize_axis': False}}}}\n",
      "TensorRT Train Config: {'qat_epochs': 10}\n",
      "TensorRT Analysis Config: {'num_batches': 20, 'num_GPU_warmup_batches': 5}\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "\n",
    "# Path to your TOML file\n",
    "toml_file_path = '/root/mase/machop/configs/examples/jsc_trt_quantization.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(toml_file_path, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt-quantize' section and its children\n",
    "tensorrt_quantize_config = pass_args.get('passes', {}).get('tensorrt-quantize', {})\n",
    "# Extract the 'passes.tensorrt-train' section and its children\n",
    "tensorrt_train_config = pass_args.get('passes', {}).get('tensorrt-train', {})\n",
    "# Extract the 'passes.tensorrt-analysis' section and its children\n",
    "tensorrt_analysis_config = pass_args.get('passes', {}).get('tensorrt-analysis', {})\n",
    "\n",
    "# Print or return the extracted section\n",
    "print(\"TensorRT Quantize Config:\", tensorrt_quantize_config)\n",
    "print(\"TensorRT Train Config:\", tensorrt_train_config)\n",
    "print(\"TensorRT Analysis Config:\", tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [tensorrt_quantize_config, tensorrt_train_config, tensorrt_analysis_config]\n",
    "for config in configs:\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0311 18:42:01.280764 140358832170816 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0311 18:42:01.285147 140358832170816 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0311 18:42:01.286568 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0311 18:42:01.287702 140358832170816 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.289383 140358832170816 calibrate.py:61] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0311 18:42:01.293609 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([5, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.295704 140358832170816 calibrate.py:61] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0311 18:42:01.298268 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.299741 140358832170816 calibrate.py:61] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0311 18:42:01.301307 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([5, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.302799 140358832170816 calibrate.py:61] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0311 18:42:01.304735 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.306131 140358832170816 calibrate.py:61] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0311 18:42:01.307576 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([5, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.309064 140358832170816 calibrate.py:61] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0311 18:42:01.310940 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.312286 140358832170816 calibrate.py:61] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=34.9958 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "W0311 18:42:01.313749 140358832170816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([5, 1]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0311 18:42:01.315257 140358832170816 calibrate.py:61] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[1.1850, 5.9336](5) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0311 18:42:01.317052 140358832170816 calibrate.py:106] Succeeded in calibrating the model in PyTorch!\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "I0311 18:42:01.322010 140358832170816 quantize.py:123] Converting PyTorch model to ONNX...\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_11/version_6/model.onnx\u001b[0m\n",
      "I0311 18:42:01.469140 140358832170816 quantize.py:133] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_11/version_6/model.onnx\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "------------------------------------------------------------------------------------------------\n",
      "Layer Name               | Type         | Input Shape(s)                       | Output Shape(s)\n",
      "------------------------------------------------------------------------------------------------\u001b[0m\n",
      "I0311 18:42:01.473316 140358832170816 quantize.py:146] \n",
      "------------------------------------------------------------------------------------------------\n",
      "Layer Name               | Type         | Input Shape(s)                       | Output Shape(s)\n",
      "------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mIdentity_4                | Identity     | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.475640 140358832170816 quantize.py:165] Identity_4                | Identity     | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.0/BatchNormalization | BatchNormalization | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.477764 140358832170816 quantize.py:165] /seq_blocks.0/BatchNormalization | BatchNormalization | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.1/Relu        | Relu         | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.479723 140358832170816 quantize.py:165] /seq_blocks.1/Relu        | Relu         | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_input_quantizer/Mul | Mul          | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.481552 140358832170816 quantize.py:165] /seq_blocks.2/_input_quantizer/Mul | Mul          | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_input_quantizer/Round | Round        | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.483006 140358832170816 quantize.py:165] /seq_blocks.2/_input_quantizer/Round | Round        | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_input_quantizer/Constant | Constant     | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.484434 140358832170816 quantize.py:165] /seq_blocks.2/_input_quantizer/Constant | Constant     | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_input_quantizer/Constant_1 | Constant     | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.485859 140358832170816 quantize.py:165] /seq_blocks.2/_input_quantizer/Constant_1 | Constant     | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_input_quantizer/Clip | Clip         | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.487223 140358832170816 quantize.py:165] /seq_blocks.2/_input_quantizer/Clip | Clip         | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_input_quantizer/Div | Div          | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.488692 140358832170816 quantize.py:165] /seq_blocks.2/_input_quantizer/Div | Div          | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_weight_quantizer/Round | Round        | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.490150 140358832170816 quantize.py:165] /seq_blocks.2/_weight_quantizer/Round | Round        | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_weight_quantizer/Constant | Constant     | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.491665 140358832170816 quantize.py:165] /seq_blocks.2/_weight_quantizer/Constant | Constant     | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_weight_quantizer/Constant_1 | Constant     | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.493350 140358832170816 quantize.py:165] /seq_blocks.2/_weight_quantizer/Constant_1 | Constant     | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_weight_quantizer/Clip | Clip         | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.494935 140358832170816 quantize.py:165] /seq_blocks.2/_weight_quantizer/Clip | Clip         | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/_weight_quantizer/Div | Div          | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.496450 140358832170816 quantize.py:165] /seq_blocks.2/_weight_quantizer/Div | Div          | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.2/Gemm        | Gemm         | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.497994 140358832170816 quantize.py:165] /seq_blocks.2/Gemm        | Gemm         | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m/seq_blocks.3/Relu        | Relu         | Unknown                               | Unknown\u001b[0m\n",
      "I0311 18:42:01.499478 140358832170816 quantize.py:165] /seq_blocks.3/Relu        | Relu         | Unknown                               | Unknown\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m------------------------------------------------------------------------------------------------\u001b[0m\n",
      "I0311 18:42:01.500957 140358832170816 quantize.py:167] ------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "I0311 18:42:01.502584 140358832170816 quantize.py:73] Converting PyTorch model to TensorRT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/11/2024-18:42:09] [TRT] [E] 4: [standardEngineBuilder.cpp::initCalibrationParams::1714] Error Code 4: Internal Error (Calibration failure occurred with no scaling factors detected. This could be due to no int8 calibrator or insufficient custom scales for network layers. Please see int8 sample to setup calibration correctly.)\n",
      "[03/11/2024-18:42:58] [TRT] [E] 4: [standardEngineBuilder.cpp::initCalibrationParams::1714] Error Code 4: Internal Error (Calibration failure occurred with no scaling factors detected. This could be due to no int8 calibrator or insufficient custom scales for network layers. Please see int8 sample to setup calibration correctly.)\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-trt_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "'''\n",
    "# Restart kernel and re-run previous cells to avoid any issues\n",
    "# mg_og = deepcopy_mase_graph(mg)\n",
    "mg_original = copy.deepcopy(mg.model)\n",
    "\n",
    "# Calibrate model (passing data samples to the quantizer and deciding the best amax for activations)\n",
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# # Conduct QAT\n",
    "# mg, _ = tensorrt_train_transform_pass(mg, pass_args=tensorrt_train_config)\n",
    "\n",
    "# Convert and store to ONNX and then TensorRT\n",
    "mg, trt_meta = tensorrt_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Compare original tensorrt with quantized graph\n",
    "mg_original, _ = tensorrt_analysis_pass(mg_original, trt_meta['trt_graph_path'], pass_args=tensorrt_analysis_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
