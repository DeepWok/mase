{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "pip install --no-cache-dir --index-url https://pypi.nvidia.com pytorch-quantization !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    " ./ch transform --config configs/examples/toy_uniform_tensorRT.toml --load /root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-03/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch train --config configs/examples/toy_uniform_tensorRT.toml\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo apt update && sudo apt upgrade -y \n",
    "\n",
    "wget https://repo.continuum.io/archive/Anaconda3-2023.09-0-Linux-x86_64.sh \n",
    "\n",
    "bash Anaconda3-2023.09-0-Linux-x86_64.sh \n",
    "\n",
    "Close and reopen terminal\n",
    "\n",
    "source /root/.bashrc\n",
    "\n",
    "conda config --set auto_activate_base false\n",
    "\n",
    "git clone https://github.com/mau-mar/mase/\n",
    "\n",
    "cd mase\n",
    "\n",
    "bash scripts/init-conda.sh\n",
    "\n",
    "source /opt/conda/etc/profile.d/conda.sh\n",
    "\n",
    "conda activate mase\n",
    "\n",
    "conda config --add channels conda-forge\n",
    "\n",
    "git checkout origin/mauro-tensorRT-integration\n",
    "\n",
    "cuda-python\n",
    "absl-py\n",
    "scipy\n",
    "prettytable\n",
    "sphinx-glpi-theme\n",
    "\n",
    "./ch train --config configs/examples/toy_uniform_tensorRT.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-15 10:10:36,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0315 10:10:37.405604 140384837449536 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# # figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch_tensorrt\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "\n",
    "from chop.passes.graph import (\n",
    "    save_node_meta_param_interface_pass,\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    tensorrt_analysis_pass,\n",
    "    )\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/TensorRTDev/vgg7-test-accu-0.9332.ckpt\u001b[0m\n",
      "I0315 10:10:44.997772 140384837449536 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/TensorRTDev/vgg7-test-accu-0.9332.ckpt\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"jsc-toy\"\n",
    "# dataset_name = \"jsc\"\n",
    "\n",
    "model_name = \"vgg7\"\n",
    "dataset_name = \"cifar10\"\n",
    "\n",
    "max_epochs = 1\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "accelerator = \"gpu\"\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# üìùÔ∏è change this CHECKPOINT_PATH to the one you trained in Lab1\n",
    "#CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "# CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-trt_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "# CHECKPOINT_PATH = \"/root/mase/TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt\"\n",
    "CHECKPOINT_PATH = \"/root/mase/TensorRTDev/vgg7-test-accu-0.9332.ckpt\"\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Args: {'model': 'vgg7', 'dataset': 'cifar10', 'max_epochs': 10, 'batch_size': 256, 'learning_rate': 0.001, 'accelerator': 'gpu', 'passes': {'tensorrt_quantize': {'by': 'type', 'calibrator': ['percentile', 'mse', 'entropy'], 'num_calibration_batches': 10, 'percentiles': [99], 'report': True, 'linear': {'config': {'calibrator': 'histogram', 'quantize': True, 'precision': 'INT8'}, 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}, 'conv2d': {'config': {'quantize': True, 'calibrator': 'histogram', 'precision': 'INT8'}, 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}, 'default': {'config': {'quantize': True, 'calibrator': 'histogram', 'precision': 'FP16', 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}}}, 'tensorrt_fine_tune': {'epochs': 10}, 'tensorrt_analysis': {'num_batches': 20, 'num_GPU_warmup_batches': 5}}}\n",
      "TensorRT Quantize Config: {'by': 'type', 'calibrator': ['percentile', 'mse', 'entropy'], 'num_calibration_batches': 10, 'percentiles': [99], 'report': True, 'linear': {'config': {'calibrator': 'histogram', 'quantize': True, 'precision': 'INT8'}, 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}, 'conv2d': {'config': {'quantize': True, 'calibrator': 'histogram', 'precision': 'INT8'}, 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}, 'default': {'config': {'quantize': True, 'calibrator': 'histogram', 'precision': 'FP16', 'input': {'quantize_axis': False}, 'weight': {'quantize_axis': False}}}}\n",
      "TensorRT Analysis Config: {'num_batches': 20, 'num_GPU_warmup_batches': 5}\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "\n",
    "# Path to your TOML file\n",
    "# toml_file_path = '/root/mase/machop/configs/examples/jsc_trt_quantization.toml'\n",
    "toml_file_path = '/root/mase/machop/configs/examples/vgg7_quantization.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(toml_file_path, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt_quantize' section and its children\n",
    "tensorrt_quantize_config = pass_args.get('passes', {}).get('tensorrt_quantize', {})\n",
    "# Extract the 'passes.tensorrt_fine_tune' section and its children\n",
    "tensorrt_train_config = pass_args.get('passes', {}).get('tensorrt_fine_tune', {})\n",
    "# Extract the 'passes.tensorrt_analysis' section and its children\n",
    "tensorrt_analysis_config = pass_args.get('passes', {}).get('tensorrt_analysis', {})\n",
    "\n",
    "# Print or return the extracted section\n",
    "print(\"Pass Args:\", pass_args)\n",
    "print(\"TensorRT Quantize Config:\", tensorrt_quantize_config)\n",
    "print(\"TensorRT Analysis Config:\", tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [tensorrt_quantize_config, tensorrt_train_config, tensorrt_analysis_config]\n",
    "for config in configs:\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0315 08:52:26.992135 139664147523392 utils.py:121] Applying fake quantization to PyTorch model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0315 08:52:29.252809 139664147523392 utils.py:137] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0315 08:52:29.255288 139664147523392 calibrate.py:65] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.287572 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.289891 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.291899 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.293761 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.295625 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.297476 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.299286 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.300956 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.302420 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.303754 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.305137 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.306592 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.311627 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.313451 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.315245 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.317046 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.318865 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0315 08:52:29.320397 139664147523392 calibrate.py:75] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.080239 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.082374 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.083441 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.085229 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.086187 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.087911 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.088863 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.090606 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.091571 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.093146 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.096615 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.098357 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.099439 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.101451 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.102664 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.104592 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.105659 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.107496 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.108510 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.110452 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.111555 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.113099 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.117205 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.119138 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.120398 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.122568 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.123655 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.125771 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.126886 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.128778 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.129601 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.131062 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.131891 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.133316 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0315 08:52:36.134087 139664147523392 calibrate.py:91] Enabling Quantization and Disabling Calibration\n",
      "W0315 08:52:36.135504 139664147523392 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0315 08:52:36.140451 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0315 08:52:36.141266 139664147523392 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.0624 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.142422 139664147523392 calibrate.py:60] feature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.0624 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.144780 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5696 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.145663 139664147523392 calibrate.py:60] feature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5696 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.147846 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=8.7983 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.148727 139664147523392 calibrate.py:60] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=8.7983 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.150839 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.4801 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.151766 139664147523392 calibrate.py:60] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.4801 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.154020 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=4.6560 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.154846 139664147523392 calibrate.py:60] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=4.6560 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.157065 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5323 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.157893 139664147523392 calibrate.py:60] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5323 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.160022 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=3.3965 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.161033 139664147523392 calibrate.py:60] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=3.3965 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.162972 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3551 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.163970 139664147523392 calibrate.py:60] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3551 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.166016 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=4.1422 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.166849 139664147523392 calibrate.py:60] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=4.1422 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.168997 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3452 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.170005 139664147523392 calibrate.py:60] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3452 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.172003 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=3.9525 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.172869 139664147523392 calibrate.py:60] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=3.9525 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.174992 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3140 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.176045 139664147523392 calibrate.py:60] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3140 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.178105 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.1259 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.179006 139664147523392 calibrate.py:60] classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.1259 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.181140 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.2223 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.182157 139664147523392 calibrate.py:60] classifier.0._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.2223 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.184163 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=78.5993 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.185032 139664147523392 calibrate.py:60] classifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=78.5993 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.187196 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1688 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.188179 139664147523392 calibrate.py:60] classifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1688 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.190380 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=34.3903 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.191408 139664147523392 calibrate.py:60] last_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=34.3903 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:36.193428 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.2025 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:36.194286 139664147523392 calibrate.py:60] last_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.2025 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:38.003159 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.3190 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:38.004971 139664147523392 calibrate.py:60] feature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.3190 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:39.588587 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5586 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:39.590046 139664147523392 calibrate.py:60] feature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5586 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:41.533700 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=10.4790 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:41.535232 139664147523392 calibrate.py:60] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=10.4790 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:43.121098 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.4989 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:43.122662 139664147523392 calibrate.py:60] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.4989 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:45.287459 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.2789 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:45.289045 139664147523392 calibrate.py:60] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.2789 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:46.932683 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5739 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:46.934411 139664147523392 calibrate.py:60] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5739 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:48.597523 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=4.2953 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:48.599134 139664147523392 calibrate.py:60] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=4.2953 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:50.261029 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3685 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:50.262633 139664147523392 calibrate.py:60] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3685 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:52.329373 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.0009 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:52.332522 139664147523392 calibrate.py:60] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.0009 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:53.998123 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3645 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:53.999744 139664147523392 calibrate.py:60] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3645 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:56.153714 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.7020 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:56.155523 139664147523392 calibrate.py:60] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.7020 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:52:57.873276 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3202 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:52:57.874880 139664147523392 calibrate.py:60] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3202 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:00.212634 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=7.7645 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:00.214451 139664147523392 calibrate.py:60] classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=7.7645 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:01.792312 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.2408 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:01.793866 139664147523392 calibrate.py:60] classifier.0._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.2408 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:03.693900 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=160.2817 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:03.695393 139664147523392 calibrate.py:60] classifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=160.2817 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:05.297776 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1952 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:05.299376 139664147523392 calibrate.py:60] classifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1952 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:07.187567 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=61.8686 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:07.189118 139664147523392 calibrate.py:60] last_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=61.8686 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:08.774180 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1997 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:08.775650 139664147523392 calibrate.py:60] last_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.1997 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:19.682379 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=2.1282 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:19.685782 139664147523392 calibrate.py:60] feature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=2.1282 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:27.927657 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3306 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:27.929639 139664147523392 calibrate.py:60] feature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3306 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:40.154656 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=10.6742 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:40.156410 139664147523392 calibrate.py:60] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=10.6742 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:53:48.819084 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5427 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:53:48.822748 139664147523392 calibrate.py:60] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.5427 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:54:03.401699 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.4447 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:54:03.403876 139664147523392 calibrate.py:60] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=5.4447 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:54:11.990021 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.6185 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:54:11.991894 139664147523392 calibrate.py:60] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.6185 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:54:20.772558 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=4.6738 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:54:20.774534 139664147523392 calibrate.py:60] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=4.6738 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:54:29.615309 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.4083 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:54:29.618677 139664147523392 calibrate.py:60] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.4083 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:54:42.215387 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.9252 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:54:42.217224 139664147523392 calibrate.py:60] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.9252 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:54:50.876409 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.4537 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:54:50.878085 139664147523392 calibrate.py:60] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.4537 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:55:04.047700 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.8929 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:55:04.051182 139664147523392 calibrate.py:60] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=5.8929 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:55:12.948154 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3877 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:55:12.950430 139664147523392 calibrate.py:60] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.3877 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:55:28.286456 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=8.3021 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:55:28.288418 139664147523392 calibrate.py:60] classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=8.3021 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:55:36.897752 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.0._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.2154 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:55:36.899338 139664147523392 calibrate.py:60] classifier.0._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.2154 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:55:47.380412 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=175.7752 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:55:47.382139 139664147523392 calibrate.py:60] classifier.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=175.7752 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:55:55.505360 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mclassifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1901 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:55:55.508341 139664147523392 calibrate.py:60] classifier.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.1901 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:56:06.308135 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=66.1210 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:56:06.309901 139664147523392 calibrate.py:60] last_layer._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=66.1210 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0315 08:56:14.476118 139664147523392 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mlast_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.2112 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0315 08:56:14.477689 139664147523392 calibrate.py:60] last_layer._weight_quantizer            : TensorQuantizer(8bit fake per-tensor amax=0.2112 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0315 08:56:14.482332 139664147523392 calibrate.py:110] Succeeded in calibrating the model in PyTorch!\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "I0315 08:56:14.493909 139664147523392 quantize.py:129] Converting PyTorch model to ONNX...\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_15/version_0/model.onnx\u001b[0m\n",
      "I0315 08:56:15.997697 139664147523392 quantize.py:146] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_15/version_0/model.onnx\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "I0315 08:56:16.002248 139664147523392 quantize.py:55] Converting PyTorch model to TensorRT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/15/2024-08:56:23] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[03/15/2024-08:56:24] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[03/15/2024-09:06:13] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[03/15/2024-09:06:13] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[03/15/2024-09:06:13] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[03/15/2024-09:06:13] [TRT] [W] - 2 weights are affected by this issue: Detected subnormal FP16 values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_15/version_0/model.trt\u001b[0m\n",
      "I0315 09:09:17.250436 139664147523392 quantize.py:124] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_15/version_0/model.trt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/TensorRT/Quantization/JSON/2024_03_15/version_0/model.json\u001b[0m\n",
      "I0315 09:09:17.837146 139664147523392 quantize.py:162] TensorRT Model Summary Exported to /root/mase/mase_output/TensorRT/Quantization/JSON/2024_03_15/version_0/model.json\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 3, 32, 32)       | (256, 3, 32, 32)       | input\n",
      "1     | Output  | FLOAT    | (256, 10)              | (256, 10)              | 292\u001b[0m\n",
      "I0315 09:09:17.925912 139664147523392 analysis.py:109] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 3, 32, 32)       | (256, 3, 32, 32)       | input\n",
      "1     | Output  | FLOAT    | (256, 10)              | (256, 10)              | 292\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0315 09:09:17.928756 139664147523392 analysis.py:179] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/15/2024-09:09:17] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m mg, trt_meta \u001b[38;5;241m=\u001b[39m tensorrt_engine_interface_pass(mg, pass_args\u001b[38;5;241m=\u001b[39mtensorrt_quantize_config)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Compare original tensorrt with quantized graph\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtensorrt_analysis_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrt_meta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgraph_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorrt_analysis_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mase/machop/chop/passes/graph/transforms/tensorrt/quantize/analysis.py:49\u001b[0m, in \u001b[0;36mtensorrt_analysis_pass\u001b[0;34m(model, pass_args)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensorrt_analysis_pass\u001b[39m(model, pass_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     48\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m QuantizationAnalysis(model, pass_args)\n\u001b[0;32m---> 49\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, results\n",
      "File \u001b[0;32m~/mase/machop/chop/passes/graph/transforms/tensorrt/quantize/analysis.py:210\u001b[0m, in \u001b[0;36mQuantizationAnalysis.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, trt\u001b[38;5;241m.\u001b[39mIExecutionContext):\n\u001b[0;32m--> 210\u001b[0m     preds, latency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_trt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     preds, latency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_mg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, xs)  \u001b[38;5;66;03m# Run model prediction\u001b[39;00m\n",
      "File \u001b[0;32m~/mase/machop/chop/passes/graph/transforms/tensorrt/quantize/analysis.py:159\u001b[0m, in \u001b[0;36mQuantizationAnalysis.infer_trt\u001b[0;34m(self, trt_context, input_data)\u001b[0m\n\u001b[1;32m    156\u001b[0m end\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Calculate latency between start and end events\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m latency \u001b[38;5;241m=\u001b[39m \u001b[43mstart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Copying data from device to host and collecting output tensors\u001b[39;00m\n\u001b[1;32m    162\u001b[0m output_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    163\u001b[0m     bufferH[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_Input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_io)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m [cudart\u001b[38;5;241m.\u001b[39mcudaMemcpy(bufferH[i]\u001b[38;5;241m.\u001b[39mctypes\u001b[38;5;241m.\u001b[39mdata, bufferD[i], bufferH[i]\u001b[38;5;241m.\u001b[39mnbytes, cudart\u001b[38;5;241m.\u001b[39mcudaMemcpyKind\u001b[38;5;241m.\u001b[39mcudaMemcpyDeviceToHost)]\n\u001b[1;32m    165\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/cuda/streams.py:213\u001b[0m, in \u001b[0;36mEvent.elapsed_time\u001b[0;34m(self, end_event)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melapsed_time\u001b[39m(\u001b[38;5;28mself\u001b[39m, end_event):\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the time elapsed.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Time reported in milliseconds after the event was recorded and\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    before the end_event was recorded.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_event\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-trt_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/vgg7-test-accu-0.9332.ckpt --load-type pl\n",
    "'''\n",
    "# Restart kernel and re-run previous cells to avoid any issues\n",
    "# mg_og = deepcopy_mase_graph(mg)\n",
    "mg_original = copy.deepcopy(mg.model)\n",
    "\n",
    "# Calibrate model (passing data samples to the quantizer and deciding the best amax for activations)\n",
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Conduct QAT\n",
    "# mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Convert and store to ONNX and then TensorRT\n",
    "mg, trt_meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 3, 32, 32)       | (256, 3, 32, 32)       | input\n",
      "1     | Output  | FLOAT    | (256, 10)              | (256, 10)              | 292\u001b[0m\n",
      "I0315 10:10:53.448795 140384837449536 analysis.py:110] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 3, 32, 32)       | (256, 3, 32, 32)       | input\n",
      "1     | Output  | FLOAT    | (256, 10)              | (256, 10)              | 292\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0315 10:10:53.456407 140384837449536 analysis.py:184] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/15/2024-10:10:53] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Configuration vgg7-quantized:\n",
      "Metric                                  | Value\n",
      "----------------------------------------|-----------------------\n",
      "Average Validation Accuracy             | 0.93011\n",
      "Average Precision                       | 0.93088\n",
      "Average Recall                          | 0.93073\n",
      "Average F1 Score                        | 0.93065\n",
      "Average Loss                            | 0.23820\n",
      "Average Latency                         | 7.58206 milliseconds\n",
      "Average GPU Power Usage                 | 70.48008 watts\n",
      "Inference Energy Consumption            | 0.00000 kW/hr\u001b[0m\n",
      "I0315 10:10:56.351813 140384837449536 analysis.py:266] \n",
      "Configuration vgg7-quantized:\n",
      "Metric                                  | Value\n",
      "----------------------------------------|-----------------------\n",
      "Average Validation Accuracy             | 0.93011\n",
      "Average Precision                       | 0.93088\n",
      "Average Recall                          | 0.93073\n",
      "Average F1 Score                        | 0.93065\n",
      "Average Loss                            | 0.23820\n",
      "Average Latency                         | 7.58206 milliseconds\n",
      "Average GPU Power Usage                 | 70.48008 watts\n",
      "Inference Energy Consumption            | 0.00000 kW/hr\n"
     ]
    }
   ],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "trt_meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "\n",
    "trt_meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_15/version_0/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(trt_meta['graph_path'], pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0315 10:11:17.024709 140384837449536 analysis.py:184] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Configuration vgg7:\n",
      "Metric                                  | Value\n",
      "----------------------------------------|-----------------------\n",
      "Average Validation Accuracy             | 0.92161\n",
      "Average Precision                       | 0.92089\n",
      "Average Recall                          | 0.92109\n",
      "Average F1 Score                        | 0.92089\n",
      "Average Loss                            | 0.25405\n",
      "Average Latency                         | 32.36946 milliseconds\n",
      "Average GPU Power Usage                 | 92.43511 watts\n",
      "Inference Energy Consumption            | 0.00000 kW/hr\u001b[0m\n",
      "I0315 10:11:23.497289 140384837449536 analysis.py:266] \n",
      "Configuration vgg7:\n",
      "Metric                                  | Value\n",
      "----------------------------------------|-----------------------\n",
      "Average Validation Accuracy             | 0.92161\n",
      "Average Precision                       | 0.92089\n",
      "Average Recall                          | 0.92109\n",
      "Average F1 Score                        | 0.92089\n",
      "Average Loss                            | 0.25405\n",
      "Average Latency                         | 32.36946 milliseconds\n",
      "Average GPU Power Usage                 | 92.43511 watts\n",
      "Inference Energy Consumption            | 0.00000 kW/hr\n"
     ]
    }
   ],
   "source": [
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
