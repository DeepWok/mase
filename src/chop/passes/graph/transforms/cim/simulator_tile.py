from typing import Optional, Union, Tuple, Any, Dict, List, Type, TYPE_CHECKING, Callable

from torch import Tensor, zeros, clamp, randn, float32, empty
from torch.autograd import no_grad
from torch.nn import Module, Parameter
from .analog_mvm import AnalogMVM
from .enums import WeightModifierType, WeightClipType, WeightRemapType
from .utils import UniformQuantize
# from aihwkit.simulator.tiles.base import SimulatorTile
from .parameters import (
    WeightModifierParameter,
    WeightClipParameter,
    WeightRemapParameter,
)

# if TYPE_CHECKING:
#     from aihwkit.simulator.configs.configs import TorchInferenceRPUConfig
class SimulatorTile:
    """Minimal class interface for implementing the simulator tile.

    Note:
        This tile is generated by ``_create_simulator_tile`` in the
        ``SimulatorTileWrapper``.
    """

    def forward(
        self,
        x_input: Tensor,
        bias: bool = False,
        in_trans: bool = False,
        out_trans: bool = False,
        is_test: bool = False,
        non_blocking: bool = False,
    ) -> Tensor:
        """General simulator tile forward."""
        raise NotImplementedError

    def backward(
        self,
        d_input: Tensor,
        bias: bool = False,
        in_trans: bool = False,
        out_trans: bool = False,
        non_blocking: bool = False,
    ) -> Tensor:
        """Backward pass.

        Only needs to be implemented if torch autograd is `not` used.
        """
        raise NotImplementedError

    def update(
        self,
        x_input: Tensor,
        d_input: Tensor,
        bias: bool = False,
        in_trans: bool = False,
        out_trans: bool = False,
        non_blocking: bool = False,
    ) -> Tensor:
        """Update.

        Only needs to be implemented if torch autograd update is `not` used.
        """
        raise NotImplementedError

    def get_brief_info(self) -> str:
        """Returns a brief info"""
        raise NotImplementedError

    def get_weights(self) -> Tensor:
        """Returns the analog weights."""
        raise NotImplementedError

    def set_weights(self, weight: Tensor) -> None:
        """Stets the analog weights."""
        raise NotImplementedError

    def get_x_size(self) -> int:
        """Returns input size of tile"""
        raise NotImplementedError

    def get_d_size(self) -> int:
        """Returns output size of tile"""
        raise NotImplementedError

    def get_hidden_parameters(self) -> Tensor:
        """Get the hidden parameters of the tile.

        Returns:
            Hidden parameter tensor.
        """
        return empty(0, dtype=float32)

    def get_hidden_parameter_names(self) -> List[str]:
        """Get the hidden parameters names.

        Each name corresponds to a slice in the Tensor slice of the
        ``get_hidden_parameters`` tensor.

        Returns:
            List of names.
        """
        return []

    def set_hidden_parameters(self, params: Tensor) -> None:
        """Set the hidden parameters of the tile."""

    def get_learning_rate(self) -> Optional[float]:
        """Get the learning rate of the tile.

        Returns:
           learning rate if exists.
        """

    def set_learning_rate(self, learning_rate: Optional[float]) -> None:
        """Set the learning rate of the tile.

        No-op for tiles that do not need a learning rate.

        Args:
           learning rate: learning rate to set
        """

    def dump_extra(self) -> Optional[Dict]:
        """Dumps any extra states / attributed necessary for
        checkpointing.

        For Tiles based on Modules, this should be normally handled by
        torch automatically.
        """

    def load_extra(self, extra: Dict, strict: bool = False) -> None:
        """Load any extra states / attributed necessary for
        loading from checkpoint.

        For Tiles based on Modules, this should be normally handled by
        torch automatically.

        Note:
            Expects the exact same RPUConfig / device etc for applying
            the states. Cross-loading of state-dicts is not supported
            for extra states, they will be just ignored.

        Args:
            extra: dictionary of states from `dump_extra`.
            strict: Whether to throw an error if keys are not found.
        """

    def set_weights_uniform_random(self, bmin: float, bmax: float) -> None:
        """Sets the weights to uniform random numbers.

        Args:
           bmin: min value
           bmax: max value
        """
        raise NotImplementedError

    def get_meta_parameters(self) -> Any:
        """Returns meta parameters."""
        raise NotImplementedError


class TorchSimulatorTile(SimulatorTile, Module):
    """Torch based tile class.

    Args:
        x_size: input size
        d_size: output size
        rpu_config: resistive processing unit configuration.
    """
    # pylint: disable=abstract-method
    def __init__(
        self,
        x_size: int,
        d_size: int,
        rpu_config: "TorchInferenceRPUConfig",
        bias: bool = False,
        analog_mvm: Type[AnalogMVM] = AnalogMVM,
    ):
        Module.__init__(self)
        self.x_size = x_size
        self.d_size = d_size

        if bias:
            raise NotImplementedError("Analog bias is not supported for TorchSimulatorTile")

        self.check_rpu_config_support(rpu_config)
        analog_mvm.check_support(rpu_config.forward)

        self._analog_mvm = analog_mvm
        self._f_io = rpu_config.forward
        self._modifier = rpu_config.modifier
        dtype = rpu_config.runtime.data_type.as_torch()
        if self._f_io.out_noise_std > 0:
            out_noise_values = (self._f_io.out_noise * (1.0 + randn((d_size,), dtype=dtype))).abs()
            self.register_buffer("out_noise_values", out_noise_values)
        else:
            self.out_noise_values = None

        # Don't use randn here to avoid changing the seed in
        # comparison to RPUCuda tiles
        self.weight = Parameter(zeros(self.d_size, self.x_size, dtype=dtype))

    def set_weights(self, weight: Tensor) -> None:
        """Set the tile weights.

        Args:
            weight: ``[out_size, in_size]`` weight matrix.
        """
        self.weight.data = weight.clone().to(self.weight.device)

    def get_weights(self) -> Tensor:
        """Get the tile weights.

        Returns:
            a tuple where the first item is the ``[out_size, in_size]`` weight
            matrix; and the second item is either the ``[out_size]`` bias vector
            or ``None`` if the tile is set not to use bias.
        """
        return self.weight.data.detach().cpu()

    def get_x_size(self) -> int:
        """Returns input size of tile"""
        return self.x_size

    def get_d_size(self) -> int:
        """Returns output size of tile"""
        return self.d_size

    def get_brief_info(self) -> str:
        """Returns a brief info"""
        return self.__class__.__name__ + "({})".format(self.extra_repr())

    def extra_repr(self) -> str:
        return "{}, {}, {}".format(self.d_size, self.x_size, self.weight.device).rstrip()

    @no_grad()
    def remap_weights(self, remap: WeightRemapParameter, scales: Tensor) -> Tensor:
        """
        Remap the weights to the specified range and return new scales.

        Args:
            remap: hyper-parameters defining the remapping
            scales: current scale values.

        Raises:
            ConfigError: If WeightRemapType is unknown.

        Returns:
            Tensor: New scales.
        """
        # pylint: disable=arguments-differ

        scaled_weights = self.weight * scales.view(-1, 1)
        if remap.type == WeightRemapType.LAYERWISE_SYMMETRIC:
            new_scale = scaled_weights.abs().max()
            self.weight.data = scaled_weights / new_scale
            return new_scale.view(-1)
        if remap.type == WeightRemapType.CHANNELWISE_SYMMETRIC:
            new_scale, _ = scaled_weights.abs().max(1)
            self.weight.data = scaled_weights / new_scale.view(-1, 1)
            return new_scale

        raise NotImplementedError(f"Unknown weight remap type {remap.type}")

    @no_grad()
    def clip_weights(self, clip: WeightClipParameter) -> None:
        """Clip the weights.

        Args:
            clip: parameters specifying the clipping methof and type.

        Raises:
            NotImplementedError: For unsupported WeightClipTypes
            ConfigError: If unknown WeightClipType used.
        """

        if clip.type == WeightClipType.FIXED_VALUE:
            self.weight.data = clamp(self.weight, -clip.fixed_value, clip.fixed_value)
        elif clip.type == WeightClipType.LAYER_GAUSSIAN:
            alpha = self.weight.std() * clip.sigma
            if clip.fixed_value > 0:
                alpha = min(clip.fixed_value, alpha)
            self.weight.data = clamp(self.weight, -alpha, alpha)

        elif clip.type == WeightClipType.AVERAGE_CHANNEL_MAX:
            raise NotImplementedError
        else:
            raise NotImplementedError(f"Unknown clip type {clip.type}")

    def set_config(self, rpu_config: "TorchInferenceRPUConfig") -> None:
        """Updated the configuration to allow on-the-fly changes.

        Args:
            rpu_config: configuration to use in the next forward passes.
        """
        self._f_io = rpu_config.forward
        self._modifier = rpu_config.modifier

    def register_weight_hook(self, hook: Callable) -> Any:
        """Register a hook to the weights."""
        return self.weight.register_hook(hook)

    def forward(
        self,
        x_input: Tensor,
        bias: bool = False,
        in_trans: bool = False,
        out_trans: bool = False,
        is_test: bool = False,
        non_blocking: bool = False,
    ) -> Tensor:
        # pylint: disable=too-many-locals, too-many-branches

        if in_trans or out_trans:
            raise NotImplementedError("Non-trans MVMs supported only.")

        if not is_test:
            noisy_weights = self.modify_weight(
                self.weight, self._modifier, x_input.shape[0]
            )
        else:
            noisy_weights = self.weight

        return self._analog_mvm.matmul(
            noisy_weights,
            x_input,
            self._f_io,
            False,
            is_test=is_test,
            out_noise_values=self.out_noise_values,
        )

    @staticmethod
    def modify_weight(
        inp_weight: Tensor, modifier: WeightModifierParameter, batch_size: int
    ) -> Tensor:
        """Weight modifier that adds noise to the weights according to rpu config.

        Args:
            inp_weight: Input weights.
            modifier: Noise injection configuration.
            batch_size (int): Batch size.

        Raises:
            TorchTileConfigError: Unsupported/unknown weight modifier type.

        Returns:
            Weights with noise injected.
        """
        per_batch_sample = modifier.per_batch_sample
        target_shape = (batch_size,) + inp_weight.shape if per_batch_sample else inp_weight.shape
        if modifier.type in [WeightModifierType.NONE, WeightModifierType.COPY]:
            return inp_weight

        if modifier.type == WeightModifierType.MULT_NORMAL:
            with no_grad():
                gauss = randn(size=target_shape, device=inp_weight.device, dtype=inp_weight.dtype)
                noise = inp_weight * modifier.std_dev * gauss
            out_weight = inp_weight.clone() + noise
            return out_weight

        assumed_wmax = modifier.assumed_wmax
        if modifier.rel_to_actual_wmax:
            assumed_wmax = inp_weight.abs().max()

        if modifier.type == WeightModifierType.DISCRETIZE:
            # - Discretize the weights on the fly and backprob through them
            out_weight = inp_weight.clone().view(target_shape)
            out_weight = UniformQuantize.apply(
                out_weight, modifier.res, assumed_wmax, modifier.sto_round
            )
        elif modifier.type == WeightModifierType.ADD_NORMAL:
            with no_grad():
                noise = (
                    modifier.std_dev
                    * assumed_wmax
                    * randn(target_shape, device=inp_weight.device, dtype=inp_weight.dtype)
                )
            out_weight = inp_weight.clone() + noise
        else:
            raise NotImplementedError(f"Weight modifier {modifier} not supported")
        return out_weight

    @classmethod
    def check_rpu_config_support(cls, rpu_config: "TorchInferenceRPUConfig") -> None:
        """Check the RPUConfig for support with TorchSimulatorTile

        Throws an assertion error when there is an incompatibility
        with the used rpu config and what the current torch tile
        supports

        Args:
            rpu_config: the rpu config to be checked
        Raises:
            TorchTileConfigError: in case a feature is not supported
        """
        # pylint: disable=too-many-branches

        if rpu_config.clip.type == WeightClipType.AVERAGE_CHANNEL_MAX:
            raise NotImplementedError("Clip type AVERAGE_CHANNEL_MAX not supported by torch tile")

        if rpu_config.modifier.enable_during_test:
            raise NotImplementedError("Modifier noise is currently always off in the torch tile")

        if rpu_config.modifier.copy_last_column:
            raise NotImplementedError("Bias is assumed to be in digital only for torch tile")

        if rpu_config.modifier.type in [
            WeightModifierType.DOREFA,
            WeightModifierType.POLY,
            WeightModifierType.PROG_NOISE,
            WeightModifierType.PCM_NOISE,
            WeightModifierType.DISCRETIZE_ADD_NORMAL,
        ]:
            raise NotImplementedError(
                "The given modifier noise type is not supported in the torch tile"
            )

        if rpu_config.modifier.pdrop > 0.0:
            raise NotImplementedError("The drop-connect is not supported in the torch tile")

        if rpu_config.remap.type not in [
            WeightRemapType.LAYERWISE_SYMMETRIC,
            WeightRemapType.CHANNELWISE_SYMMETRIC,
            WeightRemapType.NONE,
        ]:
            raise NotImplementedError("Remapping type not supported.")

        if rpu_config.remap.remapped_wmax != 1.0:
            raise NotImplementedError("Remapping to value different from 1.0 not supported.")

        if rpu_config.remap.max_scale_range != 0.0:
            raise NotImplementedError("Remap parameter max_scale_range must be 0.0.")

        if rpu_config.remap.max_scale_ref != 0.0:
            raise NotImplementedError("Remap parameter max_scale_ref must be 0.0.")
