{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor, TrainerCallback\n",
    "from datasets import DatasetDict, load_dataset, Dataset\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "import numpy as np\n",
    "\n",
    "# Import MASE graph and passes (ensure PYTHONPATH is set correctly)\n",
    "from chop import MaseGraph\n",
    "import chop.passes as passes  # type: ignore\n",
    "from chop.passes.module import report_trainable_parameters_analysis_pass  # type: ignore\n",
    "from chop.tools import get_trainer  # type: ignore\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Helper function: convert tensor to list if needed.\n",
    "# ------------------------------------------------------------------------------\n",
    "def to_list(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.tolist()\n",
    "    return x\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Revised DataCollatorCTCWithPadding: manually pad sequences\n",
    "# ------------------------------------------------------------------------------\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Convert fields to lists\n",
    "        input_values_list = [to_list(f[\"input_values\"]) for f in features]\n",
    "        attention_mask_list = [to_list(f[\"attention_mask\"]) for f in features]\n",
    "        label_list = [to_list(f[\"labels\"]) for f in features]\n",
    "\n",
    "        # Compute max lengths\n",
    "        max_input_length = max(len(seq) for seq in input_values_list)\n",
    "        max_label_length = max(len(seq) for seq in label_list)\n",
    "\n",
    "        # Manually pad sequences\n",
    "        padded_input_values = [\n",
    "            seq + [0.0] * (max_input_length - len(seq)) for seq in input_values_list\n",
    "        ]\n",
    "        padded_attention_mask = [\n",
    "            seq + [0] * (max_input_length - len(seq)) for seq in attention_mask_list\n",
    "        ]\n",
    "        padded_labels = [\n",
    "            seq + [-100] * (max_label_length - len(seq)) for seq in label_list\n",
    "        ]\n",
    "\n",
    "        batch = {\n",
    "            \"input_values\": torch.tensor(padded_input_values, dtype=torch.float),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(padded_labels, dtype=torch.long),\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Define the model and dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "checkpoint = \"facebook/wav2vec2-base-960h\"\n",
    "tokenizer_checkpoint = \"facebook/wav2vec2-base-960h\"\n",
    "dataset_name = \"librispeech_asr\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(tokenizer_checkpoint)\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "vocab = tokenizer.convert_ids_to_tokens(range(tokenizer.vocab_size))\n",
    "decoder = build_ctcdecoder(vocab)\n",
    "\n",
    "# Load a small sample for quick experimentation\n",
    "dataset = load_dataset(dataset_name, \"clean\", split=\"validation\", streaming=True, trust_remote_code=True)\n",
    "sample_list = list(dataset.take(50))\n",
    "\n",
    "def preprocess_function(example):\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "    inputs = processor(audio=audio_array, sampling_rate=int(sampling_rate),\n",
    "                       return_tensors=\"pt\", padding=True)\n",
    "    attention_mask = torch.ones(inputs.input_values.shape, dtype=torch.long)\n",
    "    with processor.as_target_processor():\n",
    "        labels = processor.tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids\n",
    "    return {\n",
    "        \"input_values\": inputs.input_values.squeeze(0),\n",
    "        \"attention_mask\": attention_mask.squeeze(0),\n",
    "        \"labels\": labels.squeeze(0)\n",
    "    }\n",
    "\n",
    "small_dataset = Dataset.from_list(sample_list)\n",
    "small_dataset = small_dataset.map(preprocess_function,\n",
    "                                  remove_columns=[\"speaker_id\", \"file\", \"id\", \"chapter_id\", \"audio\"])\n",
    "tokenized_dataset = DatasetDict({\n",
    "    \"train\": small_dataset,\n",
    "    \"test\": small_dataset\n",
    "})\n",
    "\n",
    "model = AutoModelForCTC.from_pretrained(checkpoint)\n",
    "encoder = model.wav2vec2    # static, FX-friendly\n",
    "ctc_head = model.lm_head     # dynamic CTC head\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Build the MASE graph for the encoder and initialize metadata\n",
    "# ------------------------------------------------------------------------------\n",
    "mg = MaseGraph(encoder, hf_input_names=[\"input_values\", \"attention_mask\"])\n",
    "mg, _ = passes.init_metadata_analysis_pass(mg)\n",
    "\n",
    "dummy_in = {\n",
    "    \"input_values\": torch.zeros((1, 16000), dtype=torch.float32),\n",
    "    \"attention_mask\": torch.ones((1, 16000), dtype=torch.long),\n",
    "}\n",
    "mg, _ = passes.add_common_metadata_analysis_pass(mg,\n",
    "                                                 pass_args={\n",
    "                                                     \"dummy_in\": dummy_in,\n",
    "                                                     \"add_value\": True,\n",
    "                                                     \"force_device_meta\": False,\n",
    "                                                 })\n",
    "\n",
    "if not hasattr(mg.model, \"metadata\"):\n",
    "    mg.model.metadata = {}\n",
    "for name, param in mg.model.named_parameters():\n",
    "    mg.model.metadata[name] = {\"stats\": {\"movement\": torch.zeros_like(param)}}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Build the combined model (without pruning applied yet)\n",
    "# ------------------------------------------------------------------------------\n",
    "class CombinedWav2Vec2CTC(nn.Module):\n",
    "    def __init__(self, encoder, ctc_head, blank_id=0, beam_width=10, decoder=None):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.ctc_head = ctc_head  \n",
    "        self.blank_id = blank_id\n",
    "        self.beam_width = beam_width\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        encoder_out = self.encoder(input_values, attention_mask=attention_mask)\n",
    "        hidden_states = encoder_out[\"last_hidden_state\"]\n",
    "        logits = self.ctc_head(hidden_states)\n",
    "        output = {\"logits\": logits, \"labels\": labels}\n",
    "        if labels is not None:\n",
    "            log_probs = logits.log_softmax(dim=-1).transpose(0, 1)\n",
    "            batch_size, time_steps, _ = logits.shape\n",
    "            input_lengths = torch.full((batch_size,), time_steps, dtype=torch.long, device=logits.device)\n",
    "            target_lengths = (labels != -100).sum(dim=1)\n",
    "            loss = F.ctc_loss(\n",
    "                log_probs, labels, input_lengths, target_lengths,\n",
    "                blank=self.blank_id, reduction=\"mean\", zero_infinity=True\n",
    "            )\n",
    "            output[\"loss\"] = loss\n",
    "        else:\n",
    "            if self.decoder is not None:\n",
    "                log_probs = logits.log_softmax(dim=-1)\n",
    "                log_probs_np = log_probs[0].cpu().detach().numpy()\n",
    "                transcription = self.decoder.decode(log_probs_np, beam_width=self.beam_width).lower()\n",
    "                output[\"transcription\"] = transcription  \n",
    "        return output\n",
    "\n",
    "combined_model = CombinedWav2Vec2CTC(encoder=mg.model, ctc_head=ctc_head, decoder=decoder, beam_width=10)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Define a Movement Tracking Callback to accumulate real movement data during training\n",
    "# ------------------------------------------------------------------------------\n",
    "class MovementTrackingCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.prev_params = {}\n",
    "        self.movement_stats = {}\n",
    "        model = kwargs[\"model\"]\n",
    "        for name, param in model.encoder.named_parameters():\n",
    "            self.prev_params[name] = param.detach().clone()\n",
    "            self.movement_stats[name] = torch.zeros_like(param)\n",
    "            if not hasattr(model.encoder, \"metadata\"):\n",
    "                model.encoder.metadata = {}\n",
    "            if name not in model.encoder.metadata:\n",
    "                model.encoder.metadata[name] = {\"stats\": {}}\n",
    "            model.encoder.metadata[name][\"stats\"][\"movement\"] = self.movement_stats[name]\n",
    "        return control\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        for name, param in model.encoder.named_parameters():\n",
    "            movement = (param.detach() - self.prev_params[name]).abs()\n",
    "            self.movement_stats[name] += movement\n",
    "            self.prev_params[name].copy_(param.detach())\n",
    "            model.encoder.metadata[name][\"stats\"][\"movement\"] = self.movement_stats[name]\n",
    "        return control\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Setup Trainer and run warm-up training to accumulate movement data\n",
    "# ------------------------------------------------------------------------------\n",
    "trainer = get_trainer(\n",
    "    model=combined_model,\n",
    "    tokenized_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    evaluate_metric=\"wer\",\n",
    "    num_train_epochs=1,  # Warm-up training epoch\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.add_callback(MovementTrackingCallback())\n",
    "\n",
    "print(\"Starting warm-up training to accumulate movement data...\")\n",
    "trainer.train()\n",
    "print(\"Warm-up training complete.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. Apply movement-based pruning pass using the accumulated movement data\n",
    "# ------------------------------------------------------------------------------\n",
    "pruning_config = {\n",
    "    \"weight\": {\n",
    "        \"sparsity\": 0.2,\n",
    "        \"method\": \"movement\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"sparsity\": 0.2,\n",
    "        \"method\": \"movement\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mg, _ = passes.prune_transform_pass(mg, pass_args=pruning_config)\n",
    "_, _ = report_trainable_parameters_analysis_pass(mg.model)\n",
    "combined_model.encoder = mg.model\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7. Optionally, fine-tune the pruned model and evaluate\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Starting fine-tuning of the pruned model...\")\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation WER: {eval_results['eval_wer']}\")\n",
    "print(f\"Evaluation loss: {eval_results['eval_loss']}\")\n",
    "print(f\"Evaluation runtime: {eval_results['eval_runtime']}\")\n",
    "print(f\"Evaluation samples per second: {eval_results['eval_samples_per_second']}\")\n",
    "print(f\"Evaluation steps per second: {eval_results['eval_steps_per_second']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
