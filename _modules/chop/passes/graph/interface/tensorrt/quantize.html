
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>chop.passes.graph.interface.tensorrt.quantize &#8212; MASE 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-data-viewer/jsonview.bundle.css?v=f6ef2277" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-needs/libs/html/datatables.min.css?v=4b4fd840" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-needs/common_css/need_links.css?v=2150a916" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-needs/common_css/need_style.css?v=92936fa5" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-needs/common_css/needstable.css?v=5e1b6797" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-needs/common_css/need_toggle.css?v=5c6620df" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-needs/common_css/need_core.css?v=f5b60a78" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/sphinx-needs/modern.css?v=803738c0" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../../../../_static/documentation_options.js?v=e645c8fa"></script>
    <script src="../../../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../../../../_static/sphinx-data-viewer/jsonview.bundle.js?v=18cd53c5"></script>
    <script src="../../../../../../_static/sphinx-data-viewer/jsonview_loader.js?v=f7ff7e7d"></script>
    <script src="../../../../../../_static/sphinx-needs/libs/html/datatables.min.js?v=8a4aee21"></script>
    <script src="../../../../../../_static/sphinx-needs/libs/html/datatables_loader.js?v=a2cae175"></script>
    <script src="../../../../../../_static/sphinx-needs/libs/html/sphinx_needs_collapse.js?v=dca66431"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/chop/passes/graph/interface/tensorrt/quantize';</script>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">MASE 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../modules/documentation/installation.html">Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/getting_started/Get-started-using-uv.html">Getting Started using uv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/getting_started/Get-started-using-Docker.html">Getting Started using Docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/getting_started/Get-started-students.html">Additional Instructions for Imperial College Students</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../modules/documentation/quickstart.html">Quickstart</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../modules/documentation/tutorials.html">Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_1_introduction_to_mase.html">Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_2_lora_finetune.html">Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_3_qat.html">Tutorial 3: Running Quantization-Aware Training (QAT) on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_4_pruning.html">Tutorial 4: Unstructured Pruning on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_5_nas_optuna.html">Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_6_mixed_precision_search.html">Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_7_distributed_deployment.html">Tutorial 7: Deploying a Model for Inference on Distributed Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/tutorial_9_kernel_fusion.html">Tutorial 9: Running Kernel Fusion for Inference Acceleration on GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/advanced/tensorRT_quantization_tutorial.html">Advanced: TensorRT Quantization Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/advanced/onnxrt_quantization_tutorial.html">Advanced: ONNX Runtime Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/advanced/cli.html">Advanced: Using Mase CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/developer/Add-model-to-machop.html">Developer: Guide on how to add a new model into Machop</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/developer/doc_writing.html">Developer: How to write documentations in MASE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/tutorials/developer/how_to_extend_search.html">Developer: How to extend search</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../modules/documentation/health.html">Repository Health</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../modules/documentation/specifications.html">Coding Style Specifications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/specifications/C-coding-style-specifications.html">C/C++ Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/specifications/Python-coding-style-specifications.html">Python Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/documentation/specifications/Verilog-coding-style-specifications.html">Verilog Coding Style Specifications</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machop API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../modules/machop.html">Machop Documentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/actions.html">chop.actions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/datasets.html">chop.datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/distributed.html">chop.distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/ir.html">chop.ir</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/models.html">chop.models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/nn.html">chop.nn</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../../modules/chop/nn_quantized.html">chop.nn.quantized</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../../modules/chop/nn_quantized_functional.html">chop.nn.quantized.functional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../../modules/chop/nn_quantized_modules.html">chop.nn.quantized.modules</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../../modules/chop/passes.html">chop.passes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../../../modules/chop/passes_module.html">chop.passes.module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/module_analysis/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/module_transform/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/transform/onn.html">chop.passes.module.transforms.onn</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../../../modules/chop/passes_graph.html">chop.passes.graph</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/add_metadata.html">chop.passes.graph.analysis.add_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/autosharding.html">chop.passes.graph.analysis.autosharding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/init_metadata.html">chop.passes.graph.analysis.init_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/report.html">chop.passes.graph.analysis.report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/statistical_profiler.html">chop.passes.graph.analysis.statistical_profiler.profile_statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/verify.html">chop.passes.graph.analysis.verify.verify</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/quantization.html">chop.passes.graph.calculate_avg_bits_mg_analysis_pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/pruning.html">chop.passes.graph.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/analysis/runtime.html">chop.passes.graph.analysis.runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/transform/pruning.html">chop.passes.transform.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/transform/quantize.html">chop.passes.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/transform/utils.html">chop.passes.transform.utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/transform/tensorrt.html">chop.passes.transform.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/interface/save_and_load.html">chop.passes.interface.save_and_load</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/interface/tensorrt.html">chop.passes.interface.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../../modules/chop/interface/onnxrt.html">chop.passes.interface.onnxrt</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/pipelines.html">chop.pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/chop/tools.html">chop.tools</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../modules/adls_2024.html">Advanced Deep Learning Systems: 2024/2025</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2024/lab_0_introduction.html">Lab 0: Introduction to Mase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2024/lab_1_compression.html">Lab 1: Model Compression (Quantization and Pruning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2024/lab_2_nas.html">Lab 2: Neural Architecture Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2024/lab_3_mixed_precision_search.html">Lab 3: Mixed Precision Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2024/lab4-software.html">Lab 4 (Software Stream) Performance Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2024/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../../modules/adls_2023.html">Advanced Deep Learning Systems: 2023/2024</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2023/lab1.html">Lab 1 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2023/lab2.html">Lab 2 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2023/lab3.html">Lab 3 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2023/lab4-software.html">Lab 4 (Software Stream) for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../modules/labs_2023/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for chop.passes.graph.interface.tensorrt.quantize</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">importlib.util</span><span class="w"> </span><span class="kn">import</span> <span class="n">find_spec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">copy</span><span class="p">,</span> <span class="n">deepcopy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">onnx</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">pytorch_quantization_is_installed</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">if</span> <span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;pytorch_quantization&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">find_spec</span><span class="p">(</span><span class="s2">&quot;tensorrt&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">tensorrt_engine_interface_pass</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">pass_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
            <span class="s2">&quot;tensorrt or pytorch_quantization is not installed. Cannot use tensorrt quantize pass.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">Quantizer</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
            <span class="s2">&quot;pytorch_quantization is not installed. Cannot use tensorrt quantize pass.&quot;</span>
        <span class="p">)</span>

<span class="k">else</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">tensorrt</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">trt</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">quant_modules</span><span class="p">,</span> <span class="n">calib</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span> <span class="k">as</span> <span class="n">quant_nn</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_quantization.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorQuantizer</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_quantization.tensor_quant</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantDescriptor</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.graph.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
        <span class="n">get_mase_op</span><span class="p">,</span>
        <span class="n">get_mase_type</span><span class="p">,</span>
        <span class="n">get_node_actual_target</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.graph.interface.save_and_load</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_mase_graph_interface_pass</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">...utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy_mase_graph</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">...transforms.tensorrt.quantize.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
        <span class="n">Int8Calibrator</span><span class="p">,</span>
        <span class="n">prepare_save_path</span><span class="p">,</span>
        <span class="n">check_for_value_in_dict</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">register_mase_pass</span>

<div class="viewcode-block" id="tensorrt_engine_interface_pass">
<a class="viewcode-back" href="../../../../../../modules/chop/interface/tensorrt.html#chop.passes.graph.interface.tensorrt.quantize.tensorrt_engine_interface_pass">[docs]</a>
    <span class="nd">@register_mase_pass</span><span class="p">(</span>
        <span class="s2">&quot;tensorrt_engine_interface_pass&quot;</span><span class="p">,</span>
        <span class="n">dependencies</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pytorch_quantization&quot;</span><span class="p">,</span> <span class="s2">&quot;tensorrt&quot;</span><span class="p">,</span> <span class="s2">&quot;pynvml&quot;</span><span class="p">,</span> <span class="s2">&quot;pycuda&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tensorrt_engine_interface_pass</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">pass_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a PyTorch model within a MaseGraph to a TensorRT engine, optimizing the model for faster inference speeds.</span>

<span class="sd">        This function acts as an interface between PyTorch and TensorRT, leveraging ONNX as an intermediate format. It&#39;s designed to be used post-Quantization Aware Training (QAT), ensuring that the quantized model can benefit from the performance enhancements offered by TensorRT. The conversion process saves the resulting ONNX and TensorRT engine files in their respective directories, facilitating easy deployment and version control.</span>

<span class="sd">        :param graph: The model graph to be converted. This graph should represent a model that has already been quantized and fine-tuned.</span>
<span class="sd">        :type graph: MaseGraph</span>
<span class="sd">        :param pass_args: A dictionary containing arguments that may affect the conversion process, such as optimization levels or specific TensorRT flags.</span>
<span class="sd">        :type pass_args: dict, optional</span>
<span class="sd">        :return: A tuple containing the graph with its model now linked to the generated TensorRT engine, and a dictionary with paths to the ONNX and TensorRT engine files.</span>
<span class="sd">        :rtype: tuple(MaseGraph, dict)</span>

<span class="sd">        The conversion process involves two main steps:</span>
<span class="sd">        1. Exporting the PyTorch model to ONNX format.</span>
<span class="sd">        2. Converting the ONNX model to a TensorRT engine.</span>

<span class="sd">        The paths to the saved `.onnx` and `.trt` files are included in the return value to provide easy access for subsequent deployment or analysis.</span>
<span class="sd">        The resulting files are are saved in the following directory structure, facilitating easy access and version control:</span>

<span class="sd">        - mase_output</span>
<span class="sd">            - tensorrt</span>
<span class="sd">                - quantization</span>
<span class="sd">                    - model_task_dataset_date</span>
<span class="sd">                        - cache</span>
<span class="sd">                        - ckpts</span>
<span class="sd">                            - fine_tuning</span>
<span class="sd">                        - json</span>
<span class="sd">                        - onnx</span>
<span class="sd">                        - trt</span>

<span class="sd">        Example of usage:</span>

<span class="sd">            graph = MaseGraph(...)</span>
<span class="sd">            converted_graph, paths = tensorrt_engine_interface_pass(graph, pass_args={})</span>

<span class="sd">        This example initiates the conversion of a quantized and fine-tuned PyTorch model to a TensorRT engine, with the paths to the resulting ONNX and TRT files being returned for further use.</span>

<span class="sd">        Note:</span>
<span class="sd">        The `.onnx` and `.trt` files are stored according to the directory structure outlined in Section 1.3 of the Quantized Aware Training (QAT) documentation, ensuring organized and accessible storage for these critical files.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">quantizer</span> <span class="o">=</span> <span class="n">Quantizer</span><span class="p">(</span><span class="n">pass_args</span><span class="p">)</span>
        <span class="n">trt_engine_path</span><span class="p">,</span> <span class="n">onnx_path</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">pytorch_to_trt</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>

        <span class="c1"># Link the model with the graph for further operations or evaluations</span>
        <span class="n">graph</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">graph</span><span class="o">.</span><span class="n">fx_graph</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">graph</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;trt_engine_path&quot;</span><span class="p">:</span> <span class="n">trt_engine_path</span><span class="p">,</span> <span class="s2">&quot;onnx_path&quot;</span><span class="p">:</span> <span class="n">onnx_path</span><span class="p">}</span></div>


    <span class="k">class</span><span class="w"> </span><span class="nc">Quantizer</span><span class="p">:</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;pytorch_quantization&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">pytorch_to_trt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Converts PyTorch model to TensorRT format.&quot;&quot;&quot;</span>
            <span class="c1"># Model is first converted to ONNX format and then to TensorRT</span>
            <span class="n">ONNX_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pytorch_to_ONNX</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
            <span class="n">TRT_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ONNX_to_TRT</span><span class="p">(</span><span class="n">ONNX_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_TRT_model_summary</span><span class="p">(</span><span class="n">TRT_path</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">TRT_path</span><span class="p">,</span> <span class="n">ONNX_path</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Retrieve specific configuration from the instance&#39;s config dictionary or return default.&quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;default&quot;</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">pre_quantization_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Evaluate pre-quantization performance.&quot;&quot;&quot;</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluate pre-quantization performance...&quot;</span><span class="p">)</span>
            <span class="c1"># Add evaluation code here</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">pytorch_quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Applies quantization procedures to PyTorch graph based on type.&quot;&quot;&quot;</span>
            <span class="c1"># Add quantization code here</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">ONNX_to_TRT</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ONNX_path</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Converting PyTorch model to TensorRT...&quot;</span><span class="p">)</span>

            <span class="c1"># Check for layer wise mixed precision</span>
            <span class="n">layer_wise_mixed_precision</span> <span class="o">=</span> <span class="p">(</span>
                <span class="kc">True</span>
                <span class="k">if</span> <span class="n">check_for_value_in_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;int8&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">check_for_value_in_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;fp16&quot;</span><span class="p">)</span>
                <span class="k">else</span> <span class="kc">False</span>
            <span class="p">)</span>

            <span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
            <span class="n">builder</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">Builder</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
            <span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">create_network</span><span class="p">(</span>
                <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nb">int</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">NetworkDefinitionCreationFlag</span><span class="o">.</span><span class="n">EXPLICIT_BATCH</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">parser</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">OnnxParser</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">TRT_LOGGER</span><span class="p">)</span>

            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">ONNX_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">read</span><span class="p">()):</span>
                    <span class="k">for</span> <span class="n">error</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">parser</span><span class="o">.</span><span class="n">num_errors</span><span class="p">):</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">parser</span><span class="o">.</span><span class="n">get_error</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>
                    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Failed to parse the ONNX file.&quot;</span><span class="p">)</span>

            <span class="c1"># Create the config object here</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">create_builder_config</span><span class="p">()</span>
            <span class="n">config</span><span class="o">.</span><span class="n">max_workspace_size</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">&lt;&lt;</span> <span class="mi">30</span>  <span class="c1"># 4GB</span>

            <span class="n">default_precision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">][</span><span class="s2">&quot;config&quot;</span><span class="p">][</span><span class="s2">&quot;precision&quot;</span><span class="p">]</span>

            <span class="c1"># This section may be uncommented if pytorch-quantization is not used for int8 Calibration</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            # Only required if pytorch-quantization is not used</span>
<span class="sd">            config.set_flag(trt.BuilderFlag.INT8)</span>
<span class="sd">            if default_precision == &#39;int8&#39;:</span>
<span class="sd">                config.int8_calibrator = Int8Calibrator(</span>
<span class="sd">                    self.config[&#39;num_calibration_batches&#39;],</span>
<span class="sd">                    self.config[&#39;data_module&#39;].train_dataloader(),</span>
<span class="sd">                    prepare_save_path(self.config, method=&#39;cache&#39;, suffix=&#39;cache&#39;)</span>
<span class="sd">                    )</span>
<span class="sd">            &quot;&quot;&quot;</span>

            <span class="c1"># Only quantize and calibrate non int8 pytorch-quantization</span>
            <span class="k">if</span> <span class="n">default_precision</span> <span class="o">!=</span> <span class="s2">&quot;int8&quot;</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">BuilderFlag</span><span class="o">.</span><span class="n">PREFER_PRECISION_CONSTRAINTS</span><span class="p">)</span>
                <span class="n">config</span><span class="o">.</span><span class="n">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">BuilderFlag</span><span class="o">.</span><span class="n">DIRECT_IO</span><span class="p">)</span>
                <span class="n">config</span><span class="o">.</span><span class="n">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">BuilderFlag</span><span class="o">.</span><span class="n">REJECT_EMPTY_ALGORITHMS</span><span class="p">)</span>
                <span class="n">config</span><span class="o">.</span><span class="n">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">BuilderFlag</span><span class="o">.</span><span class="n">STRICT_TYPES</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">default_precision</span> <span class="o">==</span> <span class="s2">&quot;fp16&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">layer_wise_mixed_precision</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">set_flag</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">BuilderFlag</span><span class="o">.</span><span class="n">FP16</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">layer_wise_mixed_precision</span><span class="p">:</span>
                <span class="c1"># Now, iterate over the network layers and set precision as per the config</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                    <span class="n">layer</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">layer_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;feature_layers_</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="n">layer_precision</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;passes&quot;</span><span class="p">,</span> <span class="p">{})</span>
                        <span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tensorrt&quot;</span><span class="p">,</span> <span class="p">{})</span>
                        <span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">layer_key</span><span class="p">,</span> <span class="p">{})</span>
                        <span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="p">{})</span>
                        <span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;precision&quot;</span><span class="p">,</span> <span class="n">default_precision</span><span class="p">)</span>
                    <span class="p">)</span>

                    <span class="c1"># Apply precision settings based on the layer_precision value</span>
                    <span class="k">if</span> <span class="n">layer_precision</span> <span class="o">==</span> <span class="s2">&quot;fp16&quot;</span><span class="p">:</span>
                        <span class="n">layer</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">float16</span>
                        <span class="n">layer</span><span class="o">.</span><span class="n">set_output_type</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trt</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">HALF</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">layer_precision</span> <span class="o">==</span> <span class="s2">&quot;int8&quot;</span><span class="p">:</span>
                        <span class="n">layer</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">int8</span>
                        <span class="n">layer</span><span class="o">.</span><span class="n">set_output_type</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trt</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">INT8</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># You might want to handle the default case or unsupported precision types differently</span>
                        <span class="nb">print</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Warning: Unsupported precision type &#39;</span><span class="si">{</span><span class="n">layer_precision</span><span class="si">}</span><span class="s2">&#39; for layer </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">. Defaulting to fp16.&quot;</span>
                        <span class="p">)</span>
                        <span class="n">layer</span><span class="o">.</span><span class="n">precision</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">float16</span>
                        <span class="n">layer</span><span class="o">.</span><span class="n">set_output_type</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trt</span><span class="o">.</span><span class="n">DataType</span><span class="o">.</span><span class="n">HALF</span><span class="p">)</span>

            <span class="n">serialized_engine</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">build_serialized_network</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">serialized_engine</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
                    <span class="s2">&quot;Failed to build serialized network. A builderflag or config parameter may be incorrect or the ONNX model is unsupported.&quot;</span>
                <span class="p">)</span>

            <span class="n">trt_path</span> <span class="o">=</span> <span class="n">prepare_save_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;trt&quot;</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s2">&quot;trt&quot;</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">trt_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">serialized_engine</span><span class="p">)</span>

            <span class="c1"># Optimization profiles are needed for dynamic input shapes.</span>
            <span class="n">profile</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">create_optimization_profile</span><span class="p">()</span>
            <span class="n">inputTensor</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">get_input</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">profile</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span>
                <span class="n">inputTensor</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                <span class="p">(</span><span class="mi">8</span><span class="p">,)</span> <span class="o">+</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
                <span class="p">(</span><span class="mi">32</span><span class="p">,)</span> <span class="o">+</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
            <span class="p">)</span>
            <span class="n">config</span><span class="o">.</span><span class="n">add_optimization_profile</span><span class="p">(</span><span class="n">profile</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;TensorRT Conversion Complete. Stored trt model to </span><span class="si">{</span><span class="n">trt_path</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">trt_path</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">pytorch_to_ONNX</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Converts PyTorch model to ONNX format and saves it.&quot;&quot;&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Converting PyTorch model to ONNX...&quot;</span><span class="p">)</span>

            <span class="n">onnx_path</span> <span class="o">=</span> <span class="n">prepare_save_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;onnx&quot;</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s2">&quot;onnx&quot;</span><span class="p">)</span>

            <span class="n">dataloader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;data_module&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
            <span class="n">train_sample</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">train_sample</span> <span class="o">=</span> <span class="n">train_sample</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;accelerator&quot;</span><span class="p">])</span>

<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            This line may produce the warning if the model input size is fixed:</span>
<span class="sd">                    torch.onnx.export(model.cuda(), train_sample.cuda(), onnx_path, export_params=True, opset_version=11,</span>
<span class="sd">                            do_constant_folding=True, input_names=[&#39;input&#39;])# Load the ONNX model</span>
<span class="sd">            It is a known issue: https://github.com/onnx/onnx/issues/2836 https://github.com/ultralytics/yolov5/issues/5505</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span>
                <span class="n">train_sample</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span>
                <span class="n">onnx_path</span><span class="p">,</span>
                <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">opset_version</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span>
                <span class="n">do_constant_folding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>
            <span class="p">)</span>  <span class="c1"># Load the ONNX model</span>

            <span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">onnx_path</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">ValidationError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ONNX Conversion Failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;ONNX Conversion Complete. Stored ONNX model to </span><span class="si">{</span><span class="n">onnx_path</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">onnx_path</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">export_TRT_model_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">TRT_path</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Saves TensorRT model summary to json&quot;&quot;&quot;</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">TRT_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">trt_engine</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">Runtime</span><span class="p">(</span>
                    <span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
                <span class="p">)</span><span class="o">.</span><span class="n">deserialize_cuda_engine</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
                <span class="n">inspector</span> <span class="o">=</span> <span class="n">trt_engine</span><span class="o">.</span><span class="n">create_engine_inspector</span><span class="p">()</span>

                <span class="c1"># Retrieve engine information in JSON format</span>
                <span class="n">layer_info_json</span> <span class="o">=</span> <span class="n">inspector</span><span class="o">.</span><span class="n">get_engine_information</span><span class="p">(</span>
                    <span class="n">trt</span><span class="o">.</span><span class="n">LayerInformationFormat</span><span class="o">.</span><span class="n">JSON</span>
                <span class="p">)</span>

                <span class="c1"># Save the engine information to a JSON file</span>
                <span class="n">json_filename</span> <span class="o">=</span> <span class="n">prepare_save_path</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;json&quot;</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s2">&quot;json&quot;</span>
                <span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">json_filename</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">json_file</span><span class="p">:</span>
                    <span class="n">json_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">layer_info_json</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TensorRT Model Summary Exported to </span><span class="si">{</span><span class="n">json_filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DeepWok
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2023, DeepWok.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>