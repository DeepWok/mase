{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are starting from scratch, you can load the Bert checkpoint directly from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have previously ran the tutorial on Neural Architecture Search (NAS), run the following cell to import the best model obtained from the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "lab2_out_dir = Path(\"/workspace/labs/lab2/outputs\")\n",
    "lab3_out_dir = Path(\"/workspace/labs/lab3/outputs\")\n",
    "\n",
    "with open(f\"{lab2_out_dir}/tutorial_5_best_model.pkl\", \"rb\") as f:\n",
    "    base_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chop.nn.quantized.modules.linear import (\n",
    "    LinearInteger,\n",
    "    LinearMinifloatDenorm,\n",
    "    LinearMinifloatIEEE,\n",
    "    LinearLog,\n",
    "    LinearBlockFP,\n",
    "    LinearBlockMinifloat,\n",
    "    LinearBlockLog,\n",
    "    LinearBinary,\n",
    "    LinearBinaryScaling,\n",
    "    LinearBinaryResidualSign,\n",
    ")\n",
    "\n",
    "search_space = {\n",
    "    \"linear_layer_choices\": [\n",
    "        torch.nn.Linear,\n",
    "        LinearInteger,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_categorical` function, which triggers the chosen sampler to choose a layer type. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools.utils import deepsetattr\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "\n",
    "    # Fetch the model\n",
    "    trial_model = deepcopy(base_model)\n",
    "\n",
    "    # Quantize layers according to optuna suggestions\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == torch.nn.Linear:\n",
    "                continue\n",
    "\n",
    "            kwargs = {\n",
    "                \"in_features\": layer.in_features,\n",
    "                \"out_features\": layer.out_features,\n",
    "            }\n",
    "\n",
    "            # If the chosen layer is integer, define the low precision config\n",
    "            if new_layer_cls == LinearInteger:\n",
    "                kwargs[\"config\"] = {\n",
    "                    \"data_in_width\": 8,\n",
    "                    \"data_in_frac_width\": 4,\n",
    "                    \"weight_width\": 8,\n",
    "                    \"weight_frac_width\": 4,\n",
    "                    \"bias_width\": 8,\n",
    "                    \"bias_frac_width\": 4,\n",
    "                }\n",
    "            # elif... (other precisions)\n",
    "\n",
    "            # Create the new layer (copy the weights)\n",
    "            new_layer = new_layer_cls(**kwargs)\n",
    "            new_layer.weight.data = layer.weight.data\n",
    "\n",
    "            # Replace the layer in the model\n",
    "            deepsetattr(trial_model, name, new_layer)\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "import random\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = RandomSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 01:59:46,061]\u001b[0m A new study created in memory with name: bert-tiny-nas-study\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.325600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.360600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 02:02:22,909]\u001b[0m Trial 0 finished with value: 0.87464 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 0 with value: 0.87464.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 03:17:15,577]\u001b[0m A new study created in memory with name: tutorial6_integer_layerwise_widthfrac\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task 1: Integer per-layer width/frac search ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.363700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:20:00,237]\u001b[0m Trial 0 finished with value: 0.86956 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 8, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 16, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.value.bias_width': 32, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 32, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense.bias_width': 8, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 4, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 32, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.output.dense.weight_width': 8, 'bert.encoder.layer.0.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense.weight_width': 32, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 16, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 16, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense.weight_width': 32, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense.bias_width': 8, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 0 with value: 0.86956.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.350100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:22:55,529]\u001b[0m Trial 1 finished with value: 0.86008 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 16, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 16, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.weight_width': 32, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 32, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense.bias_width': 8, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 16, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.output.dense.weight_width': 32, 'bert.encoder.layer.0.output.dense.weight_frac_width': 8, 'bert.encoder.layer.0.output.dense.bias_width': 8, 'bert.encoder.layer.0.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.weight_width': 16, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.bias_width': 32, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 32, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.weight_width': 8, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 8, 'bert.encoder.layer.1.attention.self.key.bias_width': 16, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 8, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 16, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense.bias_width': 8, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 16, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.1.output.dense.weight_width': 8, 'bert.encoder.layer.1.output.dense.weight_frac_width': 2, 'bert.encoder.layer.1.output.dense.bias_width': 8, 'bert.encoder.layer.1.output.dense.bias_frac_width': 8, 'classifier_type': 'fp'}. Best is trial 0 with value: 0.86956.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:25:26,137]\u001b[0m Trial 2 finished with value: 0.87524 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 8, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.weight_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 8, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.weight_width': 32, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.bias_width': 32, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 16, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 32, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'integer', 'classifier.data_in_width': 32, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 16, 'classifier.weight_frac_width': 4, 'classifier.bias_width': 16, 'classifier.bias_frac_width': 8}. Best is trial 2 with value: 0.87524.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.336300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:28:12,052]\u001b[0m Trial 3 finished with value: 0.8752 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.self.value.weight_width': 8, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.value.bias_width': 16, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense.bias_width': 8, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 16, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.output.dense.weight_width': 8, 'bert.encoder.layer.0.output.dense.weight_frac_width': 2, 'bert.encoder.layer.0.output.dense.bias_width': 8, 'bert.encoder.layer.0.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.bias_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 16, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.key.weight_width': 16, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.bias_width': 16, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 32, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.weight_width': 32, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.bias_width': 16, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 2 with value: 0.87524.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.468200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:30:46,777]\u001b[0m Trial 4 finished with value: 0.86704 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_width': 32, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 16, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_width': 16, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 16, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.value.bias_width': 32, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 16, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.key.weight_width': 32, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.key.bias_width': 8, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 32, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 8, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'integer', 'classifier.data_in_width': 16, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 32, 'classifier.weight_frac_width': 2, 'classifier.bias_width': 32, 'classifier.bias_frac_width': 4}. Best is trial 2 with value: 0.87524.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:33:41,549]\u001b[0m Trial 5 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 8, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 32, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.bias_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 8, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.weight_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.bias_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 16, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.value.bias_width': 32, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'integer', 'bert.encoder.layer.0.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.0.attention.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense.weight_width': 32, 'bert.encoder.layer.0.attention.output.dense.weight_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense.bias_width': 8, 'bert.encoder.layer.0.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense.bias_width': 32, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 32, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.output.dense.weight_width': 32, 'bert.encoder.layer.0.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.weight_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 16, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.1.output.dense.weight_width': 32, 'bert.encoder.layer.1.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.output.dense.bias_width': 16, 'bert.encoder.layer.1.output.dense.bias_frac_width': 8, 'classifier_type': 'integer', 'classifier.data_in_width': 8, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 32, 'classifier.weight_frac_width': 2, 'classifier.bias_width': 32, 'classifier.bias_frac_width': 2}. Best is trial 2 with value: 0.87524.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:36:10,413]\u001b[0m Trial 6 finished with value: 0.87132 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.bias_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'integer', 'bert.encoder.layer.0.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.0.attention.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense.weight_width': 8, 'bert.encoder.layer.0.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense.bias_width': 32, 'bert.encoder.layer.0.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'integer', 'classifier.data_in_width': 16, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 8, 'classifier.weight_frac_width': 8, 'classifier.bias_width': 32, 'classifier.bias_frac_width': 2}. Best is trial 2 with value: 0.87524.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.337800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:38:52,300]\u001b[0m Trial 7 finished with value: 0.87692 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 16, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_width': 32, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 32, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 32, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 16, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.output.dense.weight_width': 16, 'bert.encoder.layer.0.output.dense.weight_frac_width': 2, 'bert.encoder.layer.0.output.dense.bias_width': 16, 'bert.encoder.layer.0.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 8, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.weight_width': 32, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense.weight_width': 32, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.bias_width': 16, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 7 with value: 0.87692.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.368500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:41:45,192]\u001b[0m Trial 8 finished with value: 0.86032 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 8, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 8, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 32, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 8, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 8, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.value.bias_width': 16, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 8, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.output.dense.weight_width': 16, 'bert.encoder.layer.0.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_width': 16, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 32, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 32, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 32, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 32, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.output.dense.weight_width': 32, 'bert.encoder.layer.1.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.output.dense.bias_width': 16, 'bert.encoder.layer.1.output.dense.bias_frac_width': 2, 'classifier_type': 'fp'}. Best is trial 7 with value: 0.87692.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.338400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:44:32,609]\u001b[0m Trial 9 finished with value: 0.87436 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 16, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.bias_width': 32, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 16, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 32, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.bias_width': 32, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 8, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.output.dense.weight_width': 16, 'bert.encoder.layer.0.output.dense.weight_frac_width': 8, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 32, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.1.output.dense.weight_width': 16, 'bert.encoder.layer.1.output.dense.weight_frac_width': 2, 'bert.encoder.layer.1.output.dense.bias_width': 16, 'bert.encoder.layer.1.output.dense.bias_frac_width': 4, 'classifier_type': 'fp'}. Best is trial 7 with value: 0.87692.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.328500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:47:05,550]\u001b[0m Trial 10 finished with value: 0.8606 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 32, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.weight_width': 16, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'integer', 'bert.encoder.layer.0.attention.output.dense.data_in_width': 32, 'bert.encoder.layer.0.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense.weight_width': 16, 'bert.encoder.layer.0.attention.output.dense.weight_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense.bias_width': 16, 'bert.encoder.layer.0.attention.output.dense.bias_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 8, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.weight_width': 32, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 4, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense.weight_width': 16, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.bias_width': 16, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 4, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 7 with value: 0.87692.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.317300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.319400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.384100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:49:38,421]\u001b[0m Trial 11 finished with value: 0.87436 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 8, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.weight_width': 32, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 8, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.self.key.weight_width': 32, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.bias_width': 32, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.weight_width': 16, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.bias_width': 32, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense.weight_width': 8, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense.bias_width': 32, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 8, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'integer', 'classifier.data_in_width': 32, 'classifier.data_in_frac_width': 2, 'classifier.weight_width': 16, 'classifier.weight_frac_width': 4, 'classifier.bias_width': 16, 'classifier.bias_frac_width': 8}. Best is trial 7 with value: 0.87692.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:49:38,555]\u001b[0m A new study created in memory with name: tutorial6_prec_integer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 best: 0.87692\n",
      "Saved: /workspace/labs/lab3/outputs/tutorial6_task1_integer_layerwise_running_best.png\n",
      "\n",
      "=== Task 2: Compare precision families ===\n",
      "\n",
      "-- Running study for precision: integer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.362300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:52:22,559]\u001b[0m Trial 0 finished with value: 0.86916 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 8, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 16, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.value.bias_width': 32, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 32, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense.bias_width': 8, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 4, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 32, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.output.dense.weight_width': 8, 'bert.encoder.layer.0.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense.weight_width': 32, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 16, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 16, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense.weight_width': 32, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense.bias_width': 8, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 0 with value: 0.86916.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.350100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:55:17,348]\u001b[0m Trial 1 finished with value: 0.86008 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 16, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 16, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.weight_width': 32, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 32, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense.bias_width': 8, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 16, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.output.dense.weight_width': 32, 'bert.encoder.layer.0.output.dense.weight_frac_width': 8, 'bert.encoder.layer.0.output.dense.bias_width': 8, 'bert.encoder.layer.0.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.weight_width': 16, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.bias_width': 32, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 32, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.weight_width': 8, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 8, 'bert.encoder.layer.1.attention.self.key.bias_width': 16, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 8, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 16, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense.bias_width': 8, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 16, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.1.output.dense.weight_width': 8, 'bert.encoder.layer.1.output.dense.weight_frac_width': 2, 'bert.encoder.layer.1.output.dense.bias_width': 8, 'bert.encoder.layer.1.output.dense.bias_frac_width': 8, 'classifier_type': 'fp'}. Best is trial 0 with value: 0.86916.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.360700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 03:57:48,570]\u001b[0m Trial 2 finished with value: 0.87512 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 8, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.weight_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 8, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.weight_width': 32, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.bias_width': 32, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 2, 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 16, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 32, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'integer', 'classifier.data_in_width': 32, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 16, 'classifier.weight_frac_width': 4, 'classifier.bias_width': 16, 'classifier.bias_frac_width': 8}. Best is trial 2 with value: 0.87512.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.336300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:00:34,724]\u001b[0m Trial 3 finished with value: 0.87536 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.self.value.weight_width': 8, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.value.bias_width': 16, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense.bias_width': 8, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 16, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.output.dense.weight_width': 8, 'bert.encoder.layer.0.output.dense.weight_frac_width': 2, 'bert.encoder.layer.0.output.dense.bias_width': 8, 'bert.encoder.layer.0.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.bias_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 16, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.key.weight_width': 16, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.bias_width': 16, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 32, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.weight_width': 32, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.bias_width': 16, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 3 with value: 0.87536.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:03:09,571]\u001b[0m Trial 4 finished with value: 0.86692 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_width': 32, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 16, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_width': 16, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 16, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.value.bias_width': 32, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 16, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.key.weight_width': 32, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.key.bias_width': 8, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 32, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 8, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'integer', 'classifier.data_in_width': 16, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 32, 'classifier.weight_frac_width': 2, 'classifier.bias_width': 32, 'classifier.bias_frac_width': 4}. Best is trial 3 with value: 0.87536.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:06:04,644]\u001b[0m Trial 5 finished with value: 0.5 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 8, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 32, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.bias_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 8, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.weight_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.bias_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 16, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 16, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.value.bias_width': 32, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense_type': 'integer', 'bert.encoder.layer.0.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.0.attention.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense.weight_width': 32, 'bert.encoder.layer.0.attention.output.dense.weight_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense.bias_width': 8, 'bert.encoder.layer.0.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense.bias_width': 32, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 32, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.output.dense.weight_width': 32, 'bert.encoder.layer.0.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.weight_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 16, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.1.output.dense.weight_width': 32, 'bert.encoder.layer.1.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.output.dense.bias_width': 16, 'bert.encoder.layer.1.output.dense.bias_frac_width': 8, 'classifier_type': 'integer', 'classifier.data_in_width': 8, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 32, 'classifier.weight_frac_width': 2, 'classifier.bias_width': 32, 'classifier.bias_frac_width': 2}. Best is trial 3 with value: 0.87536.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:08:33,568]\u001b[0m Trial 6 finished with value: 0.87228 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.bias_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'integer', 'bert.encoder.layer.0.attention.output.dense.data_in_width': 16, 'bert.encoder.layer.0.attention.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense.weight_width': 8, 'bert.encoder.layer.0.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense.bias_width': 32, 'bert.encoder.layer.0.attention.output.dense.bias_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'integer', 'classifier.data_in_width': 16, 'classifier.data_in_frac_width': 4, 'classifier.weight_width': 8, 'classifier.weight_frac_width': 8, 'classifier.bias_width': 32, 'classifier.bias_frac_width': 2}. Best is trial 3 with value: 0.87536.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.337900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:11:15,497]\u001b[0m Trial 7 finished with value: 0.8768 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 32, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_width': 16, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 8, 'bert.encoder.layer.0.attention.self.query.bias_width': 32, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 32, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 32, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 16, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.output.dense.weight_width': 16, 'bert.encoder.layer.0.output.dense.weight_frac_width': 2, 'bert.encoder.layer.0.output.dense.bias_width': 16, 'bert.encoder.layer.0.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 8, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.weight_width': 32, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense.weight_width': 32, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.bias_width': 16, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 7 with value: 0.8768.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.368500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:14:08,425]\u001b[0m Trial 8 finished with value: 0.86032 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 8, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.bias_width': 16, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 8, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 32, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 2, 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 8, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.weight_width': 8, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.value.bias_width': 16, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 4, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 8, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.0.output.dense.weight_width': 16, 'bert.encoder.layer.0.output.dense.weight_frac_width': 4, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 16, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_width': 16, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 16, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'integer', 'bert.encoder.layer.1.attention.output.dense.data_in_width': 32, 'bert.encoder.layer.1.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense.weight_width': 32, 'bert.encoder.layer.1.attention.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.attention.output.dense.bias_width': 32, 'bert.encoder.layer.1.attention.output.dense.bias_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 32, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.output.dense.weight_width': 32, 'bert.encoder.layer.1.output.dense.weight_frac_width': 4, 'bert.encoder.layer.1.output.dense.bias_width': 16, 'bert.encoder.layer.1.output.dense.bias_frac_width': 2, 'classifier_type': 'fp'}. Best is trial 7 with value: 0.8768.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.291100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.341600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:16:56,316]\u001b[0m Trial 9 finished with value: 0.87476 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'integer', 'bert.encoder.layer.0.attention.self.query.data_in_width': 16, 'bert.encoder.layer.0.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.query.weight_width': 8, 'bert.encoder.layer.0.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.query.bias_width': 32, 'bert.encoder.layer.0.attention.self.query.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 16, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.self.key.weight_width': 32, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.bias_width': 32, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 4, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 8, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.output.dense.weight_width': 16, 'bert.encoder.layer.0.output.dense.weight_frac_width': 8, 'bert.encoder.layer.0.output.dense.bias_width': 32, 'bert.encoder.layer.0.output.dense.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'integer', 'bert.encoder.layer.1.output.dense.data_in_width': 32, 'bert.encoder.layer.1.output.dense.data_in_frac_width': 2, 'bert.encoder.layer.1.output.dense.weight_width': 16, 'bert.encoder.layer.1.output.dense.weight_frac_width': 2, 'bert.encoder.layer.1.output.dense.bias_width': 16, 'bert.encoder.layer.1.output.dense.bias_frac_width': 4, 'classifier_type': 'fp'}. Best is trial 7 with value: 0.8768.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.328500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:19:28,948]\u001b[0m Trial 10 finished with value: 0.8606 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'integer', 'bert.encoder.layer.0.attention.self.key.data_in_width': 32, 'bert.encoder.layer.0.attention.self.key.data_in_frac_width': 2, 'bert.encoder.layer.0.attention.self.key.weight_width': 16, 'bert.encoder.layer.0.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.0.attention.self.key.bias_width': 8, 'bert.encoder.layer.0.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'integer', 'bert.encoder.layer.0.attention.output.dense.data_in_width': 32, 'bert.encoder.layer.0.attention.output.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense.weight_width': 16, 'bert.encoder.layer.0.attention.output.dense.weight_frac_width': 2, 'bert.encoder.layer.0.attention.output.dense.bias_width': 16, 'bert.encoder.layer.0.attention.output.dense.bias_frac_width': 4, 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 8, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.weight_width': 32, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 2, 'bert.encoder.layer.1.attention.self.query.bias_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 4, 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 8, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.1.intermediate.dense.weight_width': 16, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.bias_width': 16, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 4, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 7 with value: 0.8768.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.407900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.311500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-03 04:22:12,984]\u001b[0m Trial 11 finished with value: 0.86636 and parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'integer', 'bert.encoder.layer.0.attention.self.value.data_in_width': 32, 'bert.encoder.layer.0.attention.self.value.data_in_frac_width': 4, 'bert.encoder.layer.0.attention.self.value.weight_width': 8, 'bert.encoder.layer.0.attention.self.value.weight_frac_width': 2, 'bert.encoder.layer.0.attention.self.value.bias_width': 16, 'bert.encoder.layer.0.attention.self.value.bias_frac_width': 8, 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'integer', 'bert.encoder.layer.0.intermediate.dense.data_in_width': 16, 'bert.encoder.layer.0.intermediate.dense.data_in_frac_width': 8, 'bert.encoder.layer.0.intermediate.dense.weight_width': 32, 'bert.encoder.layer.0.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.0.intermediate.dense.bias_width': 16, 'bert.encoder.layer.0.intermediate.dense.bias_frac_width': 8, 'bert.encoder.layer.0.output.dense_type': 'integer', 'bert.encoder.layer.0.output.dense.data_in_width': 16, 'bert.encoder.layer.0.output.dense.data_in_frac_width': 4, 'bert.encoder.layer.0.output.dense.weight_width': 8, 'bert.encoder.layer.0.output.dense.weight_frac_width': 2, 'bert.encoder.layer.0.output.dense.bias_width': 16, 'bert.encoder.layer.0.output.dense.bias_frac_width': 4, 'bert.encoder.layer.1.attention.self.query_type': 'integer', 'bert.encoder.layer.1.attention.self.query.data_in_width': 32, 'bert.encoder.layer.1.attention.self.query.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.self.query.weight_width': 32, 'bert.encoder.layer.1.attention.self.query.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.query.bias_width': 8, 'bert.encoder.layer.1.attention.self.query.bias_frac_width': 2, 'bert.encoder.layer.1.attention.self.key_type': 'integer', 'bert.encoder.layer.1.attention.self.key.data_in_width': 16, 'bert.encoder.layer.1.attention.self.key.data_in_frac_width': 8, 'bert.encoder.layer.1.attention.self.key.weight_width': 16, 'bert.encoder.layer.1.attention.self.key.weight_frac_width': 4, 'bert.encoder.layer.1.attention.self.key.bias_width': 16, 'bert.encoder.layer.1.attention.self.key.bias_frac_width': 8, 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'integer', 'bert.encoder.layer.1.intermediate.dense.data_in_width': 32, 'bert.encoder.layer.1.intermediate.dense.data_in_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.weight_width': 32, 'bert.encoder.layer.1.intermediate.dense.weight_frac_width': 2, 'bert.encoder.layer.1.intermediate.dense.bias_width': 16, 'bert.encoder.layer.1.intermediate.dense.bias_frac_width': 2, 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'}. Best is trial 7 with value: 0.8768.\u001b[0m\n",
      "\u001b[32m[I 2026-02-03 04:22:12,986]\u001b[0m A new study created in memory with name: tutorial6_prec_minifloat_ieee\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best for integer: 0.8768\n",
      "\n",
      "-- Running study for precision: minifloat_ieee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:14,268]\u001b[0m Trial 0 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'minifloat_ieee'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 395, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:14,270]\u001b[0m Trial 0 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:15,570]\u001b[0m Trial 1 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'minifloat_ieee', 'classifier_type': 'fp'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 395, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:15,572]\u001b[0m Trial 1 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:16,726]\u001b[0m Trial 2 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 395, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:16,728]\u001b[0m Trial 2 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:17,932]\u001b[0m Trial 3 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.output.dense_type': 'minifloat_ieee', 'classifier_type': 'fp'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 627, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 639, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 539, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:17,934]\u001b[0m Trial 3 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:19,084]\u001b[0m Trial 4 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 524, in forward\n",
      "    attention_output = self.output(self_outputs[0], hidden_states)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 466, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:19,086]\u001b[0m Trial 4 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:20,270]\u001b[0m Trial 5 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.intermediate.dense_type': 'fp', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'minifloat_ieee'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 409, in forward\n",
      "    value_layer = self.transpose_for_scores(self.value(current_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:20,272]\u001b[0m Trial 5 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:21,987]\u001b[0m Trial 6 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.value_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'minifloat_ieee'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 408, in forward\n",
      "    key_layer = self.transpose_for_scores(self.key(current_states))\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:21,989]\u001b[0m Trial 6 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:23,173]\u001b[0m Trial 7 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 627, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 639, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 539, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:23,175]\u001b[0m Trial 7 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:24,326]\u001b[0m Trial 8 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'minifloat_ieee'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 395, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:24,328]\u001b[0m Trial 8 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:25,527]\u001b[0m Trial 9 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.value_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'fp', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'fp', 'bert.encoder.layer.1.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.output.dense_type': 'minifloat_ieee', 'classifier_type': 'minifloat_ieee'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 395, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:25,529]\u001b[0m Trial 9 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:26,731]\u001b[0m Trial 10 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'fp', 'bert.encoder.layer.0.attention.self.key_type': 'fp', 'bert.encoder.layer.0.attention.self.value_type': 'fp', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'fp', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'minifloat_ieee'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 627, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 639, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 539, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:26,733]\u001b[0m Trial 10 failed with value None.\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[33m[W 2026-02-03 04:22:27,930]\u001b[0m Trial 11 failed with parameters: {'bert.encoder.layer.0.attention.self.query_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.self.value_type': 'minifloat_ieee', 'bert.encoder.layer.0.attention.output.dense_type': 'fp', 'bert.encoder.layer.0.intermediate.dense_type': 'minifloat_ieee', 'bert.encoder.layer.0.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.self.query_type': 'fp', 'bert.encoder.layer.1.attention.self.key_type': 'minifloat_ieee', 'bert.encoder.layer.1.attention.output.dense_type': 'minifloat_ieee', 'bert.encoder.layer.1.intermediate.dense_type': 'fp', 'bert.encoder.layer.1.output.dense_type': 'fp', 'classifier_type': 'fp'} because of the following error: KeyError('weight_width').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 206, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3424/1590106815.py\", line 275, in objective\n",
      "    trainer.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1675, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 1144, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 585, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "                             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 515, in forward\n",
      "    self_outputs = self.self(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\", line 395, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/modules/linear.py\", line 171, in forward\n",
      "    return linearMinifloatIEEE(x, self.weight, self.bias, self.config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/src/chop/nn/quantized/functional/linear.py\", line 131, in linearMinifloatIEEE\n",
      "    config[\"weight_width\"],\n",
      "    ~~~~~~^^^^^^^^^^^^^^^^\n",
      "KeyError: 'weight_width'\n",
      "\u001b[33m[W 2026-02-03 04:22:27,931]\u001b[0m Trial 11 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 312\u001b[39m\n\u001b[32m    310\u001b[39m st.optimize(make_objective_for_precision(prec_name), n_trials=n_trials, catch=(\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m))\n\u001b[32m    311\u001b[39m studies[prec_name] = st\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_trial\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m st.best_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    313\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprec_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(st.best_value)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py:156\u001b[39m, in \u001b[36mStudy.best_trial\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbest_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> FrozenTrial:\n\u001b[32m    141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the best trial in the study.\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m \n\u001b[32m    155\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py:308\u001b[39m, in \u001b[36mStudy._get_best_trial\u001b[39m\u001b[34m(self, deepcopy)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_multi_objective():\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m best_trial = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# violation value in the best-valued trial.\u001b[39;00m\n\u001b[32m    313\u001b[39m constraints = best_trial.system_attrs.get(_CONSTRAINTS_KEY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/optuna/storages/_in_memory.py:251\u001b[39m, in \u001b[36mInMemoryStorage.get_best_trial\u001b[39m\u001b[34m(self, study_id)\u001b[39m\n\u001b[32m    248\u001b[39m best_trial_id = \u001b[38;5;28mself\u001b[39m._studies[study_id].best_trial_id\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo trials are completed yet.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._studies[study_id].directions) > \u001b[32m1\u001b[39m:\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from chop.tools import get_tokenized_dataset, get_trainer\n",
    "from chop.tools.utils import deepsetattr\n",
    "import chop.passes as passes\n",
    "from chop import MaseGraph\n",
    "from chop.pipelines import CompressionPipeline\n",
    "\n",
    "from chop.nn.quantized.modules.linear import (\n",
    "    LinearInteger,\n",
    "    LinearMinifloatDenorm,\n",
    "    LinearMinifloatIEEE,\n",
    "    LinearLog,\n",
    "    LinearBlockFP,\n",
    "    LinearBlockMinifloat,\n",
    "    LinearBlockLog,\n",
    "    LinearBinary,\n",
    "    LinearBinaryScaling,\n",
    "    LinearBinaryResidualSign,\n",
    ")\n",
    "\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "lab3_out_dir = Path(\"/workspace/labs/lab3/outputs\")\n",
    "lab3_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lab2_out_dir = Path(\"/workspace/labs/lab2/outputs\")\n",
    "\n",
    "n_trials = 12\n",
    "search_epochs = 1\n",
    "\n",
    "pruning_sparsity = 0.5\n",
    "post_compress_epochs = 1\n",
    "\n",
    "HF_INPUT_NAMES = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def cleanup(*objs):\n",
    "    for o in objs:\n",
    "        try:\n",
    "            del o\n",
    "        except Exception:\n",
    "            pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def running_best(study: optuna.Study):\n",
    "    best = float(\"-inf\")\n",
    "    xs, ys = [], []\n",
    "    for t in sorted(study.trials, key=lambda x: x.number):\n",
    "        if t.value is None:\n",
    "            continue\n",
    "        best = max(best, float(t.value))\n",
    "        xs.append(len(xs) + 1)\n",
    "        ys.append(best)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def json_safe(obj):\n",
    "    if isinstance(obj, type):\n",
    "        return f\"{obj.__module__}.{obj.__qualname__}\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: json_safe(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [json_safe(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")\n",
    "\n",
    "\n",
    "base_model_path = lab2_out_dir / \"tutorial_5_best_model.pkl\"\n",
    "if base_model_path.exists():\n",
    "    import dill\n",
    "\n",
    "    with open(base_model_path, \"rb\") as f:\n",
    "        base_model = dill.load(f)\n",
    "else:\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "try:\n",
    "    base_model.config.problem_type = \"single_label_classification\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "INT_WIDTH_CHOICES = [8, 16, 32]\n",
    "INT_FRAC_CHOICES = [2, 4, 8]\n",
    "\n",
    "\n",
    "def make_integer_config_for_layer(trial: optuna.Trial, layer_name: str):\n",
    "    din_w = trial.suggest_categorical(f\"{layer_name}.data_in_width\", INT_WIDTH_CHOICES)\n",
    "    din_fw = trial.suggest_categorical(f\"{layer_name}.data_in_frac_width\", INT_FRAC_CHOICES)\n",
    "    w_w = trial.suggest_categorical(f\"{layer_name}.weight_width\", INT_WIDTH_CHOICES)\n",
    "    w_fw = trial.suggest_categorical(f\"{layer_name}.weight_frac_width\", INT_FRAC_CHOICES)\n",
    "    b_w = trial.suggest_categorical(f\"{layer_name}.bias_width\", INT_WIDTH_CHOICES)\n",
    "    b_fw = trial.suggest_categorical(f\"{layer_name}.bias_frac_width\", INT_FRAC_CHOICES)\n",
    "    return {\n",
    "        \"data_in_width\": din_w,\n",
    "        \"data_in_frac_width\": din_fw,\n",
    "        \"weight_width\": w_w,\n",
    "        \"weight_frac_width\": w_fw,\n",
    "        \"bias_width\": b_w,\n",
    "        \"bias_frac_width\": b_fw,\n",
    "    }\n",
    "\n",
    "\n",
    "def config_for_precision(prec_name: str, trial: optuna.Trial | None, layer_name: str):\n",
    "    if prec_name == \"integer\":\n",
    "        return make_integer_config_for_layer(trial, layer_name)\n",
    "\n",
    "    if prec_name in (\"minifloat_ieee\", \"minifloat_denorm\", \"blockminifloat\"):\n",
    "        return {\n",
    "            \"data_in_exponent_width\": 5,\n",
    "            \"data_in_mantissa_width\": 3,\n",
    "            \"data_in_exponent_bias\": 15,\n",
    "            \"weight_exponent_width\": 5,\n",
    "            \"weight_mantissa_width\": 3,\n",
    "            \"weight_exponent_bias\": 15,\n",
    "            \"bias_exponent_width\": 5,\n",
    "            \"bias_mantissa_width\": 3,\n",
    "            \"bias_exponent_bias\": 15,\n",
    "        }\n",
    "\n",
    "    if prec_name in (\"log\", \"blocklog\"):\n",
    "        return {\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "            \"block_size\": 32,\n",
    "        }\n",
    "\n",
    "    if prec_name in (\"blockfp\",):\n",
    "        return {\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "            \"block_size\": 32,\n",
    "        }\n",
    "\n",
    "    if prec_name in (\"binary\", \"binary_scaling\", \"binary_residual_sign\"):\n",
    "        return {\n",
    "            \"data_in_levels\": 2,\n",
    "            \"weight_levels\": 2,\n",
    "            \"bias_levels\": 2,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"data_in_width\": 8,\n",
    "        \"data_in_frac_width\": 4,\n",
    "        \"weight_width\": 8,\n",
    "        \"weight_frac_width\": 4,\n",
    "        \"bias_width\": 8,\n",
    "        \"bias_frac_width\": 4,\n",
    "    }\n",
    "\n",
    "\n",
    "PRECISIONS = [\n",
    "    (\"integer\", LinearInteger),\n",
    "    (\"minifloat_ieee\", LinearMinifloatIEEE),\n",
    "    (\"minifloat_denorm\", LinearMinifloatDenorm),\n",
    "    (\"log\", LinearLog),\n",
    "    (\"blockfp\", LinearBlockFP),\n",
    "    (\"blockminifloat\", LinearBlockMinifloat),\n",
    "    (\"blocklog\", LinearBlockLog),\n",
    "    (\"binary\", LinearBinary),\n",
    "    (\"binary_scaling\", LinearBinaryScaling),\n",
    "    (\"binary_residual_sign\", LinearBinaryResidualSign),\n",
    "]\n",
    "PREC_BY_NAME = {n: c for n, c in PRECISIONS}\n",
    "\n",
    "\n",
    "def build_layer_kwargs(prec_name: str, trial: optuna.Trial | None, layer_cls, name: str, layer: nn.Linear):\n",
    "    kwargs = {\n",
    "        \"in_features\": layer.in_features,\n",
    "        \"out_features\": layer.out_features,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import inspect\n",
    "\n",
    "        params = inspect.signature(layer_cls.__init__).parameters\n",
    "    except Exception:\n",
    "        params = {}\n",
    "\n",
    "    if \"bias\" in params:\n",
    "        kwargs[\"bias\"] = (layer.bias is not None)\n",
    "\n",
    "    if \"config\" in params:\n",
    "        cfg = config_for_precision(prec_name, trial, name)\n",
    "        kwargs[\"config\"] = cfg\n",
    "\n",
    "    if layer_cls is LinearInteger and \"config\" not in params and trial is not None:\n",
    "        cfg = make_integer_config_for_layer(trial, name)\n",
    "        for k, v in cfg.items():\n",
    "            if k in params:\n",
    "                kwargs[k] = v\n",
    "\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def construct_model_for_precision(trial: optuna.Trial, prec_name: str):\n",
    "    prec_cls = PREC_BY_NAME[prec_name]\n",
    "    trial_model = deepcopy(base_model)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            choice = trial.suggest_categorical(f\"{name}_type\", (\"fp\", prec_name))\n",
    "            if choice == \"fp\":\n",
    "                continue\n",
    "\n",
    "            kwargs = build_layer_kwargs(prec_name, trial, prec_cls, name, layer)\n",
    "            new_layer = prec_cls(**kwargs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    if hasattr(new_layer, \"weight\") and hasattr(layer, \"weight\") and new_layer.weight.shape == layer.weight.shape:\n",
    "                        new_layer.weight.copy_(layer.weight)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    if layer.bias is not None and hasattr(new_layer, \"bias\") and new_layer.bias is not None:\n",
    "                        if getattr(new_layer.bias, \"shape\", None) == layer.bias.shape:\n",
    "                            new_layer.bias.copy_(layer.bias)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            deepsetattr(trial_model, name, new_layer)\n",
    "\n",
    "    try:\n",
    "        trial_model.config.problem_type = \"single_label_classification\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return trial_model\n",
    "\n",
    "\n",
    "def make_objective_for_precision(prec_name: str):\n",
    "    def objective(trial: optuna.Trial):\n",
    "        trainer = None\n",
    "        model = None\n",
    "        try:\n",
    "            model = construct_model_for_precision(trial, prec_name).to(device)\n",
    "            trainer = get_trainer(\n",
    "                model=model,\n",
    "                tokenized_dataset=dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                evaluate_metric=\"accuracy\",\n",
    "                num_train_epochs=search_epochs,\n",
    "            )\n",
    "            trainer.train()\n",
    "            ev = trainer.evaluate()\n",
    "            return float(ev[\"eval_accuracy\"])\n",
    "        finally:\n",
    "            cleanup(trainer, model)\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "print(\"=== Task 1: Integer per-layer width/frac search ===\")\n",
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "study_int = optuna.create_study(direction=\"maximize\", sampler=sampler, study_name=\"tutorial6_integer_layerwise_widthfrac\")\n",
    "study_int.optimize(make_objective_for_precision(\"integer\"), n_trials=n_trials)\n",
    "\n",
    "x1, y1 = running_best(study_int)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x1, y1, marker=\"o\")\n",
    "plt.xlabel(\"Number of trials\")\n",
    "plt.ylabel(\"Running best accuracy\")\n",
    "plt.title(\"Task 1: Integer layer-wise widths (Running Best)\")\n",
    "plt.grid(True)\n",
    "task1_plot = lab3_out_dir / \"tutorial6_task1_integer_layerwise_running_best.png\"\n",
    "plt.savefig(task1_plot)\n",
    "plt.show()\n",
    "\n",
    "print(\"Task 1 best:\", float(study_int.best_value))\n",
    "print(\"Saved:\", task1_plot)\n",
    "\n",
    "\n",
    "print(\"\\n=== Task 2: Compare precision families ===\")\n",
    "studies = {}\n",
    "for prec_name, _ in PRECISIONS:\n",
    "    print(f\"\\n-- Running study for precision: {prec_name}\")\n",
    "    sampler = optuna.samplers.TPESampler(seed=0)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=sampler, study_name=f\"tutorial6_prec_{prec_name}\")\n",
    "    st.optimize(make_objective_for_precision(prec_name), n_trials=n_trials, catch=(KeyError, AssertionError, RuntimeError, TypeError))\n",
    "    studies[prec_name] = st\n",
    "    if st.best_trial is not None and st.best_value is not None:\n",
    "        print(f\"Best for {prec_name}: {float(st.best_value):.4f}\")\n",
    "    else:\n",
    "        print(f\"Best for {prec_name}: None (all trials failed)\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for prec_name, st in studies.items():\n",
    "    xs, ys = running_best(st)\n",
    "    if xs:\n",
    "        plt.plot(xs, ys, label=prec_name)\n",
    "plt.xlabel(\"Number of trials\")\n",
    "plt.ylabel(\"Running best accuracy\")\n",
    "plt.title(\"Task 2: Running Best Accuracy per Precision\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "task2_plot = lab3_out_dir / \"tutorial6_task2_all_precisions_running_best.png\"\n",
    "plt.savefig(task2_plot)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\", task2_plot)\n",
    "\n",
    "\n",
    "summary = {\n",
    "    \"task1_integer_layerwise\": {\n",
    "        \"best_value\": float(study_int.best_value),\n",
    "        \"best_params\": json_safe(study_int.best_params),\n",
    "        \"n_trials\": len(study_int.trials),\n",
    "        \"search_epochs\": search_epochs,\n",
    "        \"sampler\": \"TPESampler\",\n",
    "        \"plot\": str(task1_plot),\n",
    "    },\n",
    "    \"task2_all_precisions\": {\n",
    "        \"n_trials_each\": n_trials,\n",
    "        \"search_epochs\": search_epochs,\n",
    "        \"sampler\": \"TPESampler\",\n",
    "        \"plot\": str(task2_plot),\n",
    "        \"best_by_precision\": {\n",
    "            name: {\n",
    "                \"best_value\": (float(st.best_value) if st.best_trial is not None and st.best_value is not None else None),\n",
    "                \"best_params\": (json_safe(st.best_params) if st.best_trial is not None and st.best_value is not None else None),\n",
    "            }\n",
    "            for name, st in studies.items()\n",
    "        },\n",
    "    },\n",
    "}\n",
    "summary_path = lab3_out_dir / \"tutorial6_impl_tasks_summary.json\"\n",
    "with summary_path.open(\"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Saved summary JSON:\", summary_path)\n",
    "\n",
    "\n",
    "BASE_QUANTIZATION_CONFIG = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "BASE_PRUNING_CONFIG = {\n",
    "    \"weight\": {\"sparsity\": pruning_sparsity, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "    \"activation\": {\"sparsity\": pruning_sparsity, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "}\n",
    "\n",
    "\n",
    "def compress_with_mase(model_cpu: nn.Module):\n",
    "    mg = MaseGraph(model_cpu, hf_input_names=HF_INPUT_NAMES)\n",
    "    mg, _ = passes.init_metadata_analysis_pass(mg)\n",
    "    mg, _ = passes.add_common_metadata_analysis_pass(mg)\n",
    "\n",
    "    qcfg = deepcopy(BASE_QUANTIZATION_CONFIG)\n",
    "    pcfg = deepcopy(BASE_PRUNING_CONFIG)\n",
    "\n",
    "    pipe = CompressionPipeline()\n",
    "    mg, _ = pipe(\n",
    "        mg,\n",
    "        pass_args={\n",
    "            \"quantize_transform_pass\": qcfg,\n",
    "            \"prune_transform_pass\": pcfg,\n",
    "        },\n",
    "    )\n",
    "    return mg\n",
    "\n",
    "\n",
    "print(\"\\n=== Optional: Compression on Task 1 best model ===\")\n",
    "fixed_trial = optuna.trial.FixedTrial(study_int.best_params)\n",
    "best_model_task1 = construct_model_for_precision(fixed_trial, \"integer\")\n",
    "\n",
    "mg_a = None\n",
    "trainer_a = None\n",
    "try:\n",
    "    mg_a = compress_with_mase(deepcopy(best_model_task1).to(\"cpu\"))\n",
    "    model_a = mg_a.model.to(device)\n",
    "    trainer_a = get_trainer(\n",
    "        model=model_a,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=0,\n",
    "    )\n",
    "    ev_a = trainer_a.evaluate()\n",
    "    acc_a = float(ev_a[\"eval_accuracy\"])\n",
    "    print(f\"Compressed (no post-train) eval_accuracy: {acc_a:.4f}\")\n",
    "finally:\n",
    "    cleanup(trainer_a, mg_a)\n",
    "\n",
    "\n",
    "mg_b = None\n",
    "trainer_b = None\n",
    "try:\n",
    "    mg_b = compress_with_mase(deepcopy(best_model_task1).to(\"cpu\"))\n",
    "    model_b = mg_b.model.to(device)\n",
    "    trainer_b = get_trainer(\n",
    "        model=model_b,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=post_compress_epochs,\n",
    "    )\n",
    "    trainer_b.train()\n",
    "    ev_b = trainer_b.evaluate()\n",
    "    acc_b = float(ev_b[\"eval_accuracy\"])\n",
    "    print(f\"Compressed (+ post-train {post_compress_epochs} epoch) eval_accuracy: {acc_b:.4f}\")\n",
    "finally:\n",
    "    cleanup(trainer_b, mg_b)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mase-dev)",
   "language": "python",
   "name": "mase-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
