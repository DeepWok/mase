{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are starting from scratch, you can load the Bert checkpoint directly from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have previously ran the tutorial on Neural Architecture Search (NAS), run the following cell to import the best model obtained from the search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "lab2_out_dir = Path(\"/workspace/labs/lab2/outputs\")\n",
    "lab3_out_dir = Path(\"/workspace/labs/lab3/outputs\")\n",
    "\n",
    "with open(f\"{lab2_out_dir}/tutorial_5_best_model.pkl\", \"rb\") as f:\n",
    "    base_model = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from chop.nn.quantized.modules.linear import (\n",
    "    LinearInteger,\n",
    "    LinearMinifloatDenorm,\n",
    "    LinearMinifloatIEEE,\n",
    "    LinearLog,\n",
    "    LinearBlockFP,\n",
    "    LinearBlockMinifloat,\n",
    "    LinearBlockLog,\n",
    "    LinearBinary,\n",
    "    LinearBinaryScaling,\n",
    "    LinearBinaryResidualSign,\n",
    ")\n",
    "\n",
    "search_space = {\n",
    "    \"linear_layer_choices\": [\n",
    "        torch.nn.Linear,\n",
    "        LinearInteger,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_categorical` function, which triggers the chosen sampler to choose a layer type. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools.utils import deepsetattr\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "\n",
    "    # Fetch the model\n",
    "    trial_model = deepcopy(base_model)\n",
    "\n",
    "    # Quantize layers according to optuna suggestions\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == torch.nn.Linear:\n",
    "                continue\n",
    "\n",
    "            kwargs = {\n",
    "                \"in_features\": layer.in_features,\n",
    "                \"out_features\": layer.out_features,\n",
    "            }\n",
    "\n",
    "            # If the chosen layer is integer, define the low precision config\n",
    "            if new_layer_cls == LinearInteger:\n",
    "                kwargs[\"config\"] = {\n",
    "                    \"data_in_width\": 8,\n",
    "                    \"data_in_frac_width\": 4,\n",
    "                    \"weight_width\": 8,\n",
    "                    \"weight_frac_width\": 4,\n",
    "                    \"bias_width\": 8,\n",
    "                    \"bias_frac_width\": 4,\n",
    "                }\n",
    "            # elif... (other precisions)\n",
    "\n",
    "            # Create the new layer (copy the weights)\n",
    "            new_layer = new_layer_cls(**kwargs)\n",
    "            new_layer.weight.data = layer.weight.data\n",
    "\n",
    "            # Replace the layer in the model\n",
    "            deepsetattr(trial_model, name, new_layer)\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "import random\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = RandomSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 19:37:21,434]\u001b[0m A new study created in memory with name: bert-tiny-nas-study\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.357500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 19:39:54,034]\u001b[0m Trial 0 finished with value: 0.87656 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 0 with value: 0.87656.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n",
      "\u001b[32m[I 2026-02-02 10:26:32,795]\u001b[0m A new study created in memory with name: tutorial6_mixed_precision_search\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:29:12,285]\u001b[0m Trial 0 finished with value: 0.87464 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 0 with value: 0.87464.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.364100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:32:07,774]\u001b[0m Trial 1 finished with value: 0.87484 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 1 with value: 0.87484.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.322900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.360900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:34:37,748]\u001b[0m Trial 2 finished with value: 0.8762 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 2 with value: 0.8762.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.355100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:37:28,285]\u001b[0m Trial 3 finished with value: 0.87484 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 2 with value: 0.8762.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.366300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:40:02,088]\u001b[0m Trial 4 finished with value: 0.87388 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 2 with value: 0.8762.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:42:37,419]\u001b[0m Trial 5 finished with value: 0.8756 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 2 with value: 0.8762.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.300500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.359600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:45:22,603]\u001b[0m Trial 6 finished with value: 0.8766 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 6 with value: 0.8766.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.363200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:47:54,772]\u001b[0m Trial 7 finished with value: 0.87388 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 6 with value: 0.8766.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.310300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:50:32,116]\u001b[0m Trial 8 finished with value: 0.87608 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 6 with value: 0.8766.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.358700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:53:19,074]\u001b[0m Trial 9 finished with value: 0.87368 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 6 with value: 0.8766.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.364400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:56:12,215]\u001b[0m Trial 10 finished with value: 0.8742 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'classifier_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>}. Best is trial 6 with value: 0.8766.\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.366600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-02 10:58:51,832]\u001b[0m Trial 11 finished with value: 0.87564 and parameters: {'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.quantized.modules.linear.LinearInteger'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'classifier_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 6 with value: 0.8766.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  optuna_warn(message)\n",
      "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.quantized.modules.linear.LinearInteger'> which is of type type.\n",
      "  optuna_warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutorial 6 search done.\n",
      "Best accuracy (study.best_value): 0.8766\n",
      "Plot saved to: /workspace/labs/lab2/outputs/tutorial6_running_best.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model (as searched) eval_accuracy: 0.8246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.4737, -1.2851,  0.7269,  ..., -1.3262, -0.0807,  0.7324],\n",
      "         [-1.0127, -1.2385,  1.0493,  ..., -0.2675,  0.9101,  0.6033],\n",
      "         ...,\n",
      "         [-0.8877,  0.2255, -0.0285,  ..., -0.8598, -1.6034,  0.4798],\n",
      "         [-1.3354, -0.2770, -0.4315,  ...,  0.1540, -0.0039,  0.0438],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]],\n",
      "\n",
      "        [[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.3325, -2.1907,  0.4309,  ..., -0.4327,  0.8238,  0.6018],\n",
      "         [-2.2631, -0.5424,  1.6860,  ..., -0.1677,  0.0543, -1.1901],\n",
      "         ...,\n",
      "         [-0.6945,  0.7086, -0.0305,  ..., -0.2801, -1.4319,  1.3509],\n",
      "         [-0.9197, -0.5727,  0.8206,  ..., -0.7162,  0.1093,  0.1793],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0562, -0.1131, -0.6344,  ...,  0.7818, -0.4356, -0.4549],\n",
      "          [ 0.4613, -0.0945, -0.4052,  ...,  0.3220, -1.2746, -0.1855]],\n",
      "\n",
      "         [[ 0.4461, -0.2506, -0.2545,  ...,  0.5247, -0.2090, -0.6426],\n",
      "          [-0.0903,  0.0256,  0.0564,  ...,  0.4800, -0.8828, -0.4402]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1836,  0.3485, -0.3765,  ...,  0.2994, -0.1872, -0.3081],\n",
      "          [ 0.2338, -0.0527, -0.0200,  ...,  0.5915, -0.7918, -0.6290]],\n",
      "\n",
      "         [[ 0.2951, -0.2242,  0.0727,  ...,  0.3489, -0.2264, -0.5768],\n",
      "          [ 0.3838, -0.1085, -0.1734,  ...,  0.3543, -1.0874, -0.6689]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0497, -0.1332, -0.2219,  ...,  1.0808, -0.3719, -0.5401],\n",
      "          [ 0.3631,  0.0128, -0.6016,  ...,  0.5172, -1.2711, -0.2861]],\n",
      "\n",
      "         [[ 0.6823, -0.1618, -0.5238,  ...,  0.4329, -0.5678, -0.3586],\n",
      "          [ 0.0153,  0.0539, -0.1937,  ...,  0.3206, -1.2407, -0.4009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5027,  0.1520, -0.3113,  ...,  0.3781, -0.2348, -0.1838],\n",
      "          [ 0.1863, -0.0638, -0.0369,  ...,  0.6593, -0.8024, -0.5406]],\n",
      "\n",
      "         [[ 0.0185, -0.4286, -0.1754,  ...,  0.1903, -0.3894, -0.4808],\n",
      "          [ 0.4196, -0.0278, -0.4332,  ...,  0.3414, -1.1425, -0.8640]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0078, -0.1289,  0.4531,  ..., -0.0039,  0.1055,  0.2422],\n",
      "          [ 0.0312,  0.0703,  0.3164,  ..., -0.3633,  0.0156, -0.3281]],\n",
      "\n",
      "         [[ 0.4258,  0.0898, -0.1719,  ...,  0.1523, -0.0547, -0.1289],\n",
      "          [ 0.2305,  0.0312,  0.1250,  ..., -0.6250, -0.2617, -0.7969]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0742,  0.2031,  0.5273,  ..., -0.4180,  0.6445,  0.2031],\n",
      "          [-0.0703,  0.0625,  0.2109,  ..., -0.3906,  0.3477, -0.0625]],\n",
      "\n",
      "         [[ 0.2422, -0.1016,  0.0352,  ...,  0.2422,  0.4062,  0.2578],\n",
      "          [ 0.4883, -0.0078,  0.3945,  ..., -0.4297,  0.0625, -0.3711]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0586, -0.2344, -0.0117,  ...,  0.3438,  0.5977, -0.1641],\n",
      "          [ 0.0859,  0.3945,  0.5547,  ..., -0.3164,  0.0898,  0.0859]],\n",
      "\n",
      "         [[ 0.5430,  0.2812,  0.1641,  ..., -0.2344,  0.2344,  0.1602],\n",
      "          [-0.0312, -0.2773, -0.1641,  ..., -0.5430, -0.1641, -0.6836]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0625, -0.1836,  0.5742,  ...,  0.0781,  1.1719, -0.0703],\n",
      "          [ 0.1406, -0.0977,  0.4648,  ..., -0.1055,  0.3633, -0.2266]],\n",
      "\n",
      "         [[-0.2383, -0.6250, -0.2266,  ...,  0.0273,  0.3828,  0.2695],\n",
      "          [ 0.1211,  0.3789,  0.6211,  ..., -0.5703, -0.1406, -0.2656]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.3164,  0.3555,  0.0898,  ...,  0.2969,  0.6016,  0.4453],\n",
      "          [-0.0195,  0.0039, -0.5664,  ...,  0.1719, -0.3516, -0.6406]],\n",
      "\n",
      "         [[ 0.3477,  0.1016, -0.0938,  ...,  0.8555, -0.3516, -0.3438],\n",
      "          [-0.0664, -0.0469, -0.3672,  ..., -0.0117, -0.2969, -0.1016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0117,  0.2109,  0.1445,  ...,  0.2852, -0.1992,  0.5508],\n",
      "          [-0.2266,  0.2383,  0.3203,  ...,  0.0977, -0.0547, -0.1211]],\n",
      "\n",
      "         [[-0.0742, -0.2383, -0.0273,  ...,  1.0664,  0.0391,  0.1875],\n",
      "          [ 0.0039, -0.3242, -0.0820,  ..., -0.1797, -0.0586, -0.1680]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "\n",
      "\n",
      "        [[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.5898, -0.0859,  0.7148,  ...,  0.1797,  0.5664,  0.2578],\n",
      "          [ 0.0664,  0.2070, -0.2812,  ...,  0.1797, -0.2734, -0.5078]],\n",
      "\n",
      "         [[ 0.5938,  0.1562, -0.1602,  ...,  0.7539, -0.3164, -0.0430],\n",
      "          [-0.0820,  0.0469, -0.4141,  ...,  0.0703, -0.7539, -0.5508]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0547, -0.0273,  0.4141,  ...,  0.1367,  0.2070,  0.3906],\n",
      "          [ 0.0312, -0.3789,  0.2461,  ...,  0.1484, -0.0586, -0.3711]],\n",
      "\n",
      "         [[-0.1250, -0.0664,  0.1133,  ...,  0.3164, -0.0859,  0.1523],\n",
      "          [ 0.0859, -0.0703, -0.2383,  ...,  0.4453,  0.2031, -0.2109]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          ...,\n",
      "          [-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326]],\n",
      "\n",
      "         [[-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397],\n",
      "          ...,\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          ...,\n",
      "          [-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488]],\n",
      "\n",
      "         [[ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944],\n",
      "          ...,\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237]],\n",
      "\n",
      "         [[-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171]],\n",
      "\n",
      "         [[-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393]],\n",
      "\n",
      "         [[-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313]],\n",
      "\n",
      "         [[-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057]],\n",
      "\n",
      "         [[-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905]],\n",
      "\n",
      "         [[-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019]],\n",
      "\n",
      "         [[-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924]],\n",
      "\n",
      "         [[-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-1.1129,  1.4734, -0.9384,  ...,  0.6065, -0.6529,  1.4177],\n",
      "          [ 0.3704, -0.9319,  0.6048,  ..., -0.3328,  0.4669,  0.6202]],\n",
      "\n",
      "         [[-0.7646,  0.4657, -0.8041,  ...,  0.4373, -0.0480,  0.9552],\n",
      "          [ 0.5976, -0.5469,  0.0781,  ..., -0.4763,  0.4951,  0.2098]],\n",
      "\n",
      "         [[-0.7888,  1.1687, -0.5397,  ...,  0.5734, -0.0654,  0.2727],\n",
      "          [-0.0468, -0.4641,  0.1866,  ...,  0.0661,  0.6113,  0.0575]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4021,  0.1331, -0.2043,  ...,  0.1385, -0.0622,  0.0973],\n",
      "          [ 0.0917, -0.4530, -0.1006,  ..., -0.0134,  0.3335,  0.0101]],\n",
      "\n",
      "         [[-0.5181,  0.3185, -0.4510,  ...,  0.3619, -0.3350,  1.0064],\n",
      "          [ 0.3075, -0.5551,  0.0521,  ...,  0.2385,  0.5574,  0.1181]],\n",
      "\n",
      "         [[-0.2165,  0.3619, -0.9592,  ..., -0.0838, -0.5500,  0.9440],\n",
      "          [ 0.4487, -0.2657,  0.4180,  ..., -0.2889,  0.3050,  0.2158]]],\n",
      "\n",
      "\n",
      "        [[[-0.9999,  1.4566, -0.8748,  ...,  0.6269, -0.5588,  1.2911],\n",
      "          [ 0.3669, -0.8954,  0.6457,  ..., -0.3386,  0.3764,  0.5545]],\n",
      "\n",
      "         [[-0.4596,  0.8033, -0.5762,  ...,  0.6520,  0.1473,  0.7978],\n",
      "          [ 0.4975, -0.5188,  0.1083,  ..., -0.5528,  0.4481,  0.3905]],\n",
      "\n",
      "         [[-0.6298,  0.8572, -0.4242,  ...,  0.4010, -0.0769,  0.4064],\n",
      "          [ 0.0458, -0.4961, -0.1434,  ..., -0.2500,  0.5208,  0.0223]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2397,  0.4131, -0.1033,  ...,  0.2014, -0.0896,  0.3833],\n",
      "          [ 0.5522, -0.4919,  0.4206,  ...,  0.1394,  0.3545,  0.1110]],\n",
      "\n",
      "         [[-0.2534,  0.4085, -0.2286,  ...,  0.3257, -0.0417,  1.2924],\n",
      "          [ 0.2581, -0.2707, -0.3397,  ...,  0.2182,  0.2154,  0.2228]],\n",
      "\n",
      "         [[-0.1549,  0.3696, -0.9408,  ..., -0.0677, -0.5100,  0.8811],\n",
      "          [ 0.4627, -0.2474,  0.4603,  ..., -0.3079,  0.2522,  0.1847]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.7791, -0.0996,  0.0881,  ..., -0.1805,  0.5723, -0.1030],\n",
      "          [-0.3123,  0.1305,  0.0859,  ...,  0.2072, -0.2309, -0.2110]],\n",
      "\n",
      "         [[ 0.4952, -0.4713, -0.3020,  ..., -0.2922,  0.4016, -0.1640],\n",
      "          [-0.1513,  0.0572, -0.0277,  ..., -0.0069, -0.2822, -0.0705]],\n",
      "\n",
      "         [[ 0.0904,  0.4109, -0.1498,  ..., -0.1056,  0.2361,  0.1863],\n",
      "          [-0.0452,  0.1187,  0.0448,  ...,  0.6728,  0.3278,  0.0186]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5774, -0.2510,  0.1250,  ...,  0.0923,  0.7083, -0.2797],\n",
      "          [-0.1007,  0.5627,  0.1273,  ...,  0.6724, -0.0229,  0.1478]],\n",
      "\n",
      "         [[ 0.4430, -0.2418,  0.2823,  ..., -0.0331,  0.5006,  0.0975],\n",
      "          [-0.4843,  0.0750,  0.5932,  ...,  0.0728, -0.1789, -0.1245]],\n",
      "\n",
      "         [[ 0.4187, -0.2804,  0.2250,  ..., -0.1531,  0.4754, -0.3152],\n",
      "          [-0.4331,  0.2474, -0.0962,  ...,  0.0994, -0.4916, -0.1530]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7005,  0.0013,  0.0725,  ..., -0.2265,  0.5070, -0.0155],\n",
      "          [-0.3017,  0.1156,  0.1104,  ...,  0.1438, -0.2198, -0.1874]],\n",
      "\n",
      "         [[ 0.3386, -0.2947,  0.2519,  ..., -0.0053,  0.1211, -0.1040],\n",
      "          [-0.2981,  0.0588, -0.1370,  ..., -0.0671, -0.2978, -0.0777]],\n",
      "\n",
      "         [[ 0.3144, -0.4314, -0.0976,  ..., -0.6191,  0.3518,  0.0996],\n",
      "          [-0.3535,  0.3509,  0.0366,  ...,  0.2078, -0.0121, -0.1180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3637,  0.2229, -0.3104,  ..., -0.1601,  0.3348, -0.0243],\n",
      "          [-0.2924,  0.7171,  0.4708,  ...,  0.2324,  0.0562,  0.0937]],\n",
      "\n",
      "         [[ 0.1152, -0.2720,  0.3808,  ...,  0.0679,  0.3752,  0.0886],\n",
      "          [-0.2201, -0.2321,  0.7762,  ..., -0.0861, -0.0677, -0.4006]],\n",
      "\n",
      "         [[ 0.3809, -0.2121,  0.2211,  ..., -0.1876,  0.4539, -0.2618],\n",
      "          [-0.4511,  0.2376, -0.0835,  ...,  0.0569, -0.5051, -0.1506]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[ 0.4441, -0.5393, -0.0223,  ...,  0.3669,  0.5491,  0.5893],\n",
      "          [-0.4210,  0.0095,  1.0207,  ..., -0.5048,  2.1634, -0.3329]],\n",
      "\n",
      "         [[ 0.1847, -1.5750,  0.5564,  ..., -0.2723, -1.5266,  0.4211],\n",
      "          [-0.5277,  0.2384,  0.8735,  ..., -0.9526, -0.1079, -0.0503]],\n",
      "\n",
      "         [[-0.9441, -1.4084,  1.0631,  ..., -0.4016, -1.0913,  1.3978],\n",
      "          [-0.7660,  0.2293,  1.2399,  ..., -0.1842,  0.8046, -0.2424]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9858, -0.2138, -0.1126,  ..., -0.2230, -0.4769, -0.2408],\n",
      "          [-1.3126,  1.7988,  1.5454,  ..., -0.6033, -1.7215, -0.2564]],\n",
      "\n",
      "         [[-1.3039, -0.4972, -0.4311,  ...,  0.1353, -1.7321,  0.1073],\n",
      "          [-0.1609,  0.9560,  1.6348,  ...,  0.3072, -0.2612, -0.7549]],\n",
      "\n",
      "         [[-1.0370, -1.5806,  0.0114,  ..., -0.1052, -0.7074, -1.3986],\n",
      "          [-1.5682,  0.3915,  0.5278,  ..., -2.3107, -0.3347, -1.5439]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6372, -0.3413,  0.1306,  ...,  0.2729,  0.7936,  0.4656],\n",
      "          [-0.4715, -0.0438,  0.8119,  ..., -0.6193,  2.3129,  0.0398]],\n",
      "\n",
      "         [[ 0.4490, -2.4123,  0.5215,  ..., -0.2794, -1.5323, -1.0683],\n",
      "          [-0.6989,  0.0821, -0.1766,  ..., -0.2850,  0.8402,  0.2538]],\n",
      "\n",
      "         [[-2.0030, -0.7086,  1.9128,  ..., -1.4160, -0.6513,  0.6689],\n",
      "          [ 0.3692, -0.5820,  1.3425,  ..., -0.1195,  0.1221, -1.7771]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7380,  0.5445, -0.0858,  ...,  0.1363,  0.5892, -0.9951],\n",
      "          [-1.9997,  1.3927,  2.9849,  ..., -0.1532, -1.5135,  1.0518]],\n",
      "\n",
      "         [[-0.8891, -0.4757,  0.9061,  ...,  0.4994, -1.3647,  0.3518],\n",
      "          [-0.6762,  0.6803,  0.8676,  ..., -0.7173,  0.1832, -0.1422]],\n",
      "\n",
      "         [[-0.9290, -1.4858,  0.1111,  ..., -0.1860, -0.5638, -1.4642],\n",
      "          [-1.6667,  0.3518,  0.4068,  ..., -2.5030, -0.2250, -1.2818]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          ...,\n",
      "          [-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899]],\n",
      "\n",
      "         [[-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400],\n",
      "          ...,\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          ...,\n",
      "          [ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752]],\n",
      "\n",
      "         [[-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111],\n",
      "          ...,\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927]],\n",
      "\n",
      "         [[-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773]],\n",
      "\n",
      "         [[-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556]],\n",
      "\n",
      "         [[-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551]],\n",
      "\n",
      "         [[-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255]],\n",
      "\n",
      "         [[ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724]],\n",
      "\n",
      "         [[ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290]],\n",
      "\n",
      "         [[ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462]],\n",
      "\n",
      "         [[ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[-1.5273,  0.9727],\n",
      "        [-1.0469,  0.5234]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 0])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.4737, -1.2851,  0.7269,  ..., -1.3262, -0.0807,  0.7324],\n",
      "         [-1.0127, -1.2385,  1.0493,  ..., -0.2675,  0.9101,  0.6033],\n",
      "         ...,\n",
      "         [-0.8877,  0.2255, -0.0285,  ..., -0.8598, -1.6034,  0.4798],\n",
      "         [-1.3354, -0.2770, -0.4315,  ...,  0.1540, -0.0039,  0.0438],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]],\n",
      "\n",
      "        [[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.3325, -2.1907,  0.4309,  ..., -0.4327,  0.8238,  0.6018],\n",
      "         [-2.2631, -0.5424,  1.6860,  ..., -0.1677,  0.0543, -1.1901],\n",
      "         ...,\n",
      "         [-0.6945,  0.7086, -0.0305,  ..., -0.2801, -1.4319,  1.3509],\n",
      "         [-0.9197, -0.5727,  0.8206,  ..., -0.7162,  0.1093,  0.1793],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0562, -0.1131, -0.6344,  ...,  0.7818, -0.4356, -0.4549],\n",
      "          [ 0.4613, -0.0945, -0.4052,  ...,  0.3220, -1.2746, -0.1855]],\n",
      "\n",
      "         [[ 0.4461, -0.2506, -0.2545,  ...,  0.5247, -0.2090, -0.6426],\n",
      "          [-0.0903,  0.0256,  0.0564,  ...,  0.4800, -0.8828, -0.4402]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1836,  0.3485, -0.3765,  ...,  0.2994, -0.1872, -0.3081],\n",
      "          [ 0.2338, -0.0527, -0.0200,  ...,  0.5915, -0.7918, -0.6290]],\n",
      "\n",
      "         [[ 0.2951, -0.2242,  0.0727,  ...,  0.3489, -0.2264, -0.5768],\n",
      "          [ 0.3838, -0.1085, -0.1734,  ...,  0.3543, -1.0874, -0.6689]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0497, -0.1332, -0.2219,  ...,  1.0808, -0.3719, -0.5401],\n",
      "          [ 0.3631,  0.0128, -0.6016,  ...,  0.5172, -1.2711, -0.2861]],\n",
      "\n",
      "         [[ 0.6823, -0.1618, -0.5238,  ...,  0.4329, -0.5678, -0.3586],\n",
      "          [ 0.0153,  0.0539, -0.1937,  ...,  0.3206, -1.2407, -0.4009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5027,  0.1520, -0.3113,  ...,  0.3781, -0.2348, -0.1838],\n",
      "          [ 0.1863, -0.0638, -0.0369,  ...,  0.6593, -0.8024, -0.5406]],\n",
      "\n",
      "         [[ 0.0185, -0.4286, -0.1754,  ...,  0.1903, -0.3894, -0.4808],\n",
      "          [ 0.4196, -0.0278, -0.4332,  ...,  0.3414, -1.1425, -0.8640]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0078, -0.1289,  0.4531,  ..., -0.0039,  0.1055,  0.2422],\n",
      "          [ 0.0312,  0.0703,  0.3164,  ..., -0.3633,  0.0156, -0.3281]],\n",
      "\n",
      "         [[ 0.4258,  0.0898, -0.1719,  ...,  0.1523, -0.0547, -0.1289],\n",
      "          [ 0.2305,  0.0312,  0.1250,  ..., -0.6250, -0.2617, -0.7969]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0742,  0.2031,  0.5273,  ..., -0.4180,  0.6445,  0.2031],\n",
      "          [-0.0703,  0.0625,  0.2109,  ..., -0.3906,  0.3477, -0.0625]],\n",
      "\n",
      "         [[ 0.2422, -0.1016,  0.0352,  ...,  0.2422,  0.4062,  0.2578],\n",
      "          [ 0.4883, -0.0078,  0.3945,  ..., -0.4297,  0.0625, -0.3711]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0586, -0.2344, -0.0117,  ...,  0.3438,  0.5977, -0.1641],\n",
      "          [ 0.0859,  0.3945,  0.5547,  ..., -0.3164,  0.0898,  0.0859]],\n",
      "\n",
      "         [[ 0.5430,  0.2812,  0.1641,  ..., -0.2344,  0.2344,  0.1602],\n",
      "          [-0.0312, -0.2773, -0.1641,  ..., -0.5430, -0.1641, -0.6836]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0625, -0.1836,  0.5742,  ...,  0.0781,  1.1719, -0.0703],\n",
      "          [ 0.1406, -0.0977,  0.4648,  ..., -0.1055,  0.3633, -0.2266]],\n",
      "\n",
      "         [[-0.2383, -0.6250, -0.2266,  ...,  0.0273,  0.3828,  0.2695],\n",
      "          [ 0.1211,  0.3789,  0.6211,  ..., -0.5703, -0.1406, -0.2656]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.3164,  0.3555,  0.0898,  ...,  0.2969,  0.6016,  0.4453],\n",
      "          [-0.0195,  0.0039, -0.5664,  ...,  0.1719, -0.3516, -0.6406]],\n",
      "\n",
      "         [[ 0.3477,  0.1016, -0.0938,  ...,  0.8555, -0.3516, -0.3438],\n",
      "          [-0.0664, -0.0469, -0.3672,  ..., -0.0117, -0.2969, -0.1016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0117,  0.2109,  0.1445,  ...,  0.2852, -0.1992,  0.5508],\n",
      "          [-0.2266,  0.2383,  0.3203,  ...,  0.0977, -0.0547, -0.1211]],\n",
      "\n",
      "         [[-0.0742, -0.2383, -0.0273,  ...,  1.0664,  0.0391,  0.1875],\n",
      "          [ 0.0039, -0.3242, -0.0820,  ..., -0.1797, -0.0586, -0.1680]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "\n",
      "\n",
      "        [[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.5898, -0.0859,  0.7148,  ...,  0.1797,  0.5664,  0.2578],\n",
      "          [ 0.0664,  0.2070, -0.2812,  ...,  0.1797, -0.2734, -0.5078]],\n",
      "\n",
      "         [[ 0.5938,  0.1562, -0.1602,  ...,  0.7539, -0.3164, -0.0430],\n",
      "          [-0.0820,  0.0469, -0.4141,  ...,  0.0703, -0.7539, -0.5508]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0547, -0.0273,  0.4141,  ...,  0.1367,  0.2070,  0.3906],\n",
      "          [ 0.0312, -0.3789,  0.2461,  ...,  0.1484, -0.0586, -0.3711]],\n",
      "\n",
      "         [[-0.1250, -0.0664,  0.1133,  ...,  0.3164, -0.0859,  0.1523],\n",
      "          [ 0.0859, -0.0703, -0.2383,  ...,  0.4453,  0.2031, -0.2109]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          ...,\n",
      "          [-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326]],\n",
      "\n",
      "         [[-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397],\n",
      "          ...,\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          ...,\n",
      "          [-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488]],\n",
      "\n",
      "         [[ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944],\n",
      "          ...,\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237]],\n",
      "\n",
      "         [[-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171]],\n",
      "\n",
      "         [[-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393]],\n",
      "\n",
      "         [[-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313]],\n",
      "\n",
      "         [[-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057]],\n",
      "\n",
      "         [[-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905]],\n",
      "\n",
      "         [[-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019]],\n",
      "\n",
      "         [[-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924]],\n",
      "\n",
      "         [[-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-1.1129,  1.4734, -0.9384,  ...,  0.6065, -0.6529,  1.4177],\n",
      "          [ 0.3704, -0.9319,  0.6048,  ..., -0.3328,  0.4669,  0.6202]],\n",
      "\n",
      "         [[-0.7646,  0.4657, -0.8041,  ...,  0.4373, -0.0480,  0.9552],\n",
      "          [ 0.5976, -0.5469,  0.0781,  ..., -0.4763,  0.4951,  0.2098]],\n",
      "\n",
      "         [[-0.7888,  1.1687, -0.5397,  ...,  0.5734, -0.0654,  0.2727],\n",
      "          [-0.0468, -0.4641,  0.1866,  ...,  0.0661,  0.6113,  0.0575]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4021,  0.1331, -0.2043,  ...,  0.1385, -0.0622,  0.0973],\n",
      "          [ 0.0917, -0.4530, -0.1006,  ..., -0.0134,  0.3335,  0.0101]],\n",
      "\n",
      "         [[-0.5181,  0.3185, -0.4510,  ...,  0.3619, -0.3350,  1.0064],\n",
      "          [ 0.3075, -0.5551,  0.0521,  ...,  0.2385,  0.5574,  0.1181]],\n",
      "\n",
      "         [[-0.2165,  0.3619, -0.9592,  ..., -0.0838, -0.5500,  0.9440],\n",
      "          [ 0.4487, -0.2657,  0.4180,  ..., -0.2889,  0.3050,  0.2158]]],\n",
      "\n",
      "\n",
      "        [[[-0.9999,  1.4566, -0.8748,  ...,  0.6269, -0.5588,  1.2911],\n",
      "          [ 0.3669, -0.8954,  0.6457,  ..., -0.3386,  0.3764,  0.5545]],\n",
      "\n",
      "         [[-0.4596,  0.8033, -0.5762,  ...,  0.6520,  0.1473,  0.7978],\n",
      "          [ 0.4975, -0.5188,  0.1083,  ..., -0.5528,  0.4481,  0.3905]],\n",
      "\n",
      "         [[-0.6298,  0.8572, -0.4242,  ...,  0.4010, -0.0769,  0.4064],\n",
      "          [ 0.0458, -0.4961, -0.1434,  ..., -0.2500,  0.5208,  0.0223]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2397,  0.4131, -0.1033,  ...,  0.2014, -0.0896,  0.3833],\n",
      "          [ 0.5522, -0.4919,  0.4206,  ...,  0.1394,  0.3545,  0.1110]],\n",
      "\n",
      "         [[-0.2534,  0.4085, -0.2286,  ...,  0.3257, -0.0417,  1.2924],\n",
      "          [ 0.2581, -0.2707, -0.3397,  ...,  0.2182,  0.2154,  0.2228]],\n",
      "\n",
      "         [[-0.1549,  0.3696, -0.9408,  ..., -0.0677, -0.5100,  0.8811],\n",
      "          [ 0.4627, -0.2474,  0.4603,  ..., -0.3079,  0.2522,  0.1847]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.7791, -0.0996,  0.0881,  ..., -0.1805,  0.5723, -0.1030],\n",
      "          [-0.3123,  0.1305,  0.0859,  ...,  0.2072, -0.2309, -0.2110]],\n",
      "\n",
      "         [[ 0.4952, -0.4713, -0.3020,  ..., -0.2922,  0.4016, -0.1640],\n",
      "          [-0.1513,  0.0572, -0.0277,  ..., -0.0069, -0.2822, -0.0705]],\n",
      "\n",
      "         [[ 0.0904,  0.4109, -0.1498,  ..., -0.1056,  0.2361,  0.1863],\n",
      "          [-0.0452,  0.1187,  0.0448,  ...,  0.6728,  0.3278,  0.0186]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5774, -0.2510,  0.1250,  ...,  0.0923,  0.7083, -0.2797],\n",
      "          [-0.1007,  0.5627,  0.1273,  ...,  0.6724, -0.0229,  0.1478]],\n",
      "\n",
      "         [[ 0.4430, -0.2418,  0.2823,  ..., -0.0331,  0.5006,  0.0975],\n",
      "          [-0.4843,  0.0750,  0.5932,  ...,  0.0728, -0.1789, -0.1245]],\n",
      "\n",
      "         [[ 0.4187, -0.2804,  0.2250,  ..., -0.1531,  0.4754, -0.3152],\n",
      "          [-0.4331,  0.2474, -0.0962,  ...,  0.0994, -0.4916, -0.1530]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7005,  0.0013,  0.0725,  ..., -0.2265,  0.5070, -0.0155],\n",
      "          [-0.3017,  0.1156,  0.1104,  ...,  0.1438, -0.2198, -0.1874]],\n",
      "\n",
      "         [[ 0.3386, -0.2947,  0.2519,  ..., -0.0053,  0.1211, -0.1040],\n",
      "          [-0.2981,  0.0588, -0.1370,  ..., -0.0671, -0.2978, -0.0777]],\n",
      "\n",
      "         [[ 0.3144, -0.4314, -0.0976,  ..., -0.6191,  0.3518,  0.0996],\n",
      "          [-0.3535,  0.3509,  0.0366,  ...,  0.2078, -0.0121, -0.1180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3637,  0.2229, -0.3104,  ..., -0.1601,  0.3348, -0.0243],\n",
      "          [-0.2924,  0.7171,  0.4708,  ...,  0.2324,  0.0562,  0.0937]],\n",
      "\n",
      "         [[ 0.1152, -0.2720,  0.3808,  ...,  0.0679,  0.3752,  0.0886],\n",
      "          [-0.2201, -0.2321,  0.7762,  ..., -0.0861, -0.0677, -0.4006]],\n",
      "\n",
      "         [[ 0.3809, -0.2121,  0.2211,  ..., -0.1876,  0.4539, -0.2618],\n",
      "          [-0.4511,  0.2376, -0.0835,  ...,  0.0569, -0.5051, -0.1506]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[ 0.4441, -0.5393, -0.0223,  ...,  0.3669,  0.5491,  0.5893],\n",
      "          [-0.4210,  0.0095,  1.0207,  ..., -0.5048,  2.1634, -0.3329]],\n",
      "\n",
      "         [[ 0.1847, -1.5750,  0.5564,  ..., -0.2723, -1.5266,  0.4211],\n",
      "          [-0.5277,  0.2384,  0.8735,  ..., -0.9526, -0.1079, -0.0503]],\n",
      "\n",
      "         [[-0.9441, -1.4084,  1.0631,  ..., -0.4016, -1.0913,  1.3978],\n",
      "          [-0.7660,  0.2293,  1.2399,  ..., -0.1842,  0.8046, -0.2424]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9858, -0.2138, -0.1126,  ..., -0.2230, -0.4769, -0.2408],\n",
      "          [-1.3126,  1.7988,  1.5454,  ..., -0.6033, -1.7215, -0.2564]],\n",
      "\n",
      "         [[-1.3039, -0.4972, -0.4311,  ...,  0.1353, -1.7321,  0.1073],\n",
      "          [-0.1609,  0.9560,  1.6348,  ...,  0.3072, -0.2612, -0.7549]],\n",
      "\n",
      "         [[-1.0370, -1.5806,  0.0114,  ..., -0.1052, -0.7074, -1.3986],\n",
      "          [-1.5682,  0.3915,  0.5278,  ..., -2.3107, -0.3347, -1.5439]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6372, -0.3413,  0.1306,  ...,  0.2729,  0.7936,  0.4656],\n",
      "          [-0.4715, -0.0438,  0.8119,  ..., -0.6193,  2.3129,  0.0398]],\n",
      "\n",
      "         [[ 0.4490, -2.4123,  0.5215,  ..., -0.2794, -1.5323, -1.0683],\n",
      "          [-0.6989,  0.0821, -0.1766,  ..., -0.2850,  0.8402,  0.2538]],\n",
      "\n",
      "         [[-2.0030, -0.7086,  1.9128,  ..., -1.4160, -0.6513,  0.6689],\n",
      "          [ 0.3692, -0.5820,  1.3425,  ..., -0.1195,  0.1221, -1.7771]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7380,  0.5445, -0.0858,  ...,  0.1363,  0.5892, -0.9951],\n",
      "          [-1.9997,  1.3927,  2.9849,  ..., -0.1532, -1.5135,  1.0518]],\n",
      "\n",
      "         [[-0.8891, -0.4757,  0.9061,  ...,  0.4994, -1.3647,  0.3518],\n",
      "          [-0.6762,  0.6803,  0.8676,  ..., -0.7173,  0.1832, -0.1422]],\n",
      "\n",
      "         [[-0.9290, -1.4858,  0.1111,  ..., -0.1860, -0.5638, -1.4642],\n",
      "          [-1.6667,  0.3518,  0.4068,  ..., -2.5030, -0.2250, -1.2818]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          ...,\n",
      "          [-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899]],\n",
      "\n",
      "         [[-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400],\n",
      "          ...,\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          ...,\n",
      "          [ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752]],\n",
      "\n",
      "         [[-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111],\n",
      "          ...,\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927]],\n",
      "\n",
      "         [[-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773]],\n",
      "\n",
      "         [[-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556]],\n",
      "\n",
      "         [[-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551]],\n",
      "\n",
      "         [[-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255]],\n",
      "\n",
      "         [[ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724]],\n",
      "\n",
      "         [[ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290]],\n",
      "\n",
      "         [[ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462]],\n",
      "\n",
      "         [[ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[-1.5273,  0.9727],\n",
      "        [-1.0469,  0.5234]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_value\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: classifier\u001b[0m\n",
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed (no post-train) eval_accuracy: 0.5020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.4737, -1.2851,  0.7269,  ..., -1.3262, -0.0807,  0.7324],\n",
      "         [-1.0127, -1.2385,  1.0493,  ..., -0.2675,  0.9101,  0.6033],\n",
      "         ...,\n",
      "         [-0.8877,  0.2255, -0.0285,  ..., -0.8598, -1.6034,  0.4798],\n",
      "         [-1.3354, -0.2770, -0.4315,  ...,  0.1540, -0.0039,  0.0438],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]],\n",
      "\n",
      "        [[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.3325, -2.1907,  0.4309,  ..., -0.4327,  0.8238,  0.6018],\n",
      "         [-2.2631, -0.5424,  1.6860,  ..., -0.1677,  0.0543, -1.1901],\n",
      "         ...,\n",
      "         [-0.6945,  0.7086, -0.0305,  ..., -0.2801, -1.4319,  1.3509],\n",
      "         [-0.9197, -0.5727,  0.8206,  ..., -0.7162,  0.1093,  0.1793],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0562, -0.1131, -0.6344,  ...,  0.7818, -0.4356, -0.4549],\n",
      "          [ 0.4613, -0.0945, -0.4052,  ...,  0.3220, -1.2746, -0.1855]],\n",
      "\n",
      "         [[ 0.4461, -0.2506, -0.2545,  ...,  0.5247, -0.2090, -0.6426],\n",
      "          [-0.0903,  0.0256,  0.0564,  ...,  0.4800, -0.8828, -0.4402]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1836,  0.3485, -0.3765,  ...,  0.2994, -0.1872, -0.3081],\n",
      "          [ 0.2338, -0.0527, -0.0200,  ...,  0.5915, -0.7918, -0.6290]],\n",
      "\n",
      "         [[ 0.2951, -0.2242,  0.0727,  ...,  0.3489, -0.2264, -0.5768],\n",
      "          [ 0.3838, -0.1085, -0.1734,  ...,  0.3543, -1.0874, -0.6689]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0497, -0.1332, -0.2219,  ...,  1.0808, -0.3719, -0.5401],\n",
      "          [ 0.3631,  0.0128, -0.6016,  ...,  0.5172, -1.2711, -0.2861]],\n",
      "\n",
      "         [[ 0.6823, -0.1618, -0.5238,  ...,  0.4329, -0.5678, -0.3586],\n",
      "          [ 0.0153,  0.0539, -0.1937,  ...,  0.3206, -1.2407, -0.4009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5027,  0.1520, -0.3113,  ...,  0.3781, -0.2348, -0.1838],\n",
      "          [ 0.1863, -0.0638, -0.0369,  ...,  0.6593, -0.8024, -0.5406]],\n",
      "\n",
      "         [[ 0.0185, -0.4286, -0.1754,  ...,  0.1903, -0.3894, -0.4808],\n",
      "          [ 0.4196, -0.0278, -0.4332,  ...,  0.3414, -1.1425, -0.8640]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0078, -0.1289,  0.4531,  ..., -0.0039,  0.1055,  0.2422],\n",
      "          [ 0.0312,  0.0703,  0.3164,  ..., -0.3633,  0.0156, -0.3281]],\n",
      "\n",
      "         [[ 0.4258,  0.0898, -0.1719,  ...,  0.1523, -0.0547, -0.1289],\n",
      "          [ 0.2305,  0.0312,  0.1250,  ..., -0.6250, -0.2617, -0.7969]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0742,  0.2031,  0.5273,  ..., -0.4180,  0.6445,  0.2031],\n",
      "          [-0.0703,  0.0625,  0.2109,  ..., -0.3906,  0.3477, -0.0625]],\n",
      "\n",
      "         [[ 0.2422, -0.1016,  0.0352,  ...,  0.2422,  0.4062,  0.2578],\n",
      "          [ 0.4883, -0.0078,  0.3945,  ..., -0.4297,  0.0625, -0.3711]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0586, -0.2344, -0.0117,  ...,  0.3438,  0.5977, -0.1641],\n",
      "          [ 0.0859,  0.3945,  0.5547,  ..., -0.3164,  0.0898,  0.0859]],\n",
      "\n",
      "         [[ 0.5430,  0.2812,  0.1641,  ..., -0.2344,  0.2344,  0.1602],\n",
      "          [-0.0312, -0.2773, -0.1641,  ..., -0.5430, -0.1641, -0.6836]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0625, -0.1836,  0.5742,  ...,  0.0781,  1.1719, -0.0703],\n",
      "          [ 0.1406, -0.0977,  0.4648,  ..., -0.1055,  0.3633, -0.2266]],\n",
      "\n",
      "         [[-0.2383, -0.6250, -0.2266,  ...,  0.0273,  0.3828,  0.2695],\n",
      "          [ 0.1211,  0.3789,  0.6211,  ..., -0.5703, -0.1406, -0.2656]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.3164,  0.3555,  0.0898,  ...,  0.2969,  0.6016,  0.4453],\n",
      "          [-0.0195,  0.0039, -0.5664,  ...,  0.1719, -0.3516, -0.6406]],\n",
      "\n",
      "         [[ 0.3477,  0.1016, -0.0938,  ...,  0.8555, -0.3516, -0.3438],\n",
      "          [-0.0664, -0.0469, -0.3672,  ..., -0.0117, -0.2969, -0.1016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0117,  0.2109,  0.1445,  ...,  0.2852, -0.1992,  0.5508],\n",
      "          [-0.2266,  0.2383,  0.3203,  ...,  0.0977, -0.0547, -0.1211]],\n",
      "\n",
      "         [[-0.0742, -0.2383, -0.0273,  ...,  1.0664,  0.0391,  0.1875],\n",
      "          [ 0.0039, -0.3242, -0.0820,  ..., -0.1797, -0.0586, -0.1680]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "\n",
      "\n",
      "        [[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.5898, -0.0859,  0.7148,  ...,  0.1797,  0.5664,  0.2578],\n",
      "          [ 0.0664,  0.2070, -0.2812,  ...,  0.1797, -0.2734, -0.5078]],\n",
      "\n",
      "         [[ 0.5938,  0.1562, -0.1602,  ...,  0.7539, -0.3164, -0.0430],\n",
      "          [-0.0820,  0.0469, -0.4141,  ...,  0.0703, -0.7539, -0.5508]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0547, -0.0273,  0.4141,  ...,  0.1367,  0.2070,  0.3906],\n",
      "          [ 0.0312, -0.3789,  0.2461,  ...,  0.1484, -0.0586, -0.3711]],\n",
      "\n",
      "         [[-0.1250, -0.0664,  0.1133,  ...,  0.3164, -0.0859,  0.1523],\n",
      "          [ 0.0859, -0.0703, -0.2383,  ...,  0.4453,  0.2031, -0.2109]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          ...,\n",
      "          [-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326]],\n",
      "\n",
      "         [[-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397],\n",
      "          ...,\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          ...,\n",
      "          [-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488]],\n",
      "\n",
      "         [[ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944],\n",
      "          ...,\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237]],\n",
      "\n",
      "         [[-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171]],\n",
      "\n",
      "         [[-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393]],\n",
      "\n",
      "         [[-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313]],\n",
      "\n",
      "         [[-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057]],\n",
      "\n",
      "         [[-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905]],\n",
      "\n",
      "         [[-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019]],\n",
      "\n",
      "         [[-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924]],\n",
      "\n",
      "         [[-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-1.1129,  1.4734, -0.9384,  ...,  0.6065, -0.6529,  1.4177],\n",
      "          [ 0.3704, -0.9319,  0.6048,  ..., -0.3328,  0.4669,  0.6202]],\n",
      "\n",
      "         [[-0.7646,  0.4657, -0.8041,  ...,  0.4373, -0.0480,  0.9552],\n",
      "          [ 0.5976, -0.5469,  0.0781,  ..., -0.4763,  0.4951,  0.2098]],\n",
      "\n",
      "         [[-0.7888,  1.1687, -0.5397,  ...,  0.5734, -0.0654,  0.2727],\n",
      "          [-0.0468, -0.4641,  0.1866,  ...,  0.0661,  0.6113,  0.0575]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4021,  0.1331, -0.2043,  ...,  0.1385, -0.0622,  0.0973],\n",
      "          [ 0.0917, -0.4530, -0.1006,  ..., -0.0134,  0.3335,  0.0101]],\n",
      "\n",
      "         [[-0.5181,  0.3185, -0.4510,  ...,  0.3619, -0.3350,  1.0064],\n",
      "          [ 0.3075, -0.5551,  0.0521,  ...,  0.2385,  0.5574,  0.1181]],\n",
      "\n",
      "         [[-0.2165,  0.3619, -0.9592,  ..., -0.0838, -0.5500,  0.9440],\n",
      "          [ 0.4487, -0.2657,  0.4180,  ..., -0.2889,  0.3050,  0.2158]]],\n",
      "\n",
      "\n",
      "        [[[-0.9999,  1.4566, -0.8748,  ...,  0.6269, -0.5588,  1.2911],\n",
      "          [ 0.3669, -0.8954,  0.6457,  ..., -0.3386,  0.3764,  0.5545]],\n",
      "\n",
      "         [[-0.4596,  0.8033, -0.5762,  ...,  0.6520,  0.1473,  0.7978],\n",
      "          [ 0.4975, -0.5188,  0.1083,  ..., -0.5528,  0.4481,  0.3905]],\n",
      "\n",
      "         [[-0.6298,  0.8572, -0.4242,  ...,  0.4010, -0.0769,  0.4064],\n",
      "          [ 0.0458, -0.4961, -0.1434,  ..., -0.2500,  0.5208,  0.0223]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2397,  0.4131, -0.1033,  ...,  0.2014, -0.0896,  0.3833],\n",
      "          [ 0.5522, -0.4919,  0.4206,  ...,  0.1394,  0.3545,  0.1110]],\n",
      "\n",
      "         [[-0.2534,  0.4085, -0.2286,  ...,  0.3257, -0.0417,  1.2924],\n",
      "          [ 0.2581, -0.2707, -0.3397,  ...,  0.2182,  0.2154,  0.2228]],\n",
      "\n",
      "         [[-0.1549,  0.3696, -0.9408,  ..., -0.0677, -0.5100,  0.8811],\n",
      "          [ 0.4627, -0.2474,  0.4603,  ..., -0.3079,  0.2522,  0.1847]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.7791, -0.0996,  0.0881,  ..., -0.1805,  0.5723, -0.1030],\n",
      "          [-0.3123,  0.1305,  0.0859,  ...,  0.2072, -0.2309, -0.2110]],\n",
      "\n",
      "         [[ 0.4952, -0.4713, -0.3020,  ..., -0.2922,  0.4016, -0.1640],\n",
      "          [-0.1513,  0.0572, -0.0277,  ..., -0.0069, -0.2822, -0.0705]],\n",
      "\n",
      "         [[ 0.0904,  0.4109, -0.1498,  ..., -0.1056,  0.2361,  0.1863],\n",
      "          [-0.0452,  0.1187,  0.0448,  ...,  0.6728,  0.3278,  0.0186]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5774, -0.2510,  0.1250,  ...,  0.0923,  0.7083, -0.2797],\n",
      "          [-0.1007,  0.5627,  0.1273,  ...,  0.6724, -0.0229,  0.1478]],\n",
      "\n",
      "         [[ 0.4430, -0.2418,  0.2823,  ..., -0.0331,  0.5006,  0.0975],\n",
      "          [-0.4843,  0.0750,  0.5932,  ...,  0.0728, -0.1789, -0.1245]],\n",
      "\n",
      "         [[ 0.4187, -0.2804,  0.2250,  ..., -0.1531,  0.4754, -0.3152],\n",
      "          [-0.4331,  0.2474, -0.0962,  ...,  0.0994, -0.4916, -0.1530]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7005,  0.0013,  0.0725,  ..., -0.2265,  0.5070, -0.0155],\n",
      "          [-0.3017,  0.1156,  0.1104,  ...,  0.1438, -0.2198, -0.1874]],\n",
      "\n",
      "         [[ 0.3386, -0.2947,  0.2519,  ..., -0.0053,  0.1211, -0.1040],\n",
      "          [-0.2981,  0.0588, -0.1370,  ..., -0.0671, -0.2978, -0.0777]],\n",
      "\n",
      "         [[ 0.3144, -0.4314, -0.0976,  ..., -0.6191,  0.3518,  0.0996],\n",
      "          [-0.3535,  0.3509,  0.0366,  ...,  0.2078, -0.0121, -0.1180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3637,  0.2229, -0.3104,  ..., -0.1601,  0.3348, -0.0243],\n",
      "          [-0.2924,  0.7171,  0.4708,  ...,  0.2324,  0.0562,  0.0937]],\n",
      "\n",
      "         [[ 0.1152, -0.2720,  0.3808,  ...,  0.0679,  0.3752,  0.0886],\n",
      "          [-0.2201, -0.2321,  0.7762,  ..., -0.0861, -0.0677, -0.4006]],\n",
      "\n",
      "         [[ 0.3809, -0.2121,  0.2211,  ..., -0.1876,  0.4539, -0.2618],\n",
      "          [-0.4511,  0.2376, -0.0835,  ...,  0.0569, -0.5051, -0.1506]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[ 0.4441, -0.5393, -0.0223,  ...,  0.3669,  0.5491,  0.5893],\n",
      "          [-0.4210,  0.0095,  1.0207,  ..., -0.5048,  2.1634, -0.3329]],\n",
      "\n",
      "         [[ 0.1847, -1.5750,  0.5564,  ..., -0.2723, -1.5266,  0.4211],\n",
      "          [-0.5277,  0.2384,  0.8735,  ..., -0.9526, -0.1079, -0.0503]],\n",
      "\n",
      "         [[-0.9441, -1.4084,  1.0631,  ..., -0.4016, -1.0913,  1.3978],\n",
      "          [-0.7660,  0.2293,  1.2399,  ..., -0.1842,  0.8046, -0.2424]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9858, -0.2138, -0.1126,  ..., -0.2230, -0.4769, -0.2408],\n",
      "          [-1.3126,  1.7988,  1.5454,  ..., -0.6033, -1.7215, -0.2564]],\n",
      "\n",
      "         [[-1.3039, -0.4972, -0.4311,  ...,  0.1353, -1.7321,  0.1073],\n",
      "          [-0.1609,  0.9560,  1.6348,  ...,  0.3072, -0.2612, -0.7549]],\n",
      "\n",
      "         [[-1.0370, -1.5806,  0.0114,  ..., -0.1052, -0.7074, -1.3986],\n",
      "          [-1.5682,  0.3915,  0.5278,  ..., -2.3107, -0.3347, -1.5439]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6372, -0.3413,  0.1306,  ...,  0.2729,  0.7936,  0.4656],\n",
      "          [-0.4715, -0.0438,  0.8119,  ..., -0.6193,  2.3129,  0.0398]],\n",
      "\n",
      "         [[ 0.4490, -2.4123,  0.5215,  ..., -0.2794, -1.5323, -1.0683],\n",
      "          [-0.6989,  0.0821, -0.1766,  ..., -0.2850,  0.8402,  0.2538]],\n",
      "\n",
      "         [[-2.0030, -0.7086,  1.9128,  ..., -1.4160, -0.6513,  0.6689],\n",
      "          [ 0.3692, -0.5820,  1.3425,  ..., -0.1195,  0.1221, -1.7771]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7380,  0.5445, -0.0858,  ...,  0.1363,  0.5892, -0.9951],\n",
      "          [-1.9997,  1.3927,  2.9849,  ..., -0.1532, -1.5135,  1.0518]],\n",
      "\n",
      "         [[-0.8891, -0.4757,  0.9061,  ...,  0.4994, -1.3647,  0.3518],\n",
      "          [-0.6762,  0.6803,  0.8676,  ..., -0.7173,  0.1832, -0.1422]],\n",
      "\n",
      "         [[-0.9290, -1.4858,  0.1111,  ..., -0.1860, -0.5638, -1.4642],\n",
      "          [-1.6667,  0.3518,  0.4068,  ..., -2.5030, -0.2250, -1.2818]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          ...,\n",
      "          [-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899]],\n",
      "\n",
      "         [[-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400],\n",
      "          ...,\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          ...,\n",
      "          [ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752]],\n",
      "\n",
      "         [[-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111],\n",
      "          ...,\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927]],\n",
      "\n",
      "         [[-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773]],\n",
      "\n",
      "         [[-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556]],\n",
      "\n",
      "         [[-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551]],\n",
      "\n",
      "         [[-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255]],\n",
      "\n",
      "         [[ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724]],\n",
      "\n",
      "         [[ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290]],\n",
      "\n",
      "         [[ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462]],\n",
      "\n",
      "         [[ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[-1.5273,  0.9727],\n",
      "        [-1.0469,  0.5234]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_value\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: classifier\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.4737, -1.2851,  0.7269,  ..., -1.3262, -0.0807,  0.7324],\n",
      "         [-1.0127, -1.2385,  1.0493,  ..., -0.2675,  0.9101,  0.6033],\n",
      "         ...,\n",
      "         [-0.8877,  0.2255, -0.0285,  ..., -0.8598, -1.6034,  0.4798],\n",
      "         [-1.3354, -0.2770, -0.4315,  ...,  0.1540, -0.0039,  0.0438],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]],\n",
      "\n",
      "        [[ 0.4399, -0.1289, -0.0193,  ..., -0.7220,  2.1669,  0.2116],\n",
      "         [ 0.3325, -2.1907,  0.4309,  ..., -0.4327,  0.8238,  0.6018],\n",
      "         [-2.2631, -0.5424,  1.6860,  ..., -0.1677,  0.0543, -1.1901],\n",
      "         ...,\n",
      "         [-0.6945,  0.7086, -0.0305,  ..., -0.2801, -1.4319,  1.3509],\n",
      "         [-0.9197, -0.5727,  0.8206,  ..., -0.7162,  0.1093,  0.1793],\n",
      "         [-0.9938, -1.2972,  0.0781,  ..., -2.4799, -0.2318, -0.8143]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0562, -0.1131, -0.6344,  ...,  0.3220, -1.2746, -0.1855],\n",
      "         [ 0.4461, -0.2506, -0.2545,  ...,  0.4800, -0.8828, -0.4402],\n",
      "         ...,\n",
      "         [ 0.1836,  0.3485, -0.3765,  ...,  0.5915, -0.7918, -0.6290],\n",
      "         [ 0.2951, -0.2242,  0.0727,  ...,  0.3543, -1.0874, -0.6689],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]],\n",
      "\n",
      "        [[ 0.5482, -0.0824, -0.3307,  ...,  1.0650, -1.5947, -0.4390],\n",
      "         [ 0.0497, -0.1332, -0.2219,  ...,  0.5172, -1.2711, -0.2861],\n",
      "         [ 0.6823, -0.1618, -0.5238,  ...,  0.3206, -1.2407, -0.4009],\n",
      "         ...,\n",
      "         [ 0.5027,  0.1520, -0.3113,  ...,  0.6593, -0.8024, -0.5406],\n",
      "         [ 0.0185, -0.4286, -0.1754,  ...,  0.3414, -1.1425, -0.8640],\n",
      "         [ 0.3145, -0.0455, -0.7539,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0562, -0.1131, -0.6344,  ...,  0.7818, -0.4356, -0.4549],\n",
      "          [ 0.4613, -0.0945, -0.4052,  ...,  0.3220, -1.2746, -0.1855]],\n",
      "\n",
      "         [[ 0.4461, -0.2506, -0.2545,  ...,  0.5247, -0.2090, -0.6426],\n",
      "          [-0.0903,  0.0256,  0.0564,  ...,  0.4800, -0.8828, -0.4402]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1836,  0.3485, -0.3765,  ...,  0.2994, -0.1872, -0.3081],\n",
      "          [ 0.2338, -0.0527, -0.0200,  ...,  0.5915, -0.7918, -0.6290]],\n",
      "\n",
      "         [[ 0.2951, -0.2242,  0.0727,  ...,  0.3489, -0.2264, -0.5768],\n",
      "          [ 0.3838, -0.1085, -0.1734,  ...,  0.3543, -1.0874, -0.6689]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5482, -0.0824, -0.3307,  ...,  0.8655, -0.7416, -0.7480],\n",
      "          [ 1.1855, -0.1753, -0.8740,  ...,  1.0650, -1.5947, -0.4390]],\n",
      "\n",
      "         [[ 0.0497, -0.1332, -0.2219,  ...,  1.0808, -0.3719, -0.5401],\n",
      "          [ 0.3631,  0.0128, -0.6016,  ...,  0.5172, -1.2711, -0.2861]],\n",
      "\n",
      "         [[ 0.6823, -0.1618, -0.5238,  ...,  0.4329, -0.5678, -0.3586],\n",
      "          [ 0.0153,  0.0539, -0.1937,  ...,  0.3206, -1.2407, -0.4009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5027,  0.1520, -0.3113,  ...,  0.3781, -0.2348, -0.1838],\n",
      "          [ 0.1863, -0.0638, -0.0369,  ...,  0.6593, -0.8024, -0.5406]],\n",
      "\n",
      "         [[ 0.0185, -0.4286, -0.1754,  ...,  0.1903, -0.3894, -0.4808],\n",
      "          [ 0.4196, -0.0278, -0.4332,  ...,  0.3414, -1.1425, -0.8640]],\n",
      "\n",
      "         [[ 0.3145, -0.0455, -0.7539,  ...,  0.3978, -0.2182, -0.1392],\n",
      "          [ 0.5989, -0.8335, -0.2208,  ...,  0.3925, -0.9176, -0.4712]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0078, -0.1289,  0.4531,  ..., -0.3633,  0.0156, -0.3281],\n",
      "         [ 0.4258,  0.0898, -0.1719,  ..., -0.6250, -0.2617, -0.7969],\n",
      "         ...,\n",
      "         [ 0.0742,  0.2031,  0.5273,  ..., -0.3906,  0.3477, -0.0625],\n",
      "         [ 0.2422, -0.1016,  0.0352,  ..., -0.4297,  0.0625, -0.3711],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]],\n",
      "\n",
      "        [[ 0.3477, -0.7461,  0.3906,  ..., -0.4258,  0.1094, -0.4531],\n",
      "         [ 0.0586, -0.2344, -0.0117,  ..., -0.3164,  0.0898,  0.0859],\n",
      "         [ 0.5430,  0.2812,  0.1641,  ..., -0.5430, -0.1641, -0.6836],\n",
      "         ...,\n",
      "         [-0.0625, -0.1836,  0.5742,  ..., -0.1055,  0.3633, -0.2266],\n",
      "         [-0.2383, -0.6250, -0.2266,  ..., -0.5703, -0.1406, -0.2656],\n",
      "         [ 0.2773, -0.2852,  0.7227,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0078, -0.1289,  0.4531,  ..., -0.0039,  0.1055,  0.2422],\n",
      "          [ 0.0312,  0.0703,  0.3164,  ..., -0.3633,  0.0156, -0.3281]],\n",
      "\n",
      "         [[ 0.4258,  0.0898, -0.1719,  ...,  0.1523, -0.0547, -0.1289],\n",
      "          [ 0.2305,  0.0312,  0.1250,  ..., -0.6250, -0.2617, -0.7969]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0742,  0.2031,  0.5273,  ..., -0.4180,  0.6445,  0.2031],\n",
      "          [-0.0703,  0.0625,  0.2109,  ..., -0.3906,  0.3477, -0.0625]],\n",
      "\n",
      "         [[ 0.2422, -0.1016,  0.0352,  ...,  0.2422,  0.4062,  0.2578],\n",
      "          [ 0.4883, -0.0078,  0.3945,  ..., -0.4297,  0.0625, -0.3711]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3477, -0.7461,  0.3906,  ...,  0.2656,  0.2148,  0.0273],\n",
      "          [-0.3047,  0.2344,  0.5859,  ..., -0.4258,  0.1094, -0.4531]],\n",
      "\n",
      "         [[ 0.0586, -0.2344, -0.0117,  ...,  0.3438,  0.5977, -0.1641],\n",
      "          [ 0.0859,  0.3945,  0.5547,  ..., -0.3164,  0.0898,  0.0859]],\n",
      "\n",
      "         [[ 0.5430,  0.2812,  0.1641,  ..., -0.2344,  0.2344,  0.1602],\n",
      "          [-0.0312, -0.2773, -0.1641,  ..., -0.5430, -0.1641, -0.6836]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0625, -0.1836,  0.5742,  ...,  0.0781,  1.1719, -0.0703],\n",
      "          [ 0.1406, -0.0977,  0.4648,  ..., -0.1055,  0.3633, -0.2266]],\n",
      "\n",
      "         [[-0.2383, -0.6250, -0.2266,  ...,  0.0273,  0.3828,  0.2695],\n",
      "          [ 0.1211,  0.3789,  0.6211,  ..., -0.5703, -0.1406, -0.2656]],\n",
      "\n",
      "         [[ 0.2773, -0.2852,  0.7227,  ..., -0.4492,  0.6133,  0.3008],\n",
      "          [-0.0938,  0.1602,  0.6367,  ..., -0.3672,  0.1094,  0.1641]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.3164,  0.3555,  0.0898,  ...,  0.1719, -0.3516, -0.6406],\n",
      "         [ 0.3477,  0.1016, -0.0938,  ..., -0.0117, -0.2969, -0.1016],\n",
      "         ...,\n",
      "         [-0.0117,  0.2109,  0.1445,  ...,  0.0977, -0.0547, -0.1211],\n",
      "         [-0.0742, -0.2383, -0.0273,  ..., -0.1797, -0.0586, -0.1680],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]],\n",
      "\n",
      "        [[-0.3828,  0.2422,  0.2344,  ...,  0.1016,  0.1914,  0.1211],\n",
      "         [-0.5898, -0.0859,  0.7148,  ...,  0.1797, -0.2734, -0.5078],\n",
      "         [ 0.5938,  0.1562, -0.1602,  ...,  0.0703, -0.7539, -0.5508],\n",
      "         ...,\n",
      "         [ 0.0547, -0.0273,  0.4141,  ...,  0.1484, -0.0586, -0.3711],\n",
      "         [-0.1250, -0.0664,  0.1133,  ...,  0.4453,  0.2031, -0.2109],\n",
      "         [-0.3438,  0.0391, -0.1250,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.3164,  0.3555,  0.0898,  ...,  0.2969,  0.6016,  0.4453],\n",
      "          [-0.0195,  0.0039, -0.5664,  ...,  0.1719, -0.3516, -0.6406]],\n",
      "\n",
      "         [[ 0.3477,  0.1016, -0.0938,  ...,  0.8555, -0.3516, -0.3438],\n",
      "          [-0.0664, -0.0469, -0.3672,  ..., -0.0117, -0.2969, -0.1016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0117,  0.2109,  0.1445,  ...,  0.2852, -0.1992,  0.5508],\n",
      "          [-0.2266,  0.2383,  0.3203,  ...,  0.0977, -0.0547, -0.1211]],\n",
      "\n",
      "         [[-0.0742, -0.2383, -0.0273,  ...,  1.0664,  0.0391,  0.1875],\n",
      "          [ 0.0039, -0.3242, -0.0820,  ..., -0.1797, -0.0586, -0.1680]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]],\n",
      "\n",
      "\n",
      "        [[[-0.3828,  0.2422,  0.2344,  ..., -0.0625,  0.4453, -0.1328],\n",
      "          [ 0.4688, -0.2266,  0.1094,  ...,  0.1016,  0.1914,  0.1211]],\n",
      "\n",
      "         [[-0.5898, -0.0859,  0.7148,  ...,  0.1797,  0.5664,  0.2578],\n",
      "          [ 0.0664,  0.2070, -0.2812,  ...,  0.1797, -0.2734, -0.5078]],\n",
      "\n",
      "         [[ 0.5938,  0.1562, -0.1602,  ...,  0.7539, -0.3164, -0.0430],\n",
      "          [-0.0820,  0.0469, -0.4141,  ...,  0.0703, -0.7539, -0.5508]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0547, -0.0273,  0.4141,  ...,  0.1367,  0.2070,  0.3906],\n",
      "          [ 0.0312, -0.3789,  0.2461,  ...,  0.1484, -0.0586, -0.3711]],\n",
      "\n",
      "         [[-0.1250, -0.0664,  0.1133,  ...,  0.3164, -0.0859,  0.1523],\n",
      "          [ 0.0859, -0.0703, -0.2383,  ...,  0.4453,  0.2031, -0.2109]],\n",
      "\n",
      "         [[-0.3438,  0.0391, -0.1250,  ...,  0.4102,  0.3750,  0.2109],\n",
      "          [-0.2188, -0.4766, -0.6172,  ...,  0.0781, -0.3711, -0.5820]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          ...,\n",
      "          [-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326]],\n",
      "\n",
      "         [[-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397],\n",
      "          ...,\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          ...,\n",
      "          [-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488]],\n",
      "\n",
      "         [[ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944],\n",
      "          ...,\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.0538,  0.0422, -0.0182,  ...,  0.4866, -0.0793,  0.0175],\n",
      "          [-0.0374, -0.1617, -0.1580,  ...,  0.1706, -0.1574, -0.3237]],\n",
      "\n",
      "         [[-0.0871,  0.0736,  0.0287,  ...,  0.4768, -0.0126,  0.0781],\n",
      "          [-0.0601, -0.1350, -0.1567,  ...,  0.1573, -0.1410, -0.3171]],\n",
      "\n",
      "         [[-0.1038,  0.0842,  0.0502,  ...,  0.4851,  0.0221,  0.1208],\n",
      "          [-0.0640, -0.1366, -0.1419,  ...,  0.1788, -0.1287, -0.3397]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1149,  0.0913,  0.0614,  ...,  0.4774,  0.0406,  0.1432],\n",
      "          [-0.0795, -0.1236, -0.1451,  ...,  0.1712, -0.1319, -0.3393]],\n",
      "\n",
      "         [[-0.1187,  0.0945,  0.0590,  ...,  0.4783,  0.0433,  0.1359],\n",
      "          [-0.0660, -0.1331, -0.1417,  ...,  0.1694, -0.1269, -0.3313]],\n",
      "\n",
      "         [[-0.1161,  0.0820,  0.0516,  ...,  0.4795,  0.0229,  0.1326],\n",
      "          [-0.0655, -0.1382, -0.1386,  ...,  0.1746, -0.1277, -0.3394]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0274,  0.2418,  0.4289,  ...,  0.2323,  0.2155,  0.1675],\n",
      "          [ 0.0007, -0.2935, -0.0392,  ...,  0.3797,  0.0563, -0.4057]],\n",
      "\n",
      "         [[-0.0124,  0.2064,  0.3528,  ...,  0.2458,  0.1978,  0.1504],\n",
      "          [-0.0029, -0.2560, -0.0559,  ...,  0.3566,  0.0192, -0.3905]],\n",
      "\n",
      "         [[-0.0307,  0.1879,  0.3317,  ...,  0.2583,  0.1844,  0.1600],\n",
      "          [-0.0088, -0.2460, -0.0717,  ...,  0.3379, -0.0106, -0.3944]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0353,  0.1828,  0.2994,  ...,  0.2683,  0.1760,  0.1486],\n",
      "          [-0.0106, -0.2336, -0.0766,  ...,  0.3339, -0.0273, -0.4019]],\n",
      "\n",
      "         [[-0.0320,  0.1842,  0.3016,  ...,  0.2691,  0.1784,  0.1472],\n",
      "          [-0.0011, -0.2457, -0.0598,  ...,  0.3376, -0.0047, -0.3924]],\n",
      "\n",
      "         [[-0.0274,  0.1894,  0.3095,  ...,  0.2659,  0.1747,  0.1488],\n",
      "          [-0.0069, -0.2474, -0.0637,  ...,  0.3397, -0.0024, -0.3869]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.1129,  1.4734, -0.9384,  ..., -0.3328,  0.4669,  0.6202],\n",
      "         [-0.7646,  0.4657, -0.8041,  ..., -0.4763,  0.4951,  0.2098],\n",
      "         [-0.7888,  1.1687, -0.5397,  ...,  0.0661,  0.6113,  0.0575],\n",
      "         ...,\n",
      "         [-0.4021,  0.1331, -0.2043,  ..., -0.0134,  0.3335,  0.0101],\n",
      "         [-0.5181,  0.3185, -0.4510,  ...,  0.2385,  0.5574,  0.1181],\n",
      "         [-0.2165,  0.3619, -0.9592,  ..., -0.2889,  0.3050,  0.2158]],\n",
      "\n",
      "        [[-0.9999,  1.4566, -0.8748,  ..., -0.3386,  0.3764,  0.5545],\n",
      "         [-0.4596,  0.8033, -0.5762,  ..., -0.5528,  0.4481,  0.3905],\n",
      "         [-0.6298,  0.8572, -0.4242,  ..., -0.2500,  0.5208,  0.0223],\n",
      "         ...,\n",
      "         [-0.2397,  0.4131, -0.1033,  ...,  0.1394,  0.3545,  0.1110],\n",
      "         [-0.2534,  0.4085, -0.2286,  ...,  0.2182,  0.2154,  0.2228],\n",
      "         [-0.1549,  0.3696, -0.9408,  ..., -0.3079,  0.2522,  0.1847]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-1.1129,  1.4734, -0.9384,  ...,  0.6065, -0.6529,  1.4177],\n",
      "          [ 0.3704, -0.9319,  0.6048,  ..., -0.3328,  0.4669,  0.6202]],\n",
      "\n",
      "         [[-0.7646,  0.4657, -0.8041,  ...,  0.4373, -0.0480,  0.9552],\n",
      "          [ 0.5976, -0.5469,  0.0781,  ..., -0.4763,  0.4951,  0.2098]],\n",
      "\n",
      "         [[-0.7888,  1.1687, -0.5397,  ...,  0.5734, -0.0654,  0.2727],\n",
      "          [-0.0468, -0.4641,  0.1866,  ...,  0.0661,  0.6113,  0.0575]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4021,  0.1331, -0.2043,  ...,  0.1385, -0.0622,  0.0973],\n",
      "          [ 0.0917, -0.4530, -0.1006,  ..., -0.0134,  0.3335,  0.0101]],\n",
      "\n",
      "         [[-0.5181,  0.3185, -0.4510,  ...,  0.3619, -0.3350,  1.0064],\n",
      "          [ 0.3075, -0.5551,  0.0521,  ...,  0.2385,  0.5574,  0.1181]],\n",
      "\n",
      "         [[-0.2165,  0.3619, -0.9592,  ..., -0.0838, -0.5500,  0.9440],\n",
      "          [ 0.4487, -0.2657,  0.4180,  ..., -0.2889,  0.3050,  0.2158]]],\n",
      "\n",
      "\n",
      "        [[[-0.9999,  1.4566, -0.8748,  ...,  0.6269, -0.5588,  1.2911],\n",
      "          [ 0.3669, -0.8954,  0.6457,  ..., -0.3386,  0.3764,  0.5545]],\n",
      "\n",
      "         [[-0.4596,  0.8033, -0.5762,  ...,  0.6520,  0.1473,  0.7978],\n",
      "          [ 0.4975, -0.5188,  0.1083,  ..., -0.5528,  0.4481,  0.3905]],\n",
      "\n",
      "         [[-0.6298,  0.8572, -0.4242,  ...,  0.4010, -0.0769,  0.4064],\n",
      "          [ 0.0458, -0.4961, -0.1434,  ..., -0.2500,  0.5208,  0.0223]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2397,  0.4131, -0.1033,  ...,  0.2014, -0.0896,  0.3833],\n",
      "          [ 0.5522, -0.4919,  0.4206,  ...,  0.1394,  0.3545,  0.1110]],\n",
      "\n",
      "         [[-0.2534,  0.4085, -0.2286,  ...,  0.3257, -0.0417,  1.2924],\n",
      "          [ 0.2581, -0.2707, -0.3397,  ...,  0.2182,  0.2154,  0.2228]],\n",
      "\n",
      "         [[-0.1549,  0.3696, -0.9408,  ..., -0.0677, -0.5100,  0.8811],\n",
      "          [ 0.4627, -0.2474,  0.4603,  ..., -0.3079,  0.2522,  0.1847]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.7791, -0.0996,  0.0881,  ...,  0.2072, -0.2309, -0.2110],\n",
      "         [ 0.4952, -0.4713, -0.3020,  ..., -0.0069, -0.2822, -0.0705],\n",
      "         [ 0.0904,  0.4109, -0.1498,  ...,  0.6728,  0.3278,  0.0186],\n",
      "         ...,\n",
      "         [ 0.5774, -0.2510,  0.1250,  ...,  0.6724, -0.0229,  0.1478],\n",
      "         [ 0.4430, -0.2418,  0.2823,  ...,  0.0728, -0.1789, -0.1245],\n",
      "         [ 0.4187, -0.2804,  0.2250,  ...,  0.0994, -0.4916, -0.1530]],\n",
      "\n",
      "        [[ 0.7005,  0.0013,  0.0725,  ...,  0.1438, -0.2198, -0.1874],\n",
      "         [ 0.3386, -0.2947,  0.2519,  ..., -0.0671, -0.2978, -0.0777],\n",
      "         [ 0.3144, -0.4314, -0.0976,  ...,  0.2078, -0.0121, -0.1180],\n",
      "         ...,\n",
      "         [ 0.3637,  0.2229, -0.3104,  ...,  0.2324,  0.0562,  0.0937],\n",
      "         [ 0.1152, -0.2720,  0.3808,  ..., -0.0861, -0.0677, -0.4006],\n",
      "         [ 0.3809, -0.2121,  0.2211,  ...,  0.0569, -0.5051, -0.1506]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.7791, -0.0996,  0.0881,  ..., -0.1805,  0.5723, -0.1030],\n",
      "          [-0.3123,  0.1305,  0.0859,  ...,  0.2072, -0.2309, -0.2110]],\n",
      "\n",
      "         [[ 0.4952, -0.4713, -0.3020,  ..., -0.2922,  0.4016, -0.1640],\n",
      "          [-0.1513,  0.0572, -0.0277,  ..., -0.0069, -0.2822, -0.0705]],\n",
      "\n",
      "         [[ 0.0904,  0.4109, -0.1498,  ..., -0.1056,  0.2361,  0.1863],\n",
      "          [-0.0452,  0.1187,  0.0448,  ...,  0.6728,  0.3278,  0.0186]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5774, -0.2510,  0.1250,  ...,  0.0923,  0.7083, -0.2797],\n",
      "          [-0.1007,  0.5627,  0.1273,  ...,  0.6724, -0.0229,  0.1478]],\n",
      "\n",
      "         [[ 0.4430, -0.2418,  0.2823,  ..., -0.0331,  0.5006,  0.0975],\n",
      "          [-0.4843,  0.0750,  0.5932,  ...,  0.0728, -0.1789, -0.1245]],\n",
      "\n",
      "         [[ 0.4187, -0.2804,  0.2250,  ..., -0.1531,  0.4754, -0.3152],\n",
      "          [-0.4331,  0.2474, -0.0962,  ...,  0.0994, -0.4916, -0.1530]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7005,  0.0013,  0.0725,  ..., -0.2265,  0.5070, -0.0155],\n",
      "          [-0.3017,  0.1156,  0.1104,  ...,  0.1438, -0.2198, -0.1874]],\n",
      "\n",
      "         [[ 0.3386, -0.2947,  0.2519,  ..., -0.0053,  0.1211, -0.1040],\n",
      "          [-0.2981,  0.0588, -0.1370,  ..., -0.0671, -0.2978, -0.0777]],\n",
      "\n",
      "         [[ 0.3144, -0.4314, -0.0976,  ..., -0.6191,  0.3518,  0.0996],\n",
      "          [-0.3535,  0.3509,  0.0366,  ...,  0.2078, -0.0121, -0.1180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3637,  0.2229, -0.3104,  ..., -0.1601,  0.3348, -0.0243],\n",
      "          [-0.2924,  0.7171,  0.4708,  ...,  0.2324,  0.0562,  0.0937]],\n",
      "\n",
      "         [[ 0.1152, -0.2720,  0.3808,  ...,  0.0679,  0.3752,  0.0886],\n",
      "          [-0.2201, -0.2321,  0.7762,  ..., -0.0861, -0.0677, -0.4006]],\n",
      "\n",
      "         [[ 0.3809, -0.2121,  0.2211,  ..., -0.1876,  0.4539, -0.2618],\n",
      "          [-0.4511,  0.2376, -0.0835,  ...,  0.0569, -0.5051, -0.1506]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.4441, -0.5393, -0.0223,  ..., -0.5048,  2.1634, -0.3329],\n",
      "         [ 0.1847, -1.5750,  0.5564,  ..., -0.9526, -0.1079, -0.0503],\n",
      "         [-0.9441, -1.4084,  1.0631,  ..., -0.1842,  0.8046, -0.2424],\n",
      "         ...,\n",
      "         [-0.9858, -0.2138, -0.1126,  ..., -0.6033, -1.7215, -0.2564],\n",
      "         [-1.3039, -0.4972, -0.4311,  ...,  0.3072, -0.2612, -0.7549],\n",
      "         [-1.0370, -1.5806,  0.0114,  ..., -2.3107, -0.3347, -1.5439]],\n",
      "\n",
      "        [[ 0.6372, -0.3413,  0.1306,  ..., -0.6193,  2.3129,  0.0398],\n",
      "         [ 0.4490, -2.4123,  0.5215,  ..., -0.2850,  0.8402,  0.2538],\n",
      "         [-2.0030, -0.7086,  1.9128,  ..., -0.1195,  0.1221, -1.7771],\n",
      "         ...,\n",
      "         [-0.7380,  0.5445, -0.0858,  ..., -0.1532, -1.5135,  1.0518],\n",
      "         [-0.8891, -0.4757,  0.9061,  ..., -0.7173,  0.1832, -0.1422],\n",
      "         [-0.9290, -1.4858,  0.1111,  ..., -2.5030, -0.2250, -1.2818]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[ 0.4441, -0.5393, -0.0223,  ...,  0.3669,  0.5491,  0.5893],\n",
      "          [-0.4210,  0.0095,  1.0207,  ..., -0.5048,  2.1634, -0.3329]],\n",
      "\n",
      "         [[ 0.1847, -1.5750,  0.5564,  ..., -0.2723, -1.5266,  0.4211],\n",
      "          [-0.5277,  0.2384,  0.8735,  ..., -0.9526, -0.1079, -0.0503]],\n",
      "\n",
      "         [[-0.9441, -1.4084,  1.0631,  ..., -0.4016, -1.0913,  1.3978],\n",
      "          [-0.7660,  0.2293,  1.2399,  ..., -0.1842,  0.8046, -0.2424]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9858, -0.2138, -0.1126,  ..., -0.2230, -0.4769, -0.2408],\n",
      "          [-1.3126,  1.7988,  1.5454,  ..., -0.6033, -1.7215, -0.2564]],\n",
      "\n",
      "         [[-1.3039, -0.4972, -0.4311,  ...,  0.1353, -1.7321,  0.1073],\n",
      "          [-0.1609,  0.9560,  1.6348,  ...,  0.3072, -0.2612, -0.7549]],\n",
      "\n",
      "         [[-1.0370, -1.5806,  0.0114,  ..., -0.1052, -0.7074, -1.3986],\n",
      "          [-1.5682,  0.3915,  0.5278,  ..., -2.3107, -0.3347, -1.5439]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6372, -0.3413,  0.1306,  ...,  0.2729,  0.7936,  0.4656],\n",
      "          [-0.4715, -0.0438,  0.8119,  ..., -0.6193,  2.3129,  0.0398]],\n",
      "\n",
      "         [[ 0.4490, -2.4123,  0.5215,  ..., -0.2794, -1.5323, -1.0683],\n",
      "          [-0.6989,  0.0821, -0.1766,  ..., -0.2850,  0.8402,  0.2538]],\n",
      "\n",
      "         [[-2.0030, -0.7086,  1.9128,  ..., -1.4160, -0.6513,  0.6689],\n",
      "          [ 0.3692, -0.5820,  1.3425,  ..., -0.1195,  0.1221, -1.7771]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7380,  0.5445, -0.0858,  ...,  0.1363,  0.5892, -0.9951],\n",
      "          [-1.9997,  1.3927,  2.9849,  ..., -0.1532, -1.5135,  1.0518]],\n",
      "\n",
      "         [[-0.8891, -0.4757,  0.9061,  ...,  0.4994, -1.3647,  0.3518],\n",
      "          [-0.6762,  0.6803,  0.8676,  ..., -0.7173,  0.1832, -0.1422]],\n",
      "\n",
      "         [[-0.9290, -1.4858,  0.1111,  ..., -0.1860, -0.5638, -1.4642],\n",
      "          [-1.6667,  0.3518,  0.4068,  ..., -2.5030, -0.2250, -1.2818]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          ...,\n",
      "          [-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899]],\n",
      "\n",
      "         [[-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400],\n",
      "          ...,\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          ...,\n",
      "          [ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752]],\n",
      "\n",
      "         [[-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111],\n",
      "          ...,\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.2610, -0.7053,  0.6108,  ...,  0.1763, -1.1131,  0.9039],\n",
      "          [-0.5967, -0.4931,  0.7421,  ..., -0.6388,  0.4157,  0.1927]],\n",
      "\n",
      "         [[-0.2405, -0.7568,  0.5096,  ...,  0.1344, -1.0161,  0.6688],\n",
      "          [-0.5264, -0.1480,  0.8001,  ..., -0.6372,  0.3759, -0.0773]],\n",
      "\n",
      "         [[-0.2550, -0.7811,  0.4931,  ...,  0.0974, -0.9961,  0.5845],\n",
      "          [-0.5342, -0.0443,  0.8256,  ..., -0.6519,  0.3621, -0.1400]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2259, -0.8056,  0.4609,  ...,  0.0547, -0.9714,  0.4279],\n",
      "          [-0.5517,  0.1450,  0.8527,  ..., -0.6982,  0.2665, -0.2556]],\n",
      "\n",
      "         [[-0.2352, -0.7719,  0.4878,  ...,  0.1018, -0.9962,  0.5813],\n",
      "          [-0.5360, -0.0397,  0.8454,  ..., -0.6259,  0.3758, -0.1551]],\n",
      "\n",
      "         [[-0.2382, -0.7873,  0.5128,  ...,  0.0875, -1.0221,  0.5899],\n",
      "          [-0.5350, -0.0175,  0.8267,  ..., -0.6599,  0.3377, -0.1667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4717, -0.1199,  1.0841,  ..., -0.3307, -0.7640,  0.5989],\n",
      "          [-0.9561,  0.8194,  0.9813,  ..., -0.6418, -0.1643, -0.4255]],\n",
      "\n",
      "         [[ 0.1507, -0.4389,  0.8071,  ..., -0.2482, -0.7255,  0.2914],\n",
      "          [-0.8081,  0.7962,  1.0650,  ..., -0.6400,  0.0794, -0.3724]],\n",
      "\n",
      "         [[ 0.0876, -0.4742,  0.7876,  ..., -0.2389, -0.6710,  0.1945],\n",
      "          [-0.7865,  0.8261,  1.0514,  ..., -0.6434,  0.1517, -0.3111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0393, -0.5371,  0.7145,  ..., -0.2059, -0.6760,  0.1527],\n",
      "          [-0.8023,  0.7784,  1.0851,  ..., -0.6737,  0.1346, -0.3290]],\n",
      "\n",
      "         [[ 0.1538, -0.4552,  0.8124,  ..., -0.2513, -0.7085,  0.2653],\n",
      "          [-0.8033,  0.7960,  1.0532,  ..., -0.6524,  0.1399, -0.3462]],\n",
      "\n",
      "         [[ 0.1723, -0.4267,  0.8441,  ..., -0.2564, -0.7048,  0.2752],\n",
      "          [-0.7937,  0.8159,  1.0833,  ..., -0.6527,  0.1328, -0.3377]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[-1.5273,  0.9727],\n",
      "        [-1.0469,  0.5234]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 03:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed (+ post-train 1 epoch) eval_accuracy: 0.8630\n",
      "Saved JSON-safe best params + results to: /workspace/labs/lab2/outputs/tutorial6_best_results.json\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from chop.tools import get_tokenized_dataset, get_trainer\n",
    "from chop.tools.utils import deepsetattr\n",
    "import chop.passes as passes\n",
    "from chop import MaseGraph\n",
    "from chop.pipelines import CompressionPipeline\n",
    "from chop.nn.quantized.modules.linear import LinearInteger\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "lab2_out_dir = Path(\"/workspace/labs/lab2/outputs\")\n",
    "lab2_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Search budget\n",
    "n_trials = 12\n",
    "search_epochs = 1\n",
    "\n",
    "# Compression settings\n",
    "pruning_sparsity = 0.5\n",
    "post_compress_epochs = 1\n",
    "\n",
    "HF_INPUT_NAMES = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def cleanup(*objs):\n",
    "    for o in objs:\n",
    "        try:\n",
    "            del o\n",
    "        except Exception:\n",
    "            pass\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def running_best(study: optuna.Study):\n",
    "    best = float(\"-inf\")\n",
    "    xs, ys = [], []\n",
    "    for t in sorted(study.trials, key=lambda x: x.number):\n",
    "        if t.value is None:\n",
    "            continue\n",
    "        best = max(best, float(t.value))\n",
    "        xs.append(len(xs) + 1)\n",
    "        ys.append(best)\n",
    "    return xs, ys\n",
    "\n",
    "def json_safe(obj):\n",
    "    \"\"\"Convert Optuna params that may include Python types/classes into JSON-serializable forms.\"\"\"\n",
    "    if isinstance(obj, type):\n",
    "        return f\"{obj.__module__}.{obj.__qualname__}\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: json_safe(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [json_safe(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "# -------------------------\n",
    "# Load dataset\n",
    "# -------------------------\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Load base model\n",
    "#   - Prefer tutorial_5_best_model.pkl if it exists\n",
    "#   - Otherwise, fall back to HF bert-tiny seq-cls\n",
    "# -------------------------\n",
    "base_model_path = lab2_out_dir / \"tutorial_5_best_model.pkl\"\n",
    "\n",
    "if base_model_path.exists():\n",
    "    import dill\n",
    "    with open(base_model_path, \"rb\") as f:\n",
    "        base_model = dill.load(f)\n",
    "else:\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Ensure classification-friendly config\n",
    "try:\n",
    "    base_model.config.problem_type = \"single_label_classification\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# 1) Search space (Tutorial 6 minimal & robust)\n",
    "#   Mixed precision = per-layer choice: FP Linear vs INT8 LinearInteger\n",
    "# -------------------------\n",
    "search_space = {\n",
    "    \"linear_layer_choices\": [nn.Linear, LinearInteger],\n",
    "}\n",
    "\n",
    "# Fixed config for LinearInteger (THIS FIXES \"config is None!\")\n",
    "INT8_CONFIG = {\n",
    "    \"data_in_width\": 8,\n",
    "    \"data_in_frac_width\": 4,\n",
    "    \"weight_width\": 8,\n",
    "    \"weight_frac_width\": 4,\n",
    "    \"bias_width\": 8,\n",
    "    \"bias_frac_width\": 4,\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 2) Model constructor\n",
    "# -------------------------\n",
    "def construct_model(trial: optuna.Trial):\n",
    "    trial_model = deepcopy(base_model)\n",
    "\n",
    "    # Swap every Linear layer according to trial choice\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "            if new_layer_cls is nn.Linear:\n",
    "                continue\n",
    "\n",
    "            # Build replacement layer (LinearInteger needs `config=...`)\n",
    "            kwargs = {\n",
    "                \"in_features\": layer.in_features,\n",
    "                \"out_features\": layer.out_features,\n",
    "            }\n",
    "            if new_layer_cls is LinearInteger:\n",
    "                kwargs[\"config\"] = INT8_CONFIG\n",
    "\n",
    "            new_layer = new_layer_cls(**kwargs)\n",
    "\n",
    "            # Copy weights/bias when shapes are compatible\n",
    "            with torch.no_grad():\n",
    "                if hasattr(new_layer, \"weight\") and hasattr(layer, \"weight\"):\n",
    "                    if new_layer.weight.shape == layer.weight.shape:\n",
    "                        new_layer.weight.copy_(layer.weight)\n",
    "                if layer.bias is not None and hasattr(new_layer, \"bias\") and new_layer.bias is not None:\n",
    "                    if new_layer.bias.shape == layer.bias.shape:\n",
    "                        new_layer.bias.copy_(layer.bias)\n",
    "\n",
    "            deepsetattr(trial_model, name, new_layer)\n",
    "\n",
    "    # Ensure config flag for trainer\n",
    "    try:\n",
    "        trial_model.config.problem_type = \"single_label_classification\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return trial_model\n",
    "\n",
    "# -------------------------\n",
    "# 3) Objective\n",
    "# -------------------------\n",
    "def objective(trial: optuna.Trial):\n",
    "    trainer = None\n",
    "    model = None\n",
    "    try:\n",
    "        model = construct_model(trial).to(device)\n",
    "\n",
    "        trainer = get_trainer(\n",
    "            model=model,\n",
    "            tokenized_dataset=dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            evaluate_metric=\"accuracy\",\n",
    "            num_train_epochs=search_epochs,\n",
    "        )\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        return float(eval_results[\"eval_accuracy\"])\n",
    "    finally:\n",
    "        cleanup(trainer, model)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Launch search\n",
    "#   - GridSampler is NOT suitable here because params are dynamic per-layer.\n",
    "#   - Use TPE or Random.\n",
    "# -------------------------\n",
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"tutorial6_mixed_precision_search\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "# Plot running best\n",
    "x, y = running_best(study)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, marker=\"o\")\n",
    "plt.xlabel(\"Number of trials\")\n",
    "plt.ylabel(\"Running best accuracy\")\n",
    "plt.title(\"Tutorial 6: Mixed-Precision Search (Running Best)\")\n",
    "plt.grid(True)\n",
    "running_best_path = lab3_out_dir / \"tutorial6_running_best.png\"\n",
    "plt.savefig(running_best_path)\n",
    "plt.show()\n",
    "\n",
    "print(\"Tutorial 6 search done.\")\n",
    "print(\"Best accuracy (study.best_value):\", float(study.best_value))\n",
    "print(\"Plot saved to:\", running_best_path)\n",
    "\n",
    "# -------------------------\n",
    "# Build best model from best params\n",
    "# -------------------------\n",
    "fixed_trial = optuna.trial.FixedTrial(study.best_params)\n",
    "best_model = construct_model(fixed_trial)\n",
    "\n",
    "# Quick eval of best model as-is\n",
    "trainer_best = None\n",
    "try:\n",
    "    trainer_best = get_trainer(\n",
    "        model=best_model.to(device),\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=0,\n",
    "    )\n",
    "    best_eval = trainer_best.evaluate()\n",
    "    best_eval_acc = float(best_eval[\"eval_accuracy\"])\n",
    "    print(\"Best model (as searched) eval_accuracy:\", best_eval_acc)\n",
    "finally:\n",
    "    cleanup(trainer_best)\n",
    "\n",
    "# -------------------------\n",
    "# Import into Mase + CompressionPipeline (quantize + prune)\n",
    "#   We do two variants:\n",
    "#     A) compress, evaluate immediately\n",
    "#     B) compress, post-train, evaluate\n",
    "# -------------------------\n",
    "BASE_QUANTIZATION_CONFIG = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "    \"linear\": {  # quantize remaining nn.Linear to integer\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "BASE_PRUNING_CONFIG = {\n",
    "    \"weight\": {\"sparsity\": pruning_sparsity, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "    \"activation\": {\"sparsity\": pruning_sparsity, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "}\n",
    "\n",
    "def compress_with_mase(model_cpu: nn.Module):\n",
    "    \"\"\"\n",
    "    IMPORTANT:\n",
    "      - CompressionPipeline / passes mutate the dicts you pass in (e.g. pop(\"by\")).\n",
    "      - So we must pass FRESH COPIES every call.\n",
    "    \"\"\"\n",
    "    mg = MaseGraph(model_cpu, hf_input_names=HF_INPUT_NAMES)\n",
    "    mg, _ = passes.init_metadata_analysis_pass(mg)\n",
    "    mg, _ = passes.add_common_metadata_analysis_pass(mg)\n",
    "\n",
    "    qcfg = deepcopy(BASE_QUANTIZATION_CONFIG)\n",
    "    pcfg = deepcopy(BASE_PRUNING_CONFIG)\n",
    "\n",
    "    pipe = CompressionPipeline()\n",
    "    mg, _ = pipe(\n",
    "        mg,\n",
    "        pass_args={\n",
    "            \"quantize_transform_pass\": qcfg,\n",
    "            \"prune_transform_pass\": pcfg,\n",
    "        },\n",
    "    )\n",
    "    return mg\n",
    "\n",
    "# A) compress, no post-train\n",
    "mg_a = None\n",
    "trainer_a = None\n",
    "try:\n",
    "    # use deepcopy(best_model) so A/B are independent\n",
    "    mg_a = compress_with_mase(deepcopy(best_model).to(\"cpu\"))\n",
    "    model_a = mg_a.model.to(device)\n",
    "\n",
    "    trainer_a = get_trainer(\n",
    "        model=model_a,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=0,\n",
    "    )\n",
    "    eval_a = trainer_a.evaluate()\n",
    "    acc_a = float(eval_a[\"eval_accuracy\"])\n",
    "    print(f\"Compressed (no post-train) eval_accuracy: {acc_a:.4f}\")\n",
    "finally:\n",
    "    cleanup(trainer_a, mg_a)\n",
    "\n",
    "# B) compress, with post-train\n",
    "mg_b = None\n",
    "trainer_b = None\n",
    "try:\n",
    "    mg_b = compress_with_mase(deepcopy(best_model).to(\"cpu\"))\n",
    "    model_b = mg_b.model.to(device)\n",
    "\n",
    "    trainer_b = get_trainer(\n",
    "        model=model_b,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=post_compress_epochs,\n",
    "    )\n",
    "    trainer_b.train()\n",
    "    eval_b = trainer_b.evaluate()\n",
    "    acc_b = float(eval_b[\"eval_accuracy\"])\n",
    "    print(f\"Compressed (+ post-train {post_compress_epochs} epoch) eval_accuracy: {acc_b:.4f}\")\n",
    "finally:\n",
    "    cleanup(trainer_b, mg_b)\n",
    "\n",
    "\n",
    "best_results_path = lab3_out_dir / \"tutorial6_best_results.json\"\n",
    "with best_results_path.open(\"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"search\": {\n",
    "                \"best_value\": float(study.best_value),\n",
    "                \"best_params\": json_safe(study.best_params),\n",
    "                \"best_model_eval_accuracy\": best_eval_acc,\n",
    "                \"n_trials\": len(study.trials),\n",
    "                \"search_epochs\": search_epochs,\n",
    "                \"sampler\": sampler.__class__.__name__,\n",
    "            },\n",
    "            \"compression\": {\n",
    "                \"pruning_sparsity\": pruning_sparsity,\n",
    "                \"post_compress_epochs\": post_compress_epochs,\n",
    "                \"compressed_no_post_train_eval_accuracy\": acc_a,\n",
    "                \"compressed_with_post_train_eval_accuracy\": acc_b,\n",
    "            },\n",
    "            \"artifacts\": {\n",
    "                \"running_best_plot\": str(running_best_path),\n",
    "                \"results_json\": str(best_results_path),\n",
    "            },\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"Saved JSON-safe best params + results to:\", best_results_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mase-dev)",
   "language": "python",
   "name": "mase-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
