{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how to import a model into Mase by generating a compute graph using the [MaseGraph](https://github.com/DeepWok/mase/blob/adls_2024/src/chop/ir/graph/mase_graph.py) API and how to start optimizing models using analysis and transform passes. First, we'll import a pretrained model directly from [HuggingFace Transformers](https://github.com/huggingface/transformers). For this example, we'll use Bert for sequence classification. You can read the [Bert paper](https://arxiv.org/abs/1810.04805) for information regarding the architecture.\n",
    "\n",
    "We get a warning saying that some weights were not initialized, since only the weights in the decoder are pretrained and included in the HuggingFace Hub. When we use the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/model_doc/auto) API, a classification head is added at the end of the model, with randomly initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an FX graph for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import a model into Mase, we need to generate a compute graph. In the Machine Learning community, there are several ways of capturing and representing a compute graph, such as [ONNX](https://onnx.ai/), [Torchscript](https://pytorch.org/docs/stable/jit.html), [MLIR](https://mlir.llvm.org/), [TVM](https://tvm.apache.org/), etc. Mase relies on [Torch FX](https://pytorch.org/docs/stable/fx.html), which has the following features and benefits:\n",
    "\n",
    "- **High-level IR**: unlike `LLVM` or `MLIR`, `FX` offers a high-level representation of the computation which enables fast optimizations.\n",
    "\n",
    "- **Pytorch native**: every operator in the FX graph correlates to a Python object or callable, meaning we can transform and optimize the graph, then simply regenerate the Python code required to run it. Unlike ONNX, there is no requirement for a dedicated runtime: all you need is Python.\n",
    "\n",
    "When you call `MaseGraph(model)`, the [MaseTracer](https://github.com/DeepWok/mase/blob/main/src/chop/ir/graph/mase_graph.py) class runs a forward pass of the model with `Proxy` objects instead of the regular `Tensor` objects. These Proxies record every operation performed on them, which is then used to generate the compute graph. The following cell generates the graph and generates a drawing of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "\n",
    "# Add Homebrew path for macOS (Graphviz is typically in system PATH on Linux)\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    homebrew_bin = '/opt/homebrew/bin'  # ARM64 Mac\n",
    "    if not os.path.exists(homebrew_bin):\n",
    "        homebrew_bin = '/usr/local/bin'  # Intel Mac\n",
    "    if os.path.exists(homebrew_bin):\n",
    "        os.environ['PATH'] = homebrew_bin + ':' + os.environ.get('PATH', '')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"/workspace/labs/lab1/outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "from chop import MaseGraph\n",
    "\n",
    "mg = MaseGraph(model)\n",
    "mg.draw(out_dir / \"bert-base-uncased.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 different types of nodes in an FX graph: `placeholder`, `get_attr`, `call_function`, `call_module`, `call_method`, `output`. Each node has several associated attributes, such as `name`, `args`/`kwargs` and `target`. These have different contents and meaning depending on the node type. We provide a summary below, but for more details, see the [FX documentation](https://pytorch.org/docs/stable/fx.html). \n",
    "\n",
    "- **placeholder**: represents a function input, which can be a `Tensor` or another Python object.\n",
    "\n",
    "- **get_attr**: retrieves a parameter from the Pytorch module hierarchy. `target` is the fully-qualified string name of the parameter’s position in the module hierarchy.\n",
    "\n",
    "- **call_function**: applies a free function to some values. `target` is a handle to the Python callable. `args` and `kwargs` represent the arguments to the function, following the Python calling convention.\n",
    "\n",
    "- **call_module**: applies a module in the module hierarchy’s `forward()` method with the given arguments. `target` is the fully-qualified string name of the module in the module hierarchy to call.\n",
    "\n",
    "- **call_method**: calls a method on a value. `target` is the string name of the method to apply to the self argument.\n",
    "\n",
    "- **output**: contains the output of the traced function in its args[0] attribute. This corresponds to the `return` statement in the Graph printout.\n",
    "\n",
    "You may be wondering the difference between `call_function`, `call_method` and `call_module` nodes: `call_function` nodes can have arbitrary Python callable as targets, while the target for `call_method` nodes must be a `Tensor` class method. `call_module` nodes refer to `torch.nn.Module` objects which must be included in the Pytorch module hierarchy. For example, the Pytorch ReLU activation function can be seen any of these node types:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "random_tensor = torch.randn(2, 2)\n",
    "\n",
    "function_relu = torch.relu(random_tensor)\n",
    "method_relu = random_tensor.relu()\n",
    "module_relu = torch.nn.ReLU()(random_tensor)\n",
    "\n",
    "assert torch.equal(function_relu, method_relu)\n",
    "assert torch.equal(function_relu, module_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the generated SVG file (you may find this [VSCode extension](https://marketplace.visualstudio.com/items?itemName=SimonSiefke.svg-preview) useful) and inspect each node. If you can't generate the image, we show below a segment of the graph that corresponds to the first attention layer of the Bert encoder. If you also inspect the [Bert implementation](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py) in the HuggingFace repository, you can see how each node in the generated graph corresponds to lines in the Python code. For example, the `bert_encoder_layer_0_attention_self_<query/key/value>` nodes correspond to the calls to the Query/Key/Value linear layers defined in the `BertSelfAttention` class. You can also see how not every piece of code has an associated node in the graph - when the code is being symbolically traced, parts of the code that aren't executed (for example, if statements which never yield `True`) don't interact with the `Proxy` objects, hence they're not included in the graph.\n",
    "\n",
    "<img src=\"imgs/fx_graph_bert_base_uncased_num_hidden_layers_1_segment.png\" alt=\"drawing\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Mase IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, the Mase IR is built on top of Torch FX. However, the FX operator associated with each node in the graph refers broadly to Python semantics, such as how to execute code generation for a transformed graph. For example, when the FX code generator encounters a `call_function` node, it would know to generate code equivalent to `node.target(*node.args, **node.kwargs)`, while for `call_method` nodes, the code would correspond to `getattr(node.args[0], node.target)(*args[1:], **kwargs)`. However, beyond code generation, the FX IR has no information regarding the workload being executed by the graph - that's where the Mase IR comes in. \n",
    "\n",
    "As described in previous publications, the major benefit of the Mase IR is in offering a common abstraction layer for both hardware and software workloads (see [here](https://arxiv.org/abs/2307.15517), [here](https://openreview.net/forum?id=Z7v6mxNVdU)). You can find a list of Mase operators under the [IR definition file](https://github.com/DeepWok/mase/blob/main/src/chop/ir/common.py). You can see that most operators correspond strongly with either Pytorch or ONNX operators. Each operator is also associated with a node type, which can be one of the following. \n",
    "\n",
    "-  ``module_related_func``: includes functions under ``torch.nn.functional`` and the ``torch.nn.Module`` that wraps them. For example, ``torch.nn.functional.relu`` and ``torch.nn.ReLU`` both fall under this category.\n",
    "\n",
    "-  ``module``: a MASE module is a subclass of ``torch.nn.Module`` that does not have corresponding ``torch.nn.functional`` counterpart. For example, ``torch.nn.BatchNorm2D`` is a MASE module because ``torch.nn.functional.batch_norm_2d`` does not exist.\n",
    "\n",
    "-  ``builtin_func``: MASE builtin_func includes functions under ``torch`` that are not ``torch.nn.functional`` and ``torch.nn.Module``, such as ``torch.cat`` and ``torch.bmm``.\n",
    "\n",
    "The following types are also present, which have the same meaning as in Torch FX.\n",
    "\n",
    "-  ``placeholder``: input node of a MASEGraph.\n",
    "\n",
    "-  ``get_attr``: represents the attribute of a MASE module.\n",
    "\n",
    "-  ``output``: equivalent to the return statement in the forward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Pass System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have worked with compilers, you might be familiar with the concept of a pass, which is a function that iterates over each node in the graph to perform some task. In Mase, there are two categories of passes: analysis and transform passes. \n",
    "\n",
    "- **Analysis passes**: extract some information about each node, annotate nodes with relevant data, and generate payloads to be used by subsequent passes.\n",
    "- **Transform passes**: change the topology of the graph by inserting, removing or replacing nodes.\n",
    "\n",
    "All passes, whether analysis or transform, have the following structure. Every pass accepts a dictionary `pass_args` containing required arguments, and outputs a tuple of the output graph (which can be annotated or transformed) and a `pass_outputs` dictionary. A pass doesn't need to use any arguments or generate any outputs (other than the output graph), however the argument and return signatures must follow this standard such that passes can be chained together.\n",
    "\n",
    "```python\n",
    "\n",
    "def dummy_pass(mg, pass_args={}):\n",
    "    \n",
    "    # ... do some setup \n",
    "    pass_outputs = {}\n",
    "\n",
    "    for node in mg.fx_graph.nodes:\n",
    "        # ... do stuff\n",
    "\n",
    "    return mg, pass_outputs\n",
    "\n",
    "```\n",
    "\n",
    "Next, we'll show how to run some analysis passes required to raise the generated FX graph to the Mase IR. Then, we'll come back to see how to write some simple analysis passes to do useful things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raising the FX graph to the Mase IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the simple FX graph we generated into the Mase IR, we must run the following analysis passes, which annotate each node with relevant metadata. Note that metadata follows under three categories: `common`, `hardware` and `software`. Hardware metadata is used for generating FPGA accelerators in the [emit Verilog toolflow](https://github.com/DeepWok/mase/tree/adls_2024/src/chop/passes/graph/transforms/verilog) (see Lab 4), while software metadata is used by passes such as [autosharding](https://github.com/DeepWok/mase/tree/main/src/chop/passes/graph/analysis/autosharding), which automatically finds a model parallelism configuration in a GPU cluster. Common metadata is generally required by all workflows in Mase.\n",
    "\n",
    "- **init_metadata_analysis_pass**: initializes a `MaseMetadata` object for each node in the graph, which behaves like a dictionary and is stored under `node.meta[\"mase\"]`. Each metadata instance has the following structure, which is empty at initialization. See [here](https://deepwok.github.io/mase/modules/api/analysis/init_metadata.html) for details on the implementation.\n",
    "\n",
    "```python\n",
    "        node.meta[\"mase\"] = {\n",
    "            \"common\": {},\n",
    "            \"hardware\": {},\n",
    "            \"software\": {},\n",
    "        }\n",
    "```\n",
    "\n",
    "- **add_common_metadata_analysis_pass**: populates the `node.meta[\"mase\"][\"common\"]` dictionary by executing the following two steps. See [here](https://deepwok.github.io/mase/modules/api/analysis/add_metadata.html) for details on the implementation.\n",
    "    - **Operator inference**: determine the operator associated with each node in the graph from its fx operator and target, and annotate under `node.meta[\"mase\"][\"common\"][\"mase_op\"]`\n",
    "    - **Shape Propagation**: similarly to the [Interpreter Pattern](https://pytorch.org/docs/stable/fx.html#the-interpreter-pattern) in the FX documentation, this involves running a forward pass of the entire model with a provided dummy input, and observing the Tensor metadata (shape, data type, stride, etc) of each argument and result for every node in the graph. This is then annotated under `node.meta[\"mase\"][\"common\"][\"args\"]` and `node.meta[\"mase\"][\"common\"][\"results\"]`.\n",
    "\n",
    "The `add_common_metadata_analysis_pass` requires a dummy Tensor input to run the shape propagation step. In the following cell, we show how this can be done using the HuggingFace tokenizer, to which we pass two truthful statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[ 0.7973,  0.0109, -8.8405,  ...,  1.4170,  0.1046, -0.1551],\n",
      "         [-1.1766,  1.2879, -1.0986,  ...,  0.4749, -0.5899,  0.8746],\n",
      "         [-2.0560,  0.7748, -0.8909,  ..., -0.4034,  0.5352, -1.3657],\n",
      "         ...,\n",
      "         [ 0.2317, -0.7896,  0.9634,  ..., -0.8037,  0.4834, -0.5868],\n",
      "         [ 0.0243, -1.0235, -1.2771,  ..., -2.2378,  1.8530,  0.1558],\n",
      "         [-1.3637,  0.7055, -0.2177,  ...,  0.3557, -0.3971, -0.3107]],\n",
      "\n",
      "        [[ 0.7973,  0.0109, -8.8405,  ...,  1.4170,  0.1046, -0.1551],\n",
      "         [-2.6940,  0.6198, -0.4564,  ..., -1.4367, -1.5705, -3.1260],\n",
      "         [-1.7524,  0.8535, -0.2155,  ..., -0.5222, -1.2430, -1.7199],\n",
      "         ...,\n",
      "         [-0.0347,  0.7446,  1.4462,  ..., -1.1578, -2.6197,  0.2612],\n",
      "         [ 2.4334, -0.3068,  0.8250,  ...,  0.1475,  0.1790,  2.2907],\n",
      "         [-1.3637,  0.7055, -0.2177,  ...,  0.3557, -0.3971, -0.3107]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,\n",
      "           1.8900e-01,  2.8282e-01],\n",
      "         [-3.5020e-02, -6.1047e-02, -1.0465e-01,  ..., -8.1892e-01,\n",
      "           1.1978e+00,  2.1808e+00],\n",
      "         [ 4.3982e-01, -1.9602e+00, -6.8830e-01,  ..., -6.3025e-01,\n",
      "          -1.5967e-01,  1.3284e+00],\n",
      "         ...,\n",
      "         [ 1.4783e+00,  1.0907e-01, -1.5222e+00,  ..., -3.0983e-01,\n",
      "          -1.2971e-01,  1.1265e+00],\n",
      "         [ 1.5890e+00, -1.6859e+00,  7.8703e-01,  ..., -1.3174e+00,\n",
      "           2.2258e-01,  8.8157e-01],\n",
      "         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,\n",
      "           7.2677e-02,  1.1724e-01]],\n",
      "\n",
      "        [[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,\n",
      "           1.8900e-01,  2.8282e-01],\n",
      "         [-4.4781e-01, -7.9224e-01, -2.1741e+00,  ..., -5.9181e-01,\n",
      "           1.4373e+00,  2.4267e+00],\n",
      "         [-2.5942e-01,  9.7163e-01, -3.2928e+00,  ..., -5.9773e-01,\n",
      "          -3.0482e-01,  1.4038e+00],\n",
      "         ...,\n",
      "         [ 5.1575e-02,  3.5218e-01, -3.8926e-01,  ..., -1.1508e+00,\n",
      "           7.5490e-01,  8.2911e-01],\n",
      "         [ 1.6107e+00,  6.8170e-02,  9.2537e-01,  ..., -1.5233e+00,\n",
      "          -6.0733e-01,  3.3097e-01],\n",
      "         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,\n",
      "           7.2677e-02,  1.1724e-01]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,\n",
      "           1.8900e-01,  2.8282e-01],\n",
      "         [-3.5020e-02, -6.1047e-02, -1.0465e-01,  ..., -8.1892e-01,\n",
      "           1.1978e+00,  2.1808e+00],\n",
      "         [ 4.3982e-01, -1.9602e+00, -6.8830e-01,  ..., -6.3025e-01,\n",
      "          -1.5967e-01,  1.3284e+00],\n",
      "         ...,\n",
      "         [ 1.4783e+00,  1.0907e-01, -1.5222e+00,  ..., -3.0983e-01,\n",
      "          -1.2971e-01,  1.1265e+00],\n",
      "         [ 1.5890e+00, -1.6859e+00,  7.8703e-01,  ..., -1.3174e+00,\n",
      "           2.2258e-01,  8.8157e-01],\n",
      "         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,\n",
      "           7.2677e-02,  1.1724e-01]],\n",
      "\n",
      "        [[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,\n",
      "           1.8900e-01,  2.8282e-01],\n",
      "         [-4.4781e-01, -7.9224e-01, -2.1741e+00,  ..., -5.9181e-01,\n",
      "           1.4373e+00,  2.4267e+00],\n",
      "         [-2.5942e-01,  9.7163e-01, -3.2928e+00,  ..., -5.9773e-01,\n",
      "          -3.0482e-01,  1.4038e+00],\n",
      "         ...,\n",
      "         [ 5.1575e-02,  3.5218e-01, -3.8926e-01,  ..., -1.1508e+00,\n",
      "           7.5490e-01,  8.2911e-01],\n",
      "         [ 1.6107e+00,  6.8170e-02,  9.2537e-01,  ..., -1.5233e+00,\n",
      "          -6.0733e-01,  3.3097e-01],\n",
      "         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,\n",
      "           7.2677e-02,  1.1724e-01]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ..., -5.2614e-01,\n",
      "           -3.5687e-01, -2.6793e-01],\n",
      "          [ 2.0335e-01, -5.4534e-01,  3.0686e-01,  ...,  1.4757e-01,\n",
      "            1.8900e-01,  2.8282e-01]],\n",
      "\n",
      "         [[-3.5020e-02, -6.1047e-02, -1.0465e-01,  ..., -2.0138e+00,\n",
      "            4.5529e-01, -7.8171e-01],\n",
      "          [ 1.1969e+00,  1.6337e+00,  2.5047e-01,  ..., -8.1892e-01,\n",
      "            1.1978e+00,  2.1808e+00]],\n",
      "\n",
      "         [[ 4.3982e-01, -1.9602e+00, -6.8830e-01,  ..., -2.2501e-01,\n",
      "            7.2290e-02, -1.8290e+00],\n",
      "          [ 8.9952e-01,  1.0029e+00,  7.4536e-04,  ..., -6.3025e-01,\n",
      "           -1.5967e-01,  1.3284e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4783e+00,  1.0907e-01, -1.5222e+00,  ...,  1.1867e+00,\n",
      "           -1.3561e+00,  6.5158e-01],\n",
      "          [ 9.5466e-01,  4.5887e-01,  7.8078e-01,  ..., -3.0983e-01,\n",
      "           -1.2971e-01,  1.1265e+00]],\n",
      "\n",
      "         [[ 1.5890e+00, -1.6859e+00,  7.8703e-01,  ...,  6.5467e-01,\n",
      "           -6.8451e-01,  6.5081e-01],\n",
      "          [ 7.0729e-01,  1.4499e+00, -1.5089e-01,  ..., -1.3174e+00,\n",
      "            2.2258e-01,  8.8157e-01]],\n",
      "\n",
      "         [[-3.7517e-01,  1.5191e+00, -2.6796e-01,  ...,  3.3130e-01,\n",
      "           -3.2756e-01, -6.3130e-01],\n",
      "          [ 8.6773e-01,  2.0996e-01, -3.4332e-01,  ..., -1.6159e+00,\n",
      "            7.2677e-02,  1.1724e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ..., -5.2614e-01,\n",
      "           -3.5687e-01, -2.6793e-01],\n",
      "          [ 2.0335e-01, -5.4534e-01,  3.0686e-01,  ...,  1.4757e-01,\n",
      "            1.8900e-01,  2.8282e-01]],\n",
      "\n",
      "         [[-4.4781e-01, -7.9224e-01, -2.1741e+00,  ...,  1.7508e+00,\n",
      "           -3.6708e-01, -1.3251e+00],\n",
      "          [ 7.9208e-01, -1.3537e-01,  2.3756e-01,  ..., -5.9181e-01,\n",
      "            1.4373e+00,  2.4267e+00]],\n",
      "\n",
      "         [[-2.5942e-01,  9.7163e-01, -3.2928e+00,  ..., -9.6646e-01,\n",
      "           -4.8876e-01, -1.4426e+00],\n",
      "          [ 1.0250e+00, -6.9093e-01, -1.2734e+00,  ..., -5.9773e-01,\n",
      "           -3.0482e-01,  1.4038e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1575e-02,  3.5218e-01, -3.8926e-01,  ..., -1.2252e-02,\n",
      "            1.0394e+00,  4.2402e-01],\n",
      "          [-4.7386e-01,  2.6401e+00,  1.7024e+00,  ..., -1.1508e+00,\n",
      "            7.5490e-01,  8.2911e-01]],\n",
      "\n",
      "         [[ 1.6107e+00,  6.8170e-02,  9.2537e-01,  ..., -6.1665e-01,\n",
      "            2.7627e-01, -1.2083e+00],\n",
      "          [ 9.3395e-01, -9.7541e-01, -2.5442e-02,  ..., -1.5233e+00,\n",
      "           -6.0733e-01,  3.3097e-01]],\n",
      "\n",
      "         [[-3.7517e-01,  1.5191e+00, -2.6796e-01,  ...,  3.3130e-01,\n",
      "           -3.2756e-01, -6.3130e-01],\n",
      "          [ 8.6773e-01,  2.0996e-01, -3.4332e-01,  ..., -1.6159e+00,\n",
      "            7.2677e-02,  1.1724e-01]]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],\n",
      "         [-0.5842,  0.9588,  1.5642,  ..., -1.0731, -0.7330,  0.3132],\n",
      "         [-0.8601, -1.3756,  0.5042,  ..., -0.0476,  0.2650,  1.2150],\n",
      "         ...,\n",
      "         [ 0.0520,  1.1719, -1.5471,  ..., -0.7894,  0.1419,  1.6964],\n",
      "         [ 0.7654, -1.5053, -0.4142,  ..., -1.4622, -0.8975,  1.4576],\n",
      "         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]],\n",
      "\n",
      "        [[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],\n",
      "         [-1.3806,  0.2626, -0.5207,  ..., -1.6714, -0.0554,  1.0225],\n",
      "         [-1.7116,  1.8788, -2.5695,  ..., -0.6958,  0.5728,  0.5461],\n",
      "         ...,\n",
      "         [-1.3246,  1.2196, -0.3034,  ..., -1.1955, -0.6708,  0.5128],\n",
      "         [ 0.9854,  0.8260,  0.2892,  ..., -0.6428,  0.3637,  0.4339],\n",
      "         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],\n",
      "         [-0.5842,  0.9588,  1.5642,  ..., -1.0731, -0.7330,  0.3132],\n",
      "         [-0.8601, -1.3756,  0.5042,  ..., -0.0476,  0.2650,  1.2150],\n",
      "         ...,\n",
      "         [ 0.0520,  1.1719, -1.5471,  ..., -0.7894,  0.1419,  1.6964],\n",
      "         [ 0.7654, -1.5053, -0.4142,  ..., -1.4622, -0.8975,  1.4576],\n",
      "         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]],\n",
      "\n",
      "        [[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],\n",
      "         [-1.3806,  0.2626, -0.5207,  ..., -1.6714, -0.0554,  1.0225],\n",
      "         [-1.7116,  1.8788, -2.5695,  ..., -0.6958,  0.5728,  0.5461],\n",
      "         ...,\n",
      "         [-1.3246,  1.2196, -0.3034,  ..., -1.1955, -0.6708,  0.5128],\n",
      "         [ 0.9854,  0.8260,  0.2892,  ..., -0.6428,  0.3637,  0.4339],\n",
      "         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.1709,  0.5230, -0.8713,  ...,  0.4365,  0.6238, -0.9414],\n",
      "          [-1.3731,  1.1521,  0.1321,  ..., -1.3382,  0.5892,  0.4026]],\n",
      "\n",
      "         [[-0.5842,  0.9588,  1.5642,  ..., -1.5431,  0.4999, -1.1350],\n",
      "          [ 0.9615,  0.8694,  0.0998,  ..., -1.0731, -0.7330,  0.3132]],\n",
      "\n",
      "         [[-0.8601, -1.3756,  0.5042,  ...,  0.9764, -0.8321, -1.0204],\n",
      "          [ 1.5175,  1.1454,  0.7791,  ..., -0.0476,  0.2650,  1.2150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0520,  1.1719, -1.5471,  ...,  1.9402, -1.1294,  0.4793],\n",
      "          [ 1.0053,  0.8099,  1.6415,  ..., -0.7894,  0.1419,  1.6964]],\n",
      "\n",
      "         [[ 0.7654, -1.5053, -0.4142,  ...,  1.7455, -0.7326,  1.5248],\n",
      "          [ 1.0806,  1.1457,  2.2163,  ..., -1.4622, -0.8975,  1.4576]],\n",
      "\n",
      "         [[-1.2008, -0.6008, -1.4608,  ...,  2.0905,  1.8849, -1.5708],\n",
      "          [ 1.9999,  0.3493, -0.8524,  ..., -1.2105, -0.4289,  0.3827]]],\n",
      "\n",
      "\n",
      "        [[[-0.1709,  0.5230, -0.8713,  ...,  0.4365,  0.6238, -0.9414],\n",
      "          [-1.3731,  1.1521,  0.1321,  ..., -1.3382,  0.5892,  0.4026]],\n",
      "\n",
      "         [[-1.3806,  0.2626, -0.5207,  ...,  1.6517, -0.2316, -1.3171],\n",
      "          [ 0.6812, -0.0090,  0.3803,  ..., -1.6714, -0.0554,  1.0225]],\n",
      "\n",
      "         [[-1.7116,  1.8788, -2.5695,  ...,  0.4927, -0.4850, -1.0645],\n",
      "          [ 1.2646,  1.6481,  0.9055,  ..., -0.6958,  0.5728,  0.5461]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3246,  1.2196, -0.3034,  ...,  1.2747,  1.2353,  0.2825],\n",
      "          [ 1.5373,  0.8648,  0.6062,  ..., -1.1955, -0.6708,  0.5128]],\n",
      "\n",
      "         [[ 0.9854,  0.8260,  0.2892,  ...,  1.3848, -0.0103, -1.0700],\n",
      "          [ 1.3827,  2.9809,  0.0276,  ..., -0.6428,  0.3637,  0.4339]],\n",
      "\n",
      "         [[-1.2008, -0.6008, -1.4608,  ...,  2.0905,  1.8849, -1.5708],\n",
      "          [ 1.9999,  0.3493, -0.8524,  ..., -1.2105, -0.4289,  0.3827]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],\n",
      "         [-1.1465, -1.5578, -0.6984,  ...,  1.0310,  0.4824, -0.2291],\n",
      "         [-1.0361, -1.8192, -2.3055,  ...,  1.5286, -1.5941,  1.1762],\n",
      "         ...,\n",
      "         [-0.7992,  0.0886,  0.4887,  ..., -1.7941,  0.4835,  1.3780],\n",
      "         [-1.4692, -0.9135, -0.2802,  ..., -0.9691,  0.3500,  1.8863],\n",
      "         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]],\n",
      "\n",
      "        [[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],\n",
      "         [-0.3700, -1.9754, -0.7315,  ...,  0.2293,  0.6996,  3.1299],\n",
      "         [-0.6252,  0.2879, -1.4036,  ..., -2.0560, -2.4623, -0.9584],\n",
      "         ...,\n",
      "         [-1.1306, -1.4343, -1.4422,  ..., -1.6115, -0.0475,  1.3975],\n",
      "         [-0.9816, -1.4909, -1.0086,  ..., -0.9284,  0.5260,  1.5330],\n",
      "         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],\n",
      "         [-1.1465, -1.5578, -0.6984,  ...,  1.0310,  0.4824, -0.2291],\n",
      "         [-1.0361, -1.8192, -2.3055,  ...,  1.5286, -1.5941,  1.1762],\n",
      "         ...,\n",
      "         [-0.7992,  0.0886,  0.4887,  ..., -1.7941,  0.4835,  1.3780],\n",
      "         [-1.4692, -0.9135, -0.2802,  ..., -0.9691,  0.3500,  1.8863],\n",
      "         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]],\n",
      "\n",
      "        [[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],\n",
      "         [-0.3700, -1.9754, -0.7315,  ...,  0.2293,  0.6996,  3.1299],\n",
      "         [-0.6252,  0.2879, -1.4036,  ..., -2.0560, -2.4623, -0.9584],\n",
      "         ...,\n",
      "         [-1.1306, -1.4343, -1.4422,  ..., -1.6115, -0.0475,  1.3975],\n",
      "         [-0.9816, -1.4909, -1.0086,  ..., -0.9284,  0.5260,  1.5330],\n",
      "         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0123,  0.5761,  0.2209,  ..., -0.1457, -0.7538,  0.1761],\n",
      "          [-0.0705,  0.9215,  0.7990,  ..., -0.1027,  1.1061, -2.5200]],\n",
      "\n",
      "         [[-1.1465, -1.5578, -0.6984,  ...,  0.0289, -2.1112, -0.8728],\n",
      "          [ 0.6506, -1.6966,  1.4463,  ...,  1.0310,  0.4824, -0.2291]],\n",
      "\n",
      "         [[-1.0361, -1.8192, -2.3055,  ..., -0.2195, -1.1732,  0.3182],\n",
      "          [-0.5841, -0.0227,  3.0901,  ...,  1.5286, -1.5941,  1.1762]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7992,  0.0886,  0.4887,  ...,  0.7859, -1.0127, -0.2676],\n",
      "          [-0.3055,  0.6270, -3.0705,  ..., -1.7941,  0.4835,  1.3780]],\n",
      "\n",
      "         [[-1.4692, -0.9135, -0.2802,  ...,  0.1197, -0.7532,  0.0731],\n",
      "          [ 0.6096, -1.0893, -0.6959,  ..., -0.9691,  0.3500,  1.8863]],\n",
      "\n",
      "         [[-0.5760, -0.0452,  0.4230,  ...,  0.8851,  0.3078,  0.8106],\n",
      "          [-1.1804,  0.9512,  0.3169,  ..., -0.7179, -0.7858,  1.6879]]],\n",
      "\n",
      "\n",
      "        [[[-0.0123,  0.5761,  0.2209,  ..., -0.1457, -0.7538,  0.1761],\n",
      "          [-0.0705,  0.9215,  0.7990,  ..., -0.1027,  1.1061, -2.5200]],\n",
      "\n",
      "         [[-0.3700, -1.9754, -0.7315,  ...,  0.5756, -1.5559,  0.0326],\n",
      "          [ 1.4229,  2.3970, -0.4516,  ...,  0.2293,  0.6996,  3.1299]],\n",
      "\n",
      "         [[-0.6252,  0.2879, -1.4036,  ...,  0.5306, -0.5608,  1.1861],\n",
      "          [-2.5980,  0.2673,  3.3016,  ..., -2.0560, -2.4623, -0.9584]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1306, -1.4343, -1.4422,  ...,  0.3918, -1.5336, -0.5026],\n",
      "          [ 1.8587,  0.8501, -1.2402,  ..., -1.6115, -0.0475,  1.3975]],\n",
      "\n",
      "         [[-0.9816, -1.4909, -1.0086,  ...,  0.2956,  0.0351, -1.0685],\n",
      "          [-0.6594, -0.0133, -1.1863,  ..., -0.9284,  0.5260,  1.5330]],\n",
      "\n",
      "         [[-0.5760, -0.0452,  0.4230,  ...,  0.8851,  0.3078,  0.8106],\n",
      "          [-1.1804,  0.9512,  0.3169,  ..., -0.7179, -0.7858,  1.6879]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.5911, -0.4682, -0.4314,  ...,  0.0366, -1.0405, -0.0579],\n",
      "          [-1.1141, -1.4785, -0.6477,  ...,  0.0631, -1.9950, -0.7912],\n",
      "          [-0.7059, -0.9954, -1.3923,  ..., -0.1557, -0.9998,  0.2774],\n",
      "          ...,\n",
      "          [-0.6300,  0.1252,  0.3486,  ...,  0.5692, -0.9417, -0.1399],\n",
      "          [-1.0593, -0.6395, -0.3343,  ...,  0.0571, -0.7842,  0.1042],\n",
      "          [-0.2081,  0.2785,  0.0613,  ..., -0.0353, -0.8389, -0.0026]],\n",
      "\n",
      "         [[-0.3266,  0.6360,  0.0214,  ...,  0.1677,  0.3883,  0.4382],\n",
      "          [-0.6299,  0.6012,  0.7379,  ...,  0.2989, -0.1569,  0.9508],\n",
      "          [-0.4097,  0.6374, -0.1589,  ...,  0.0258, -0.0364,  0.9990],\n",
      "          ...,\n",
      "          [-0.0088,  0.1378, -0.4819,  ..., -0.1261,  0.2908,  1.0980],\n",
      "          [-0.5584,  0.9932, -0.1105,  ...,  0.2486,  0.4005,  0.6046],\n",
      "          [-0.4516,  0.8092, -0.0513,  ...,  0.1182,  0.4294,  0.4781]]],\n",
      "\n",
      "\n",
      "        [[[-0.6850, -0.2168, -0.6087,  ...,  0.1402, -0.7267, -0.1502],\n",
      "          [-0.2084, -0.1325, -0.1151,  ...,  0.0938, -0.8963,  0.1296],\n",
      "          [-0.3665,  0.3408, -0.6133,  ...,  0.1938, -0.6681,  0.5929],\n",
      "          ...,\n",
      "          [-1.0985, -1.2861, -1.2531,  ...,  0.3609, -1.4022, -0.4444],\n",
      "          [-0.9787, -1.4207, -0.9908,  ...,  0.2989, -0.0159, -1.0060],\n",
      "          [-0.1970,  0.2371, -0.0115,  ...,  0.0049, -0.8135,  0.1201]],\n",
      "\n",
      "         [[ 0.0907,  0.7927, -0.0524,  ..., -0.4610, -0.4295,  0.4391],\n",
      "          [-0.2352,  1.0586, -0.1117,  ..., -0.4943, -0.6546,  0.6617],\n",
      "          [ 0.8269,  1.7861, -1.1207,  ..., -0.0885,  0.2791,  1.4721],\n",
      "          ...,\n",
      "          [-0.1038,  0.9623, -0.7062,  ..., -0.3340, -0.3050,  0.8792],\n",
      "          [ 0.0507,  0.6634, -0.2642,  ..., -0.4959, -0.7944,  0.7687],\n",
      "          [ 0.3364,  0.9047, -0.2037,  ..., -0.3705, -0.2893,  0.6787]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.5911, -0.4682, -0.4314,  ...,  0.0366, -1.0405, -0.0579],\n",
      "          [-0.3266,  0.6360,  0.0214,  ...,  0.1677,  0.3883,  0.4382]],\n",
      "\n",
      "         [[-1.1141, -1.4785, -0.6477,  ...,  0.0631, -1.9950, -0.7912],\n",
      "          [-0.6299,  0.6012,  0.7379,  ...,  0.2989, -0.1569,  0.9508]],\n",
      "\n",
      "         [[-0.7059, -0.9954, -1.3923,  ..., -0.1557, -0.9998,  0.2774],\n",
      "          [-0.4097,  0.6374, -0.1589,  ...,  0.0258, -0.0364,  0.9990]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6300,  0.1252,  0.3486,  ...,  0.5692, -0.9417, -0.1399],\n",
      "          [-0.0088,  0.1378, -0.4819,  ..., -0.1261,  0.2908,  1.0980]],\n",
      "\n",
      "         [[-1.0593, -0.6395, -0.3343,  ...,  0.0571, -0.7842,  0.1042],\n",
      "          [-0.5584,  0.9932, -0.1105,  ...,  0.2486,  0.4005,  0.6046]],\n",
      "\n",
      "         [[-0.2081,  0.2785,  0.0613,  ..., -0.0353, -0.8389, -0.0026],\n",
      "          [-0.4516,  0.8092, -0.0513,  ...,  0.1182,  0.4294,  0.4781]]],\n",
      "\n",
      "\n",
      "        [[[-0.6850, -0.2168, -0.6087,  ...,  0.1402, -0.7267, -0.1502],\n",
      "          [ 0.0907,  0.7927, -0.0524,  ..., -0.4610, -0.4295,  0.4391]],\n",
      "\n",
      "         [[-0.2084, -0.1325, -0.1151,  ...,  0.0938, -0.8963,  0.1296],\n",
      "          [-0.2352,  1.0586, -0.1117,  ..., -0.4943, -0.6546,  0.6617]],\n",
      "\n",
      "         [[-0.3665,  0.3408, -0.6133,  ...,  0.1938, -0.6681,  0.5929],\n",
      "          [ 0.8269,  1.7861, -1.1207,  ..., -0.0885,  0.2791,  1.4721]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0985, -1.2861, -1.2531,  ...,  0.3609, -1.4022, -0.4444],\n",
      "          [-0.1038,  0.9623, -0.7062,  ..., -0.3340, -0.3050,  0.8792]],\n",
      "\n",
      "         [[-0.9787, -1.4207, -0.9908,  ...,  0.2989, -0.0159, -1.0060],\n",
      "          [ 0.0507,  0.6634, -0.2642,  ..., -0.4959, -0.7944,  0.7687]],\n",
      "\n",
      "         [[-0.1970,  0.2371, -0.0115,  ...,  0.0049, -0.8135,  0.1201],\n",
      "          [ 0.3364,  0.9047, -0.2037,  ..., -0.3705, -0.2893,  0.6787]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-0.9552,  0.6594, -6.5403,  ..., -0.7144,  0.0906,  0.3369],\n",
      "         [-2.5251,  1.3955, -0.8914,  ..., -2.1363,  0.0271,  1.1132],\n",
      "         [-3.7148,  0.6796, -0.8710,  ..., -2.6492,  0.5694, -0.1085],\n",
      "         ...,\n",
      "         [-2.2403, -0.7594,  0.5414,  ..., -3.0426,  0.8895, -0.0546],\n",
      "         [-1.6945, -0.6326, -0.8632,  ..., -4.0678,  1.7219,  0.6481],\n",
      "         [-2.9625,  0.7451, -0.8037,  ..., -2.5048,  0.3125,  0.5537]],\n",
      "\n",
      "        [[-0.5150,  0.8150, -6.5015,  ..., -0.5377, -0.4171,  0.1350],\n",
      "         [-2.9979,  1.0930, -0.2619,  ..., -3.1811, -1.0048, -1.8349],\n",
      "         [-2.8788,  0.5405, -0.0789,  ..., -2.3969, -0.7016, -0.7332],\n",
      "         ...,\n",
      "         [-1.7194,  1.5158,  1.0070,  ..., -2.8931, -2.3309,  1.1685],\n",
      "         [ 0.0717, -0.1039,  0.5084,  ..., -2.1932,  0.0751,  2.8236],\n",
      "         [-2.4774,  0.7563, -0.7502,  ..., -2.1312, -0.0685,  0.8700]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.0455,  0.6529,  0.6297,  ...,  0.4139, -0.9381,  0.6769],\n",
      "         [-3.1104, -3.7282, -2.3953,  ..., -0.9155, -0.8280, -1.7070],\n",
      "         [-0.9324, -2.9333, -2.3249,  ..., -0.8455, -0.0326, -0.6998],\n",
      "         ...,\n",
      "         [-1.9000, -1.1028, -1.1281,  ..., -0.2809,  2.0206, -1.0802],\n",
      "         [-1.1088, -1.0420, -2.4026,  ..., -0.4478,  0.7391, -0.0354],\n",
      "         [ 0.7349,  0.6742, -2.6697,  ..., -0.5114,  1.5155,  2.0246]],\n",
      "\n",
      "        [[-0.0799,  0.7813,  0.4918,  ...,  0.6888, -0.7680,  0.9805],\n",
      "         [-2.8720, -1.0602, -2.3610,  ..., -2.1143,  0.9664, -1.1212],\n",
      "         [-1.4705, -2.1384, -1.9955,  ..., -0.9722,  1.5909, -0.1668],\n",
      "         ...,\n",
      "         [-2.9884, -1.1566, -2.5215,  ...,  1.1460,  0.7120, -0.6320],\n",
      "         [-3.3666, -0.7966, -3.3154,  ...,  0.5316,  1.7058,  2.1950],\n",
      "         [ 0.6626,  0.8537, -2.7251,  ..., -0.0901,  1.5883,  2.3840]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0455,  0.6529,  0.6297,  ...,  0.4139, -0.9381,  0.6769],\n",
      "         [-3.1104, -3.7282, -2.3953,  ..., -0.9155, -0.8280, -1.7070],\n",
      "         [-0.9324, -2.9333, -2.3249,  ..., -0.8455, -0.0326, -0.6998],\n",
      "         ...,\n",
      "         [-1.9000, -1.1028, -1.1281,  ..., -0.2809,  2.0206, -1.0802],\n",
      "         [-1.1088, -1.0420, -2.4026,  ..., -0.4478,  0.7391, -0.0354],\n",
      "         [ 0.7349,  0.6742, -2.6697,  ..., -0.5114,  1.5155,  2.0246]],\n",
      "\n",
      "        [[-0.0799,  0.7813,  0.4918,  ...,  0.6888, -0.7680,  0.9805],\n",
      "         [-2.8720, -1.0602, -2.3610,  ..., -2.1143,  0.9664, -1.1212],\n",
      "         [-1.4705, -2.1384, -1.9955,  ..., -0.9722,  1.5909, -0.1668],\n",
      "         ...,\n",
      "         [-2.9884, -1.1566, -2.5215,  ...,  1.1460,  0.7120, -0.6320],\n",
      "         [-3.3666, -0.7966, -3.3154,  ...,  0.5316,  1.7058,  2.1950],\n",
      "         [ 0.6626,  0.8537, -2.7251,  ..., -0.0901,  1.5883,  2.3840]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0455,  0.6529,  0.6297,  ...,  1.0165, -1.6055,  0.0557],\n",
      "          [-0.9141, -1.5101, -0.8415,  ...,  0.4139, -0.9381,  0.6769]],\n",
      "\n",
      "         [[-3.1104, -3.7282, -2.3953,  ..., -1.6195,  0.7426, -3.2794],\n",
      "          [-1.2694,  0.3821, -0.5687,  ..., -0.9155, -0.8280, -1.7070]],\n",
      "\n",
      "         [[-0.9324, -2.9333, -2.3249,  ..., -1.0254,  1.8158, -1.8835],\n",
      "          [-1.5265, -0.3901,  0.2734,  ..., -0.8455, -0.0326, -0.6998]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9000, -1.1028, -1.1281,  ..., -1.2688, -0.0851, -2.3190],\n",
      "          [-2.4374,  0.0718, -2.7276,  ..., -0.2809,  2.0206, -1.0802]],\n",
      "\n",
      "         [[-1.1088, -1.0420, -2.4026,  ..., -1.0658,  0.1932, -1.7012],\n",
      "          [-2.3622, -0.5291, -1.9931,  ..., -0.4478,  0.7391, -0.0354]],\n",
      "\n",
      "         [[ 0.7349,  0.6742, -2.6697,  ..., -1.4630, -0.1686, -2.5682],\n",
      "          [-0.1401, -0.9712, -2.3801,  ..., -0.5114,  1.5155,  2.0246]]],\n",
      "\n",
      "\n",
      "        [[[-0.0799,  0.7813,  0.4918,  ...,  1.2364, -1.9500, -0.1275],\n",
      "          [-0.4080, -1.5069, -0.8504,  ...,  0.6888, -0.7680,  0.9805]],\n",
      "\n",
      "         [[-2.8720, -1.0602, -2.3610,  ..., -2.3225, -0.0351, -2.7432],\n",
      "          [-0.2305, -0.5940, -1.1570,  ..., -2.1143,  0.9664, -1.1212]],\n",
      "\n",
      "         [[-1.4705, -2.1384, -1.9955,  ..., -0.6524, -1.8025, -1.8321],\n",
      "          [-1.7742, -0.6800, -0.2172,  ..., -0.9722,  1.5909, -0.1668]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9884, -1.1566, -2.5215,  ..., -0.5054, -1.0314, -3.4883],\n",
      "          [-1.9535,  0.5573, -2.1564,  ...,  1.1460,  0.7120, -0.6320]],\n",
      "\n",
      "         [[-3.3666, -0.7966, -3.3154,  ...,  0.7587, -0.6289, -3.4848],\n",
      "          [-1.4099, -2.0919, -1.5870,  ...,  0.5316,  1.7058,  2.1950]],\n",
      "\n",
      "         [[ 0.6626,  0.8537, -2.7251,  ..., -1.1831, -0.7083, -2.7717],\n",
      "          [ 0.4486, -1.1639, -2.1203,  ..., -0.0901,  1.5883,  2.3840]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.8947,  0.0412, -1.2359,  ...,  0.4410, -0.3965,  0.0106],\n",
      "         [-1.9196,  0.3326,  0.8482,  ..., -1.5790, -1.1817, -1.0156],\n",
      "         [-2.1664,  0.3959,  0.7476,  ..., -2.1767, -0.6488,  0.1889],\n",
      "         ...,\n",
      "         [-1.6009,  0.4887, -0.4818,  ..., -1.1268,  0.4111,  0.7892],\n",
      "         [-0.1528,  1.1728, -0.5164,  ..., -0.4340,  0.1499,  1.6704],\n",
      "         [ 1.0253,  1.4222, -0.1805,  ..., -0.6130, -0.5380,  1.6164]],\n",
      "\n",
      "        [[-0.6736, -0.0718, -1.1724,  ...,  0.2001, -0.5481,  0.0232],\n",
      "         [-1.8698, -1.2184,  0.2913,  ..., -1.1398, -1.3523, -0.7851],\n",
      "         [-1.3725, -0.8212,  0.1984,  ..., -1.8218, -1.4800, -0.2956],\n",
      "         ...,\n",
      "         [-0.5946,  0.5680,  0.8938,  ..., -1.6653,  0.8218,  1.1902],\n",
      "         [ 1.2800,  1.9566,  0.2540,  ..., -1.2290,  0.5257,  1.2667],\n",
      "         [ 1.3511,  1.3329, -0.0782,  ..., -0.8454, -0.7400,  1.5966]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.8947,  0.0412, -1.2359,  ...,  0.4410, -0.3965,  0.0106],\n",
      "         [-1.9196,  0.3326,  0.8482,  ..., -1.5790, -1.1817, -1.0156],\n",
      "         [-2.1664,  0.3959,  0.7476,  ..., -2.1767, -0.6488,  0.1889],\n",
      "         ...,\n",
      "         [-1.6009,  0.4887, -0.4818,  ..., -1.1268,  0.4111,  0.7892],\n",
      "         [-0.1528,  1.1728, -0.5164,  ..., -0.4340,  0.1499,  1.6704],\n",
      "         [ 1.0253,  1.4222, -0.1805,  ..., -0.6130, -0.5380,  1.6164]],\n",
      "\n",
      "        [[-0.6736, -0.0718, -1.1724,  ...,  0.2001, -0.5481,  0.0232],\n",
      "         [-1.8698, -1.2184,  0.2913,  ..., -1.1398, -1.3523, -0.7851],\n",
      "         [-1.3725, -0.8212,  0.1984,  ..., -1.8218, -1.4800, -0.2956],\n",
      "         ...,\n",
      "         [-0.5946,  0.5680,  0.8938,  ..., -1.6653,  0.8218,  1.1902],\n",
      "         [ 1.2800,  1.9566,  0.2540,  ..., -1.2290,  0.5257,  1.2667],\n",
      "         [ 1.3511,  1.3329, -0.0782,  ..., -0.8454, -0.7400,  1.5966]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.8947,  0.0412, -1.2359,  ...,  1.5140, -1.9812, -2.5532],\n",
      "          [-0.2951, -1.6086, -0.6381,  ...,  0.4410, -0.3965,  0.0106]],\n",
      "\n",
      "         [[-1.9196,  0.3326,  0.8482,  ..., -2.3348,  1.3935,  1.1452],\n",
      "          [-0.5277,  0.1234,  0.7865,  ..., -1.5790, -1.1817, -1.0156]],\n",
      "\n",
      "         [[-2.1664,  0.3959,  0.7476,  ..., -2.0817,  0.2852,  0.8173],\n",
      "          [-0.8414,  0.5154, -0.4553,  ..., -2.1767, -0.6488,  0.1889]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6009,  0.4887, -0.4818,  ..., -1.8165,  1.4764,  0.5091],\n",
      "          [-0.6869,  0.4007, -1.5818,  ..., -1.1268,  0.4111,  0.7892]],\n",
      "\n",
      "         [[-0.1528,  1.1728, -0.5164,  ..., -1.3611,  1.0621,  1.1810],\n",
      "          [-0.7595, -0.1699, -1.5305,  ..., -0.4340,  0.1499,  1.6704]],\n",
      "\n",
      "         [[ 1.0253,  1.4222, -0.1805,  ..., -0.6989,  0.4721,  2.6129],\n",
      "          [-1.2381, -0.4573, -1.7561,  ..., -0.6130, -0.5380,  1.6164]]],\n",
      "\n",
      "\n",
      "        [[[-0.6736, -0.0718, -1.1724,  ...,  1.4816, -1.7920, -2.5177],\n",
      "          [-0.3929, -1.5120, -0.5353,  ...,  0.2001, -0.5481,  0.0232]],\n",
      "\n",
      "         [[-1.8698, -1.2184,  0.2913,  ..., -1.5227,  1.9764,  0.6389],\n",
      "          [-0.4202,  0.4572, -1.0780,  ..., -1.1398, -1.3523, -0.7851]],\n",
      "\n",
      "         [[-1.3725, -0.8212,  0.1984,  ..., -2.1553,  1.7041,  0.7166],\n",
      "          [-1.0124,  0.9351, -0.0954,  ..., -1.8218, -1.4800, -0.2956]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5946,  0.5680,  0.8938,  ..., -2.1904,  1.7986,  1.0902],\n",
      "          [-1.3820,  1.0268, -1.0041,  ..., -1.6653,  0.8218,  1.1902]],\n",
      "\n",
      "         [[ 1.2800,  1.9566,  0.2540,  ..., -1.6180,  1.6176,  2.5636],\n",
      "          [-2.0592,  0.7059, -1.3359,  ..., -1.2290,  0.5257,  1.2667]],\n",
      "\n",
      "         [[ 1.3511,  1.3329, -0.0782,  ..., -0.5836,  0.6491,  2.6554],\n",
      "          [-1.3686, -0.2348, -1.7438,  ..., -0.8454, -0.7400,  1.5966]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 3.8307e-01,  1.8836e-02, -1.0314e+00,  ...,  6.4335e-01,\n",
      "          -3.1830e-01, -1.7296e+00],\n",
      "         [ 1.5897e+00,  1.3689e-01, -6.8915e-01,  ...,  1.5973e+00,\n",
      "           1.1907e+00, -9.0454e-01],\n",
      "         [-5.8138e-01,  6.7943e-01, -1.3203e+00,  ..., -5.3627e-01,\n",
      "          -1.0456e+00, -1.6301e+00],\n",
      "         ...,\n",
      "         [-1.7466e-01,  3.0707e-02,  7.5225e-01,  ..., -1.3217e+00,\n",
      "          -1.3415e+00, -3.8328e-01],\n",
      "         [ 1.5170e-01,  5.1089e-01,  1.3993e-01,  ..., -1.6600e-01,\n",
      "          -6.5011e-01,  2.1798e-02],\n",
      "         [-2.4311e-01,  1.6726e+00,  1.6682e-01,  ...,  1.3441e-03,\n",
      "          -1.6754e+00,  3.1771e-01]],\n",
      "\n",
      "        [[ 4.7844e-01, -3.1772e-01, -1.0617e+00,  ...,  6.4928e-01,\n",
      "          -3.2944e-01, -2.4185e+00],\n",
      "         [ 5.9122e-01, -2.2648e-01,  1.7474e-01,  ..., -1.8623e+00,\n",
      "          -1.1230e+00,  3.5013e-01],\n",
      "         [ 1.1642e-01,  1.2460e+00,  7.7942e-02,  ...,  6.4975e-01,\n",
      "          -7.3862e-01, -2.1510e+00],\n",
      "         ...,\n",
      "         [ 1.1291e+00, -1.3637e+00, -1.5779e+00,  ...,  1.7637e+00,\n",
      "           9.1331e-01, -1.7033e+00],\n",
      "         [ 1.5909e+00, -1.4922e+00,  1.0060e+00,  ...,  9.8096e-01,\n",
      "           8.6736e-01, -2.3894e+00],\n",
      "         [ 1.8451e-01,  1.2740e+00,  4.2857e-01,  ...,  6.2708e-01,\n",
      "          -1.3601e+00, -3.9984e-01]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 3.8307e-01,  1.8836e-02, -1.0314e+00,  ...,  6.4335e-01,\n",
      "          -3.1830e-01, -1.7296e+00],\n",
      "         [ 1.5897e+00,  1.3689e-01, -6.8915e-01,  ...,  1.5973e+00,\n",
      "           1.1907e+00, -9.0454e-01],\n",
      "         [-5.8138e-01,  6.7943e-01, -1.3203e+00,  ..., -5.3627e-01,\n",
      "          -1.0456e+00, -1.6301e+00],\n",
      "         ...,\n",
      "         [-1.7466e-01,  3.0707e-02,  7.5225e-01,  ..., -1.3217e+00,\n",
      "          -1.3415e+00, -3.8328e-01],\n",
      "         [ 1.5170e-01,  5.1089e-01,  1.3993e-01,  ..., -1.6600e-01,\n",
      "          -6.5011e-01,  2.1798e-02],\n",
      "         [-2.4311e-01,  1.6726e+00,  1.6682e-01,  ...,  1.3441e-03,\n",
      "          -1.6754e+00,  3.1771e-01]],\n",
      "\n",
      "        [[ 4.7844e-01, -3.1772e-01, -1.0617e+00,  ...,  6.4928e-01,\n",
      "          -3.2944e-01, -2.4185e+00],\n",
      "         [ 5.9122e-01, -2.2648e-01,  1.7474e-01,  ..., -1.8623e+00,\n",
      "          -1.1230e+00,  3.5013e-01],\n",
      "         [ 1.1642e-01,  1.2460e+00,  7.7942e-02,  ...,  6.4975e-01,\n",
      "          -7.3862e-01, -2.1510e+00],\n",
      "         ...,\n",
      "         [ 1.1291e+00, -1.3637e+00, -1.5779e+00,  ...,  1.7637e+00,\n",
      "           9.1331e-01, -1.7033e+00],\n",
      "         [ 1.5909e+00, -1.4922e+00,  1.0060e+00,  ...,  9.8096e-01,\n",
      "           8.6736e-01, -2.3894e+00],\n",
      "         [ 1.8451e-01,  1.2740e+00,  4.2857e-01,  ...,  6.2708e-01,\n",
      "          -1.3601e+00, -3.9984e-01]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 3.8307e-01,  1.8836e-02, -1.0314e+00,  ..., -8.0955e-01,\n",
      "           -2.8557e-01, -2.3318e-01],\n",
      "          [-9.9661e-02,  3.1722e-01, -3.0517e-01,  ...,  6.4335e-01,\n",
      "           -3.1830e-01, -1.7296e+00]],\n",
      "\n",
      "         [[ 1.5897e+00,  1.3689e-01, -6.8915e-01,  ..., -7.9549e-01,\n",
      "           -6.9279e-01, -1.8082e-01],\n",
      "          [-9.9201e-01,  9.4938e-01,  4.4198e-02,  ...,  1.5973e+00,\n",
      "            1.1907e+00, -9.0454e-01]],\n",
      "\n",
      "         [[-5.8138e-01,  6.7943e-01, -1.3203e+00,  ..., -1.8905e+00,\n",
      "            1.6226e-01, -1.2953e+00],\n",
      "          [-7.0312e-01, -6.4926e-01, -5.0913e-01,  ..., -5.3627e-01,\n",
      "           -1.0456e+00, -1.6301e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7466e-01,  3.0707e-02,  7.5225e-01,  ..., -1.9281e+00,\n",
      "            1.1489e+00, -2.4530e-01],\n",
      "          [-7.6226e-02,  8.5814e-01, -1.5467e+00,  ..., -1.3217e+00,\n",
      "           -1.3415e+00, -3.8328e-01]],\n",
      "\n",
      "         [[ 1.5170e-01,  5.1089e-01,  1.3993e-01,  ..., -2.4168e+00,\n",
      "            3.3385e-01, -6.2115e-02],\n",
      "          [-1.6390e+00, -1.6085e-01, -1.9118e+00,  ..., -1.6600e-01,\n",
      "           -6.5011e-01,  2.1798e-02]],\n",
      "\n",
      "         [[-2.4311e-01,  1.6726e+00,  1.6682e-01,  ..., -1.0481e+00,\n",
      "           -2.7634e+00,  2.2741e-01],\n",
      "          [-1.4603e+00,  3.1239e-02,  3.8892e-01,  ...,  1.3441e-03,\n",
      "           -1.6754e+00,  3.1771e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7844e-01, -3.1772e-01, -1.0617e+00,  ..., -7.8511e-01,\n",
      "           -3.2510e-01, -2.1300e-01],\n",
      "          [-3.9893e-01,  6.1469e-01, -3.9206e-01,  ...,  6.4928e-01,\n",
      "           -3.2944e-01, -2.4185e+00]],\n",
      "\n",
      "         [[ 5.9122e-01, -2.2648e-01,  1.7474e-01,  ..., -1.6488e+00,\n",
      "           -8.6854e-01, -8.0783e-01],\n",
      "          [-2.1516e+00, -2.4247e-01, -8.1713e-01,  ..., -1.8623e+00,\n",
      "           -1.1230e+00,  3.5013e-01]],\n",
      "\n",
      "         [[ 1.1642e-01,  1.2460e+00,  7.7942e-02,  ..., -1.3121e+00,\n",
      "           -6.7044e-01, -1.1324e+00],\n",
      "          [ 4.3930e-01,  3.4082e-01, -1.2243e+00,  ...,  6.4975e-01,\n",
      "           -7.3862e-01, -2.1510e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1291e+00, -1.3637e+00, -1.5779e+00,  ...,  8.5980e-01,\n",
      "            3.2796e-01, -1.9442e+00],\n",
      "          [-8.9502e-01,  9.7357e-01,  8.5447e-01,  ...,  1.7637e+00,\n",
      "            9.1331e-01, -1.7033e+00]],\n",
      "\n",
      "         [[ 1.5909e+00, -1.4922e+00,  1.0060e+00,  ..., -1.5965e+00,\n",
      "           -3.9380e-01, -4.3585e-01],\n",
      "          [-2.2103e+00,  4.4127e-01,  1.1554e+00,  ...,  9.8096e-01,\n",
      "            8.6736e-01, -2.3894e+00]],\n",
      "\n",
      "         [[ 1.8451e-01,  1.2740e+00,  4.2857e-01,  ..., -1.1366e+00,\n",
      "           -2.8409e+00,  4.6711e-01],\n",
      "          [-1.9576e+00,  2.0176e-01, -4.1035e-02,  ...,  6.2708e-01,\n",
      "           -1.3601e+00, -3.9984e-01]]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 3.7393e-01,  3.7078e-01, -7.5553e-01,  ..., -9.4280e-01,\n",
      "           -6.5765e-01, -1.4711e-01],\n",
      "          [ 3.9396e-01,  4.7658e-02, -1.0277e+00,  ..., -8.4926e-01,\n",
      "           -2.7815e-01, -2.6628e-01],\n",
      "          [ 9.9586e-01,  2.4414e-01, -8.3459e-01,  ..., -8.5795e-01,\n",
      "           -4.3860e-01, -2.1582e-01],\n",
      "          ...,\n",
      "          [ 1.4504e+00, -1.2077e-01, -4.8160e-01,  ..., -1.6701e+00,\n",
      "            6.3807e-01, -8.3788e-02],\n",
      "          [ 3.8821e-01,  1.4150e-01, -1.5051e-01,  ..., -1.5785e+00,\n",
      "            4.1589e-01, -1.8024e-01],\n",
      "          [ 1.0467e-01,  5.5196e-01,  1.0818e-01,  ..., -2.0373e+00,\n",
      "            8.4396e-02, -6.9033e-02]],\n",
      "\n",
      "         [[-7.7230e-01, -1.1852e-01, -8.0275e-02,  ..., -1.3792e-03,\n",
      "           -5.2249e-01, -4.2095e-01],\n",
      "          [-4.6209e-01, -1.2680e-01, -2.5711e-01,  ...,  1.3235e-01,\n",
      "           -4.1385e-01, -1.3744e+00],\n",
      "          [-1.7985e-01, -2.6037e-01,  3.5677e-01,  ...,  2.4736e-01,\n",
      "           -1.6626e-01, -7.0940e-01],\n",
      "          ...,\n",
      "          [-6.2069e-01,  2.5298e-01, -7.8416e-01,  ...,  8.6186e-02,\n",
      "           -6.9561e-01, -8.5675e-01],\n",
      "          [-9.1553e-01,  1.4648e-01, -5.5621e-02,  ...,  1.8643e-01,\n",
      "           -1.0965e+00, -4.8097e-01],\n",
      "          [-1.0678e+00,  9.0884e-02, -4.5400e-02,  ...,  1.5424e-01,\n",
      "           -1.1762e+00, -2.9385e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.9445e-01,  9.6669e-03, -5.7118e-01,  ..., -9.7640e-01,\n",
      "           -9.0948e-01, -1.3761e-01],\n",
      "          [ 4.7672e-01, -2.3672e-01, -8.9246e-01,  ..., -8.9927e-01,\n",
      "           -3.9088e-01, -3.1635e-01],\n",
      "          [ 4.5120e-01, -5.8034e-02, -6.8736e-01,  ..., -1.0352e+00,\n",
      "           -4.7996e-01, -4.5624e-01],\n",
      "          ...,\n",
      "          [ 3.3362e-01,  1.5919e-03, -1.0108e+00,  ..., -1.5704e+00,\n",
      "           -3.9079e-01, -2.1742e-01],\n",
      "          [ 1.0123e+00, -1.1147e+00, -1.3032e+00,  ...,  3.4082e-01,\n",
      "            1.3298e-01, -1.5481e+00],\n",
      "          [ 1.4273e+00, -1.2951e+00,  5.7719e-01,  ..., -1.2490e+00,\n",
      "           -4.1355e-01, -5.8558e-01]],\n",
      "\n",
      "         [[-1.0884e+00,  2.0838e-01, -5.0948e-01,  ...,  2.2845e-01,\n",
      "           -8.2075e-01, -1.1496e+00],\n",
      "          [-3.4296e-01,  5.0833e-01, -8.6644e-01,  ...,  3.3008e-01,\n",
      "           -5.5070e-01, -1.8440e+00],\n",
      "          [-3.2602e-01,  6.0671e-01, -7.5711e-01,  ...,  4.7493e-01,\n",
      "           -4.6589e-01, -2.0454e+00],\n",
      "          ...,\n",
      "          [-1.7399e+00,  4.6359e-01,  6.5213e-01,  ...,  9.0025e-01,\n",
      "            3.1642e-01, -2.0784e+00],\n",
      "          [-1.8375e+00,  2.6226e-01,  3.7727e-02,  ...,  6.6286e-01,\n",
      "           -1.0588e+00, -7.6781e-01],\n",
      "          [-1.6402e+00,  3.1250e-01, -6.6174e-04,  ...,  6.6493e-01,\n",
      "           -9.1110e-01, -1.0382e+00]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[ 3.7393e-01,  3.7078e-01, -7.5553e-01,  ..., -9.4280e-01,\n",
      "           -6.5765e-01, -1.4711e-01],\n",
      "          [-7.7230e-01, -1.1852e-01, -8.0275e-02,  ..., -1.3792e-03,\n",
      "           -5.2249e-01, -4.2095e-01]],\n",
      "\n",
      "         [[ 3.9396e-01,  4.7658e-02, -1.0277e+00,  ..., -8.4926e-01,\n",
      "           -2.7815e-01, -2.6628e-01],\n",
      "          [-4.6209e-01, -1.2680e-01, -2.5711e-01,  ...,  1.3235e-01,\n",
      "           -4.1385e-01, -1.3744e+00]],\n",
      "\n",
      "         [[ 9.9586e-01,  2.4414e-01, -8.3459e-01,  ..., -8.5795e-01,\n",
      "           -4.3860e-01, -2.1582e-01],\n",
      "          [-1.7985e-01, -2.6037e-01,  3.5677e-01,  ...,  2.4736e-01,\n",
      "           -1.6626e-01, -7.0940e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4504e+00, -1.2077e-01, -4.8160e-01,  ..., -1.6701e+00,\n",
      "            6.3807e-01, -8.3788e-02],\n",
      "          [-6.2069e-01,  2.5298e-01, -7.8416e-01,  ...,  8.6186e-02,\n",
      "           -6.9561e-01, -8.5675e-01]],\n",
      "\n",
      "         [[ 3.8821e-01,  1.4150e-01, -1.5051e-01,  ..., -1.5785e+00,\n",
      "            4.1589e-01, -1.8024e-01],\n",
      "          [-9.1553e-01,  1.4648e-01, -5.5621e-02,  ...,  1.8643e-01,\n",
      "           -1.0965e+00, -4.8097e-01]],\n",
      "\n",
      "         [[ 1.0467e-01,  5.5196e-01,  1.0818e-01,  ..., -2.0373e+00,\n",
      "            8.4396e-02, -6.9033e-02],\n",
      "          [-1.0678e+00,  9.0884e-02, -4.5400e-02,  ...,  1.5424e-01,\n",
      "           -1.1762e+00, -2.9385e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.9445e-01,  9.6669e-03, -5.7118e-01,  ..., -9.7640e-01,\n",
      "           -9.0948e-01, -1.3761e-01],\n",
      "          [-1.0884e+00,  2.0838e-01, -5.0948e-01,  ...,  2.2845e-01,\n",
      "           -8.2075e-01, -1.1496e+00]],\n",
      "\n",
      "         [[ 4.7672e-01, -2.3672e-01, -8.9246e-01,  ..., -8.9927e-01,\n",
      "           -3.9088e-01, -3.1635e-01],\n",
      "          [-3.4296e-01,  5.0833e-01, -8.6644e-01,  ...,  3.3008e-01,\n",
      "           -5.5070e-01, -1.8440e+00]],\n",
      "\n",
      "         [[ 4.5120e-01, -5.8034e-02, -6.8736e-01,  ..., -1.0352e+00,\n",
      "           -4.7996e-01, -4.5624e-01],\n",
      "          [-3.2602e-01,  6.0671e-01, -7.5711e-01,  ...,  4.7493e-01,\n",
      "           -4.6589e-01, -2.0454e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.3362e-01,  1.5919e-03, -1.0108e+00,  ..., -1.5704e+00,\n",
      "           -3.9079e-01, -2.1742e-01],\n",
      "          [-1.7399e+00,  4.6359e-01,  6.5213e-01,  ...,  9.0025e-01,\n",
      "            3.1642e-01, -2.0784e+00]],\n",
      "\n",
      "         [[ 1.0123e+00, -1.1147e+00, -1.3032e+00,  ...,  3.4082e-01,\n",
      "            1.3298e-01, -1.5481e+00],\n",
      "          [-1.8375e+00,  2.6226e-01,  3.7727e-02,  ...,  6.6286e-01,\n",
      "           -1.0588e+00, -7.6781e-01]],\n",
      "\n",
      "         [[ 1.4273e+00, -1.2951e+00,  5.7719e-01,  ..., -1.2490e+00,\n",
      "           -4.1355e-01, -5.8558e-01],\n",
      "          [-1.6402e+00,  3.1250e-01, -6.6174e-04,  ...,  6.6493e-01,\n",
      "           -9.1110e-01, -1.0382e+00]]]], grad_fn=<TransposeBackward0>)\n",
      "input_ids {}\n",
      "input_ids {'is_implicit': True}\n",
      "size {}\n",
      "size {'is_implicit': True}\n",
      "getitem {}\n",
      "getitem {}\n",
      "getitem_1 {}\n",
      "getitem_1 {}\n",
      "bert_embeddings_token_type_ids {}\n",
      "bert_embeddings_token_type_ids {}\n",
      "getitem_2 {}\n",
      "getitem_2 {}\n",
      "expand {}\n",
      "expand {}\n",
      "size_1 {}\n",
      "size_1 {'is_implicit': True}\n",
      "getitem_3 {}\n",
      "getitem_3 {}\n",
      "bert_embeddings_position_ids {}\n",
      "bert_embeddings_position_ids {}\n",
      "add {}\n",
      "add {}\n",
      "getitem_4 {}\n",
      "getitem_4 {}\n",
      "bert_embeddings_word_embeddings {}\n",
      "bert_embeddings_word_embeddings {'is_implicit': True}\n",
      "bert_embeddings_token_type_embeddings {}\n",
      "bert_embeddings_token_type_embeddings {}\n",
      "add_1 {}\n",
      "add_1 {}\n",
      "bert_embeddings_position_embeddings {}\n",
      "bert_embeddings_position_embeddings {}\n",
      "add_2 {}\n",
      "add_2 {}\n",
      "bert_embeddings_layer_norm {}\n",
      "bert_embeddings_layer_norm {}\n",
      "bert_embeddings_dropout {}\n",
      "bert_embeddings_dropout {}\n",
      "add_3 {}\n",
      "add_3 {}\n",
      "getattr_1 {}\n",
      "getattr_1 {'is_implicit': True}\n",
      "ones {}\n",
      "ones {'is_implicit': True}\n",
      "dim {}\n",
      "dim {'is_implicit': True}\n",
      "eq {}\n",
      "eq {'is_implicit': True}\n",
      "size_2 {}\n",
      "size_2 {'is_implicit': True}\n",
      "getitem_5 {}\n",
      "getitem_5 {}\n",
      "getitem_6 {}\n",
      "getitem_6 {}\n",
      "size_3 {}\n",
      "size_3 {'is_implicit': True}\n",
      "getitem_7 {}\n",
      "getitem_7 {}\n",
      "getitem_8 {}\n",
      "getitem_8 {}\n",
      "getitem_9 {}\n",
      "getitem_9 {}\n",
      "expand_1 {}\n",
      "expand_1 {}\n",
      "getattr_2 {}\n",
      "getattr_2 {}\n",
      "to {}\n",
      "to {}\n",
      "sub {}\n",
      "sub {}\n",
      "to_1 {}\n",
      "to_1 {}\n",
      "finfo {}\n",
      "finfo {}\n",
      "getattr_3 {}\n",
      "getattr_3 {}\n",
      "masked_fill {}\n",
      "masked_fill {}\n",
      "size_4 {}\n",
      "size_4 {}\n",
      "getitem_10 {}\n",
      "getitem_10 {}\n",
      "getitem_11 {}\n",
      "getitem_11 {}\n",
      "getitem_12 {}\n",
      "getitem_12 {}\n",
      "bert_encoder_layer_0_attention_self_query {}\n",
      "bert_encoder_layer_0_attention_self_query {}\n",
      "size_5 {}\n",
      "size_5 {}\n",
      "getitem_13 {}\n",
      "getitem_13 {}\n",
      "add_4 {}\n",
      "add_4 {}\n",
      "view {}\n",
      "view {}\n",
      "permute {}\n",
      "permute {}\n",
      "bert_encoder_layer_0_attention_self_key {}\n",
      "bert_encoder_layer_0_attention_self_key {}\n",
      "size_6 {}\n",
      "size_6 {}\n",
      "getitem_14 {}\n",
      "getitem_14 {}\n",
      "add_5 {}\n",
      "add_5 {}\n",
      "view_1 {}\n",
      "view_1 {}\n",
      "permute_1 {}\n",
      "permute_1 {}\n",
      "bert_encoder_layer_0_attention_self_value {}\n",
      "bert_encoder_layer_0_attention_self_value {}\n",
      "size_7 {}\n",
      "size_7 {}\n",
      "getitem_15 {}\n",
      "getitem_15 {}\n",
      "add_6 {}\n",
      "add_6 {}\n",
      "view_2 {}\n",
      "view_2 {}\n",
      "permute_2 {}\n",
      "permute_2 {}\n",
      "scaled_dot_product_attention {}\n",
      "scaled_dot_product_attention {}\n",
      "transpose {}\n",
      "transpose {}\n",
      "reshape {}\n",
      "reshape {}\n",
      "bert_encoder_layer_0_attention_output_dense {}\n",
      "bert_encoder_layer_0_attention_output_dense {}\n",
      "bert_encoder_layer_0_attention_output_dropout {}\n",
      "bert_encoder_layer_0_attention_output_dropout {}\n",
      "add_7 {}\n",
      "add_7 {}\n",
      "bert_encoder_layer_0_attention_output_layer_norm {}\n",
      "bert_encoder_layer_0_attention_output_layer_norm {}\n",
      "bert_encoder_layer_0_intermediate_dense {}\n",
      "bert_encoder_layer_0_intermediate_dense {}\n",
      "gelu {}\n",
      "gelu {}\n",
      "bert_encoder_layer_0_output_dense {}\n",
      "bert_encoder_layer_0_output_dense {}\n",
      "bert_encoder_layer_0_output_dropout {}\n",
      "bert_encoder_layer_0_output_dropout {}\n",
      "add_8 {}\n",
      "add_8 {}\n",
      "bert_encoder_layer_0_output_layer_norm {}\n",
      "bert_encoder_layer_0_output_layer_norm {}\n",
      "size_8 {}\n",
      "size_8 {}\n",
      "getitem_16 {}\n",
      "getitem_16 {}\n",
      "getitem_17 {}\n",
      "getitem_17 {}\n",
      "getitem_18 {}\n",
      "getitem_18 {}\n",
      "bert_encoder_layer_1_attention_self_query {}\n",
      "bert_encoder_layer_1_attention_self_query {}\n",
      "size_9 {}\n",
      "size_9 {}\n",
      "getitem_19 {}\n",
      "getitem_19 {}\n",
      "add_9 {}\n",
      "add_9 {}\n",
      "view_3 {}\n",
      "view_3 {}\n",
      "permute_3 {}\n",
      "permute_3 {}\n",
      "bert_encoder_layer_1_attention_self_key {}\n",
      "bert_encoder_layer_1_attention_self_key {}\n",
      "size_10 {}\n",
      "size_10 {}\n",
      "getitem_20 {}\n",
      "getitem_20 {}\n",
      "add_10 {}\n",
      "add_10 {}\n",
      "view_4 {}\n",
      "view_4 {}\n",
      "permute_4 {}\n",
      "permute_4 {}\n",
      "bert_encoder_layer_1_attention_self_value {}\n",
      "bert_encoder_layer_1_attention_self_value {}\n",
      "size_11 {}\n",
      "size_11 {}\n",
      "getitem_21 {}\n",
      "getitem_21 {}\n",
      "add_11 {}\n",
      "add_11 {}\n",
      "view_5 {}\n",
      "view_5 {}\n",
      "permute_5 {}\n",
      "permute_5 {}\n",
      "scaled_dot_product_attention_1 {}\n",
      "scaled_dot_product_attention_1 {}\n",
      "transpose_1 {}\n",
      "transpose_1 {}\n",
      "reshape_1 {}\n",
      "reshape_1 {}\n",
      "bert_encoder_layer_1_attention_output_dense {}\n",
      "bert_encoder_layer_1_attention_output_dense {}\n",
      "bert_encoder_layer_1_attention_output_dropout {}\n",
      "bert_encoder_layer_1_attention_output_dropout {}\n",
      "add_12 {}\n",
      "add_12 {}\n",
      "bert_encoder_layer_1_attention_output_layer_norm {}\n",
      "bert_encoder_layer_1_attention_output_layer_norm {}\n",
      "bert_encoder_layer_1_intermediate_dense {}\n",
      "bert_encoder_layer_1_intermediate_dense {}\n",
      "gelu_1 {}\n",
      "gelu_1 {}\n",
      "bert_encoder_layer_1_output_dense {}\n",
      "bert_encoder_layer_1_output_dense {}\n",
      "bert_encoder_layer_1_output_dropout {}\n",
      "bert_encoder_layer_1_output_dropout {}\n",
      "add_13 {}\n",
      "add_13 {}\n",
      "bert_encoder_layer_1_output_layer_norm {}\n",
      "bert_encoder_layer_1_output_layer_norm {}\n",
      "getitem_22 {}\n",
      "getitem_22 {}\n",
      "bert_pooler_dense {}\n",
      "bert_pooler_dense {}\n",
      "bert_pooler_activation {}\n",
      "bert_pooler_activation {}\n",
      "dropout {}\n",
      "dropout {}\n",
      "classifier {}\n",
      "classifier {}\n",
      "output {}\n",
      "output {'is_implicit': True}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import chop.passes as passes\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "dummy_input = tokenizer(\n",
    "    [\n",
    "        \"AI may take over the world one day\",\n",
    "        \"This is why you should learn ADLS\",\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "mg, _ = passes.init_metadata_analysis_pass(mg)\n",
    "mg, _ = passes.add_common_metadata_analysis_pass(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"dummy_in\": dummy_input,\n",
    "        \"add_value\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "for node in mg.fx_graph.nodes:\n",
    "    print(node.name,node.meta[\"mase\"][\"software\"])\n",
    "    print(node.name,node.meta[\"mase\"][\"hardware\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: writing an analysis pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing an analysis pass is often simple - in the following example, we implement a pass which counts the number of dropout layers in the graph. We also show how to use the `get_logger` API from `chop.tools` to provide information about the graph to the user at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound dropout module: bert.embeddings.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound dropout module: bert.encoder.layer.0.attention.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound dropout module: bert.encoder.layer.0.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound dropout module: bert.encoder.layer.1.attention.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound dropout module: bert.encoder.layer.1.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound dropout module: dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_token_type_ids of type get_attr\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_position_ids of type get_attr\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_word_embeddings of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_token_type_embeddings of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_position_embeddings of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_layer_norm of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_dropout of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDropout count is: 6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 bert_embeddings nodes\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_logger\n",
    "\n",
    "logger = get_logger(\"mase_logger\")\n",
    "logger.setLevel(\"INFO\")\n",
    "\n",
    "\n",
    "def count_dropout_analysis_pass(mg, pass_args={}):\n",
    "\n",
    "    dropout_modules = 0\n",
    "    dropout_functions = 0\n",
    "\n",
    "    for node in mg.fx_graph.nodes:\n",
    "        if node.op == \"call_module\" and \"dropout\" in node.target:\n",
    "            logger.info(f\"Found dropout module: {node.target}\")\n",
    "            dropout_modules += 1\n",
    "        else:\n",
    "            logger.debug(f\"Skipping node: {node.target}\")\n",
    "\n",
    "    return mg, {\"dropout_count\": dropout_modules + dropout_functions}\n",
    "\n",
    "\n",
    "mg, pass_out = count_dropout_analysis_pass(mg)\n",
    "count = 0\n",
    "for node in mg.fx_graph.nodes:\n",
    "    if \"bert_embeddings\" in node.name:\n",
    "        logger.info(f\"Found node: {node.name} of type {node.op}\")\n",
    "        count+=1\n",
    "print(f\"Found {count} bert_embeddings nodes\")\n",
    "\n",
    "logger.info(f\"Dropout count is: {pass_out['dropout_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: writing a transform pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we delete all dropout nodes from the graph. Dropout is a useful training technique, but it doesn't have any effect on the activations at inference time, hence these nodes can be removed to simplify the graph. Transform passes may involve deleting, inserting, or replacing nodes in the graph. When doing this, we must carefully handle the arguments to ensure the graph topology is valid after transformation. Before erasing the dropout nodes, we must first find all other nodes that take the output of the dropout node as arguments, by running `node.replace_all_uses_with`. Without doing this, there would still be nodes that require arguments that no longer exist. \n",
    "\n",
    "> **Task**: Delete the call to `replace_all_uses_with` to verify that FX will report a RuntimeError.\n",
    "\n",
    "Finally, we rerun the analysis pass previously implemented to recount the number of dropout modules, and verify this is now zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mRemoving dropout module: bert.embeddings.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRemoving dropout module: bert.encoder.layer.0.attention.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRemoving dropout module: bert.encoder.layer.0.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRemoving dropout module: bert.encoder.layer.1.attention.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRemoving dropout module: bert.encoder.layer.1.output.dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mRemoving dropout module: dropout\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_token_type_ids of type get_attr\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_position_ids of type get_attr\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_word_embeddings of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_token_type_embeddings of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_position_embeddings of type call_module\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFound node: bert_embeddings_layer_norm of type call_module\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 bert_embeddings nodes\n",
      "input_ids {}\n",
      "input_ids {'is_implicit': True}\n",
      "size {}\n",
      "size {'is_implicit': True}\n",
      "getitem {}\n",
      "getitem {}\n",
      "getitem_1 {}\n",
      "getitem_1 {}\n",
      "bert_embeddings_token_type_ids {}\n",
      "bert_embeddings_token_type_ids {}\n",
      "getitem_2 {}\n",
      "getitem_2 {}\n",
      "expand {}\n",
      "expand {}\n",
      "size_1 {}\n",
      "size_1 {'is_implicit': True}\n",
      "getitem_3 {}\n",
      "getitem_3 {}\n",
      "bert_embeddings_position_ids {}\n",
      "bert_embeddings_position_ids {}\n",
      "add {}\n",
      "add {}\n",
      "getitem_4 {}\n",
      "getitem_4 {}\n",
      "bert_embeddings_word_embeddings {}\n",
      "bert_embeddings_word_embeddings {'is_implicit': True}\n",
      "bert_embeddings_token_type_embeddings {}\n",
      "bert_embeddings_token_type_embeddings {}\n",
      "add_1 {}\n",
      "add_1 {}\n",
      "bert_embeddings_position_embeddings {}\n",
      "bert_embeddings_position_embeddings {}\n",
      "add_2 {}\n",
      "add_2 {}\n",
      "bert_embeddings_layer_norm {}\n",
      "bert_embeddings_layer_norm {}\n",
      "add_3 {}\n",
      "add_3 {}\n",
      "getattr_1 {}\n",
      "getattr_1 {'is_implicit': True}\n",
      "ones {}\n",
      "ones {'is_implicit': True}\n",
      "dim {}\n",
      "dim {'is_implicit': True}\n",
      "eq {}\n",
      "eq {'is_implicit': True}\n",
      "size_2 {}\n",
      "size_2 {'is_implicit': True}\n",
      "getitem_5 {}\n",
      "getitem_5 {}\n",
      "getitem_6 {}\n",
      "getitem_6 {}\n",
      "size_3 {}\n",
      "size_3 {'is_implicit': True}\n",
      "getitem_7 {}\n",
      "getitem_7 {}\n",
      "getitem_8 {}\n",
      "getitem_8 {}\n",
      "getitem_9 {}\n",
      "getitem_9 {}\n",
      "expand_1 {}\n",
      "expand_1 {}\n",
      "getattr_2 {}\n",
      "getattr_2 {}\n",
      "to {}\n",
      "to {}\n",
      "sub {}\n",
      "sub {}\n",
      "to_1 {}\n",
      "to_1 {}\n",
      "finfo {}\n",
      "finfo {}\n",
      "getattr_3 {}\n",
      "getattr_3 {}\n",
      "masked_fill {}\n",
      "masked_fill {}\n",
      "size_4 {}\n",
      "size_4 {}\n",
      "getitem_10 {}\n",
      "getitem_10 {}\n",
      "getitem_11 {}\n",
      "getitem_11 {}\n",
      "getitem_12 {}\n",
      "getitem_12 {}\n",
      "bert_encoder_layer_0_attention_self_query {}\n",
      "bert_encoder_layer_0_attention_self_query {}\n",
      "size_5 {}\n",
      "size_5 {}\n",
      "getitem_13 {}\n",
      "getitem_13 {}\n",
      "add_4 {}\n",
      "add_4 {}\n",
      "view {}\n",
      "view {}\n",
      "permute {}\n",
      "permute {}\n",
      "bert_encoder_layer_0_attention_self_key {}\n",
      "bert_encoder_layer_0_attention_self_key {}\n",
      "size_6 {}\n",
      "size_6 {}\n",
      "getitem_14 {}\n",
      "getitem_14 {}\n",
      "add_5 {}\n",
      "add_5 {}\n",
      "view_1 {}\n",
      "view_1 {}\n",
      "permute_1 {}\n",
      "permute_1 {}\n",
      "bert_encoder_layer_0_attention_self_value {}\n",
      "bert_encoder_layer_0_attention_self_value {}\n",
      "size_7 {}\n",
      "size_7 {}\n",
      "getitem_15 {}\n",
      "getitem_15 {}\n",
      "add_6 {}\n",
      "add_6 {}\n",
      "view_2 {}\n",
      "view_2 {}\n",
      "permute_2 {}\n",
      "permute_2 {}\n",
      "scaled_dot_product_attention {}\n",
      "scaled_dot_product_attention {}\n",
      "transpose {}\n",
      "transpose {}\n",
      "reshape {}\n",
      "reshape {}\n",
      "bert_encoder_layer_0_attention_output_dense {}\n",
      "bert_encoder_layer_0_attention_output_dense {}\n",
      "add_7 {}\n",
      "add_7 {}\n",
      "bert_encoder_layer_0_attention_output_layer_norm {}\n",
      "bert_encoder_layer_0_attention_output_layer_norm {}\n",
      "bert_encoder_layer_0_intermediate_dense {}\n",
      "bert_encoder_layer_0_intermediate_dense {}\n",
      "gelu {}\n",
      "gelu {}\n",
      "bert_encoder_layer_0_output_dense {}\n",
      "bert_encoder_layer_0_output_dense {}\n",
      "add_8 {}\n",
      "add_8 {}\n",
      "bert_encoder_layer_0_output_layer_norm {}\n",
      "bert_encoder_layer_0_output_layer_norm {}\n",
      "size_8 {}\n",
      "size_8 {}\n",
      "getitem_16 {}\n",
      "getitem_16 {}\n",
      "getitem_17 {}\n",
      "getitem_17 {}\n",
      "getitem_18 {}\n",
      "getitem_18 {}\n",
      "bert_encoder_layer_1_attention_self_query {}\n",
      "bert_encoder_layer_1_attention_self_query {}\n",
      "size_9 {}\n",
      "size_9 {}\n",
      "getitem_19 {}\n",
      "getitem_19 {}\n",
      "add_9 {}\n",
      "add_9 {}\n",
      "view_3 {}\n",
      "view_3 {}\n",
      "permute_3 {}\n",
      "permute_3 {}\n",
      "bert_encoder_layer_1_attention_self_key {}\n",
      "bert_encoder_layer_1_attention_self_key {}\n",
      "size_10 {}\n",
      "size_10 {}\n",
      "getitem_20 {}\n",
      "getitem_20 {}\n",
      "add_10 {}\n",
      "add_10 {}\n",
      "view_4 {}\n",
      "view_4 {}\n",
      "permute_4 {}\n",
      "permute_4 {}\n",
      "bert_encoder_layer_1_attention_self_value {}\n",
      "bert_encoder_layer_1_attention_self_value {}\n",
      "size_11 {}\n",
      "size_11 {}\n",
      "getitem_21 {}\n",
      "getitem_21 {}\n",
      "add_11 {}\n",
      "add_11 {}\n",
      "view_5 {}\n",
      "view_5 {}\n",
      "permute_5 {}\n",
      "permute_5 {}\n",
      "scaled_dot_product_attention_1 {}\n",
      "scaled_dot_product_attention_1 {}\n",
      "transpose_1 {}\n",
      "transpose_1 {}\n",
      "reshape_1 {}\n",
      "reshape_1 {}\n",
      "bert_encoder_layer_1_attention_output_dense {}\n",
      "bert_encoder_layer_1_attention_output_dense {}\n",
      "add_12 {}\n",
      "add_12 {}\n",
      "bert_encoder_layer_1_attention_output_layer_norm {}\n",
      "bert_encoder_layer_1_attention_output_layer_norm {}\n",
      "bert_encoder_layer_1_intermediate_dense {}\n",
      "bert_encoder_layer_1_intermediate_dense {}\n",
      "gelu_1 {}\n",
      "gelu_1 {}\n",
      "bert_encoder_layer_1_output_dense {}\n",
      "bert_encoder_layer_1_output_dense {}\n",
      "add_13 {}\n",
      "add_13 {}\n",
      "bert_encoder_layer_1_output_layer_norm {}\n",
      "bert_encoder_layer_1_output_layer_norm {}\n",
      "getitem_22 {}\n",
      "getitem_22 {}\n",
      "bert_pooler_dense {}\n",
      "bert_pooler_dense {}\n",
      "bert_pooler_activation {}\n",
      "bert_pooler_activation {}\n",
      "classifier {}\n",
      "classifier {}\n",
      "output {}\n",
      "output {'is_implicit': True}\n"
     ]
    }
   ],
   "source": [
    "import torch.fx as fx\n",
    "\n",
    "\n",
    "def remove_dropout_transform_pass(mg, pass_args={}):\n",
    "\n",
    "    for node in mg.fx_graph.nodes:\n",
    "        if node.op == \"call_module\" and \"dropout\" in node.target:\n",
    "            logger.info(f\"Removing dropout module: {node.target}\")\n",
    "\n",
    "            # Replace all users of the dropout node with its parent node\n",
    "            parent_node = node.args[0]\n",
    "            logger.debug(f\"This dropout module has parent node: {parent_node}\")\n",
    "            node.replace_all_uses_with(parent_node)\n",
    "\n",
    "            # Erase the dropout node\n",
    "            mg.fx_graph.erase_node(node)\n",
    "        else:\n",
    "            logger.debug(f\"Skipping node: {node.target}\")\n",
    "\n",
    "    return mg, {}\n",
    "\n",
    "\n",
    "mg, _ = remove_dropout_transform_pass(mg)\n",
    "mg, pass_out = count_dropout_analysis_pass(mg)\n",
    "count = 0\n",
    "for node in mg.fx_graph.nodes:\n",
    "    if \"bert_embeddings\" in node.name:\n",
    "        logger.info(f\"Found node: {node.name} of type {node.op}\")\n",
    "        count+=1\n",
    "print(f\"Found {count} bert_embeddings nodes\")\n",
    "assert pass_out[\"dropout_count\"] == 0\n",
    "for node in mg.fx_graph.nodes:\n",
    "    print(node.name,node.meta[\"mase\"][\"software\"])\n",
    "    print(node.name,node.meta[\"mase\"][\"hardware\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the MaseGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export the transformed MaseGraph to be shared and used in future tutorials, by running the `mg.export()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting MaseGraph to /workspace/labs/lab1/outputs/tutorial_1.pt, /workspace/labs/lab1/outputs/tutorial_1.mz\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting GraphModule to /workspace/labs/lab1/outputs/tutorial_1.pt\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSaving full model format\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mExporting MaseMetadata to /workspace/labs/lab1/outputs/tutorial_1.mz\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "mg.export(f\"{out_dir}/tutorial_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exporting, you can pick up where you left off by running the `MaseGraph.from_checkpoint` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mg = MaseGraph.from_checkpoint(f\"{out_dir}/tutorial_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (mase-dev)",
   "language": "python",
   "name": "mase-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
