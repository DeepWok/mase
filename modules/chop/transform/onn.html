
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>chop.passes.module.transforms.onn &#8212; MASE 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-data-viewer/jsonview.bundle.css?v=f6ef2277" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/libs/html/datatables.min.css?v=4b4fd840" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_links.css?v=2150a916" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_style.css?v=92936fa5" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/needstable.css?v=5e1b6797" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_toggle.css?v=5c6620df" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_core.css?v=f5b60a78" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/modern.css?v=803738c0" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../_static/documentation_options.js?v=e645c8fa"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/sphinx-data-viewer/jsonview.bundle.js?v=18cd53c5"></script>
    <script src="../../../_static/sphinx-data-viewer/jsonview_loader.js?v=f7ff7e7d"></script>
    <script src="../../../_static/sphinx-needs/libs/html/datatables.min.js?v=8a4aee21"></script>
    <script src="../../../_static/sphinx-needs/libs/html/datatables_loader.js?v=a2cae175"></script>
    <script src="../../../_static/sphinx-needs/libs/html/sphinx_needs_collapse.js?v=dca66431"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/chop/transform/onn';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="chop.passes.graph" href="../passes_graph.html" />
    <link rel="prev" title="chop.passes.module.transform.quantize" href="../module_transform/quantization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">MASE 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../documentation/installation.html">Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/getting_started/Get-started-using-uv.html">Getting Started using uv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/getting_started/Get-started-using-Docker.html">Getting Started using Docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/getting_started/Get-started-students.html">Additional Instructions for Imperial College Students</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../documentation/quickstart.html">Quickstart</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../documentation/tutorials.html">Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_1_introduction_to_mase.html">Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_2_lora_finetune.html">Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_3_qat.html">Tutorial 3: Running Quantization-Aware Training (QAT) on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_4_pruning.html">Tutorial 4: Unstructured Pruning on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_5_nas_optuna.html">Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_6_mixed_precision_search.html">Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_7_distributed_deployment.html">Tutorial 7: Deploying a Model for Inference on Distributed Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/tutorial_9_kernel_fusion.html">Tutorial 9: Running Kernel Fusion for Inference Acceleration on GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/advanced/tensorRT_quantization_tutorial.html">Advanced: TensorRT Quantization Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/advanced/onnxrt_quantization_tutorial.html">Advanced: ONNX Runtime Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/advanced/cli.html">Advanced: Using Mase CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/developer/Add-model-to-machop.html">Developer: Guide on how to add a new model into Machop</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/developer/doc_writing.html">Developer: How to write documentations in MASE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/tutorials/developer/how_to_extend_search.html">Developer: How to extend search</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../documentation/health.html">Repository Health</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../documentation/specifications.html">Coding Style Specifications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/specifications/C-coding-style-specifications.html">C/C++ Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/specifications/Python-coding-style-specifications.html">Python Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documentation/specifications/Verilog-coding-style-specifications.html">Verilog Coding Style Specifications</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machop API</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../machop.html">Machop Documentation</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../actions.html">chop.actions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../datasets.html">chop.datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../distributed.html">chop.distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ir.html">chop.ir</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html">chop.models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nn.html">chop.nn</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../nn_quantized.html">chop.nn.quantized</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../nn_quantized_functional.html">chop.nn.quantized.functional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nn_quantized_modules.html">chop.nn.quantized.modules</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../passes.html">chop.passes</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active has-children"><a class="reference internal" href="../passes_module.html">chop.passes.module</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../module_analysis/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../module_transform/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4 current active"><a class="current reference internal" href="#">chop.passes.module.transforms.onn</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../passes_graph.html">chop.passes.graph</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../analysis/add_metadata.html">chop.passes.graph.analysis.add_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/autosharding.html">chop.passes.graph.analysis.autosharding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/init_metadata.html">chop.passes.graph.analysis.init_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/report.html">chop.passes.graph.analysis.report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/statistical_profiler.html">chop.passes.graph.analysis.statistical_profiler.profile_statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/verify.html">chop.passes.graph.analysis.verify.verify</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/quantization.html">chop.passes.graph.calculate_avg_bits_mg_analysis_pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/pruning.html">chop.passes.graph.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../analysis/runtime.html">chop.passes.graph.analysis.runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="pruning.html">chop.passes.transform.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantize.html">chop.passes.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="utils.html">chop.passes.transform.utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="tensorrt.html">chop.passes.transform.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../interface/save_and_load.html">chop.passes.interface.save_and_load</a></li>
<li class="toctree-l4"><a class="reference internal" href="../interface/tensorrt.html">chop.passes.interface.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../interface/onnxrt.html">chop.passes.interface.onnxrt</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pipelines.html">chop.pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tools.html">chop.tools</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../adls_2024.html">Advanced Deep Learning Systems: 2024/2025</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_0_introduction.html">Lab 0: Introduction to Mase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_1_compression.html">Lab 1: Model Compression (Quantization and Pruning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_2_nas.html">Lab 2: Neural Architecture Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_3_mixed_precision_search.html">Lab 3: Mixed Precision Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab4-software.html">Lab 4 (Software Stream) Performance Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../adls_2023.html">Advanced Deep Learning Systems: 2023/2024</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab1.html">Lab 1 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab2.html">Lab 2 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab3.html">Lab 3 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab4-software.html">Lab 4 (Software Stream) for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/modules/chop/transform/onn.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>chop.passes.module.transforms.onn</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transform-pass">Transform Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optical-transformer-module-transform-pass">optical_transformer_module_transform_pass</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.optical_transformer_module_transform_pass"><code class="docutils literal notranslate"><span class="pre">optical_transformer_module_transform_pass()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration-parameters">Configuration Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers">Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#otlinear">OtLinear</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.linear.OtLinear"><code class="docutils literal notranslate"><span class="pre">chop.passes.module.transforms.onn.layers.linear.OtLinear</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.linear.from_linear"><code class="docutils literal notranslate"><span class="pre">chop.passes.module.transforms.onn.layers.linear.from_linear()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#otllamaattention">OtLlamaAttention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.query_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.query_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.key_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.key_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.value_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.value_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.qk_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.qk_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.attn_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.attn_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.av_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.av_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.seed"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.seed</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.__init__"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.__init__()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.forward"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.forward()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.from_pretrained"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.from_pretrained()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#functional-api">Functional API</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ot-eager-attention-forward">ot_eager_attention_forward</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.ot_eager_attention_forward"><code class="docutils literal notranslate"><span class="pre">ot_eager_attention_forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#usage-example">Usage Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selective-layer-transformation">Selective Layer Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bypass-mode-for-debugging">Bypass Mode for Debugging</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="chop-passes-module-transforms-onn">
<h1>chop.passes.module.transforms.onn<a class="headerlink" href="#chop-passes-module-transforms-onn" title="Link to this heading">#</a></h1>
<p>This module provides transformation passes for converting standard neural network
modules into Optical Neural Network (ONN) equivalents. The optical transformer
implementation is based on the <a class="reference external" href="https://arxiv.org/abs/2302.10360">Optical Transformers paper</a>.</p>
<p>Optical neural networks leverage photonic hardware to perform matrix multiplications
with reduced power consumption. This transform simulates the quantization effects
and constraints of optical compute hardware, enabling model development and evaluation
before deployment on physical optical accelerators.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This module requires the <code class="docutils literal notranslate"><span class="pre">mase-triton</span></code> package to be installed.
Install via: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">mase-triton</span></code></p>
</div>
<section id="transform-pass">
<h2>Transform Pass<a class="headerlink" href="#transform-pass" title="Link to this heading">#</a></h2>
<section id="optical-transformer-module-transform-pass">
<h3>optical_transformer_module_transform_pass<a class="headerlink" href="#optical-transformer-module-transform-pass" title="Link to this heading">#</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.optical_transformer_module_transform_pass">
<span class="sig-prename descclassname"><span class="pre">chop.passes.module.transforms.onn.</span></span><span class="sig-name descname"><span class="pre">optical_transformer_module_transform_pass</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pass_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span></span></span><a class="reference internal" href="../../../_modules/chop/passes/module/transforms/onn/transform.html#optical_transformer_module_transform_pass"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#chop.passes.module.transforms.onn.optical_transformer_module_transform_pass" title="Link to this definition">#</a></dt>
<dd><p>Transform a neural network by replacing supported modules with their optical transformer equivalents.</p>
<p>This pass simulates optical neural network (ONN) computation by replacing standard PyTorch
modules with quantized optical transformer layers. The optical transformer model is based on
the <a class="reference external" href="https://arxiv.org/abs/2302.10360">Optical Transformers paper</a>.</p>
<p>Supported module replacements:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> → <code class="docutils literal notranslate"><span class="pre">OtLinear</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transformers.models.llama.modeling_llama.LlamaAttention</span></code> → <code class="docutils literal notranslate"><span class="pre">OtLlamaAttention</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<em>torch.nn.Module</em>) – The input network to be transformed.</p></li>
<li><p><strong>pass_args</strong> (<em>dict</em>) – <p>A dictionary containing transformation configurations.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">by</span></code> (str): Layer matching strategy. Either <code class="docutils literal notranslate"><span class="pre">'name'</span></code> for exact name matching
or <code class="docutils literal notranslate"><span class="pre">'regex_name'</span></code> for regex-based pattern matching. Defaults to <code class="docutils literal notranslate"><span class="pre">'regex_name'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">default</span></code> (dict, optional): Default configuration applied to all matching layers.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;layer_name_or_pattern&gt;</span></code> (dict): Per-layer configuration. Each layer config
can contain the following keys:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">q_levels</span></code> (int): Number of quantization levels. Default: 256.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">q_lut_min</span></code> (float): Minimum value for lookup table. Default: 0.020040.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">q_smooth_factor</span></code> (float): Smoothing factor for running statistics. Default: 0.9.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">q_init_seed</span></code> (int): Random seed for quantization initialization. Default: 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">q_bypass</span></code> (bool): If True, bypass optical quantization. Default: False.</p></li>
</ul>
</li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The transformed network with optical transformer modules.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – If <code class="docutils literal notranslate"><span class="pre">mase-triton</span></code> is not installed.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.module.transforms.onn</span><span class="w"> </span><span class="kn">import</span> <span class="n">optical_transformer_module_transform_pass</span>

<span class="c1"># Transform all linear layers with default config</span>
<span class="n">pass_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;by&quot;</span><span class="p">:</span> <span class="s2">&quot;regex_name&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;q_levels&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;q_lut_min&quot;</span><span class="p">:</span> <span class="mf">0.020040</span><span class="p">,</span>
        <span class="s2">&quot;q_bypass&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">transformed_model</span> <span class="o">=</span> <span class="n">optical_transformer_module_transform_pass</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pass_args</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This pass requires the <code class="docutils literal notranslate"><span class="pre">mase-triton</span></code> package to be installed.
Install via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">mase-triton</span></code>.</p>
</div>
</dd></dl>

</section>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">#</a></h2>
<p>The transform pass accepts configuration through the <code class="docutils literal notranslate"><span class="pre">pass_args</span></code> dictionary.
Layer matching can be done by exact name or regex patterns.</p>
<p>Example configuration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pass_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;by&quot;</span><span class="p">:</span> <span class="s2">&quot;regex_name&quot;</span><span class="p">,</span>  <span class="c1"># or &quot;name&quot; for exact matching</span>
    <span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;q_levels&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;q_lut_min&quot;</span><span class="p">:</span> <span class="mf">0.020040</span><span class="p">,</span>
        <span class="s2">&quot;q_smooth_factor&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="s2">&quot;q_init_seed&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;q_bypass&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="c1"># Override for specific layers using regex</span>
    <span class="s2">&quot;.*mlp.*&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;q_levels&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;q_bypass&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
<section id="configuration-parameters">
<h3>Configuration Parameters<a class="headerlink" href="#configuration-parameters" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 15.0%" />
<col style="width: 15.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">q_levels</span></code></p></td>
<td><p>int</p></td>
<td><p>256</p></td>
<td><p>Number of quantization levels for optical simulation</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">q_lut_min</span></code></p></td>
<td><p>float</p></td>
<td><p>0.020040</p></td>
<td><p>Minimum value for the lookup table used in quantization</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">q_quantiles</span></code></p></td>
<td><p>tuple[float, float] or None</p></td>
<td><p>None</p></td>
<td><p>Quantile range for min/max statistics. If None, uses absolute min/max</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">q_smooth_factor</span></code></p></td>
<td><p>float</p></td>
<td><p>0.9</p></td>
<td><p>Exponential moving average factor for updating running statistics</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">q_init_seed</span></code></p></td>
<td><p>int</p></td>
<td><p>0</p></td>
<td><p>Random seed for quantization noise initialization</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">q_bypass</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>If True, bypass optical quantization (useful for debugging)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="layers">
<h2>Layers<a class="headerlink" href="#layers" title="Link to this heading">#</a></h2>
<section id="otlinear">
<h3>OtLinear<a class="headerlink" href="#otlinear" title="Link to this heading">#</a></h3>
<dl class="py data">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.linear.OtLinear">
<span class="sig-prename descclassname"><span class="pre">chop.passes.module.transforms.onn.layers.linear.</span></span><span class="sig-name descname"><span class="pre">OtLinear</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.linear.OtLinear" title="Link to this definition">#</a></dt>
<dd><p>Optical Transformer Linear layer.</p>
<p>This is an alias to <code class="docutils literal notranslate"><span class="pre">mase_triton.optical_compute.layers.OpticalTransformerLinear</span></code>.
It replaces standard <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> layers with quantized optical transformer
equivalents that simulate optical neural network hardware constraints.</p>
<p>The layer applies quantization to both the input activations and weights during
matrix multiplication, and tracks running min/max statistics for calibration.</p>
<p><strong>Class method:</strong></p>
<dl class="py method">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.linear.from_linear">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">linear</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.linear.from_linear" title="Link to this definition">#</a></dt>
<dd><p>Create an OtLinear from an existing <code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>linear</strong> (<em>torch.nn.Linear</em>) – Source linear layer</p></li>
<li><p><strong>kwargs</strong> – Quantization parameters (q_levels, q_lut_min, q_smooth_factor, q_init_seed, q_bypass, etc.)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Optical transformer linear layer with copied weights</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="otllamaattention">
<h3>OtLlamaAttention<a class="headerlink" href="#otllamaattention" title="Link to this heading">#</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">chop.passes.module.transforms.onn.layers.attn.</span></span><span class="sig-name descname"><span class="pre">OtLlamaAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LlamaConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_levels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_lut_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.02004</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_quantiles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_smooth_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_init_seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_bypass</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/chop/passes/module/transforms/onn/layers/attn.html#OtLlamaAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Optical Transformer attention module for LLaMA models.</p>
<p>This module replaces the standard HuggingFace LlamaAttention with an optical
transformer equivalent that simulates quantized matrix multiplications as would
occur in optical neural network hardware. The implementation is based on the
<a class="reference external" href="https://arxiv.org/abs/2302.10360">Optical Transformers paper</a>.</p>
<p>The attention computation uses optical transformer scaled dot-product attention
(SDPA) which applies quantization to the query-key and attention-value matrix
multiplications to simulate optical compute constraints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> – HuggingFace LLaMA configuration object.</p></li>
<li><p><strong>layer_idx</strong> (<em>int</em>) – Index of this attention layer in the model.</p></li>
<li><p><strong>q_levels</strong> (<em>int</em>) – Number of quantization levels for optical simulation. Default: 256.</p></li>
<li><p><strong>q_lut_min</strong> (<em>float</em>) – Minimum value for the lookup table used in quantization. Default: 0.020040.</p></li>
<li><p><strong>q_quantiles</strong> (<em>tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – Quantile range for min/max statistics.
If None, uses absolute min/max. Default: None.</p></li>
<li><p><strong>q_smooth_factor</strong> (<em>float</em>) – Exponential moving average factor for updating
running min/max statistics during training. Default: 0.9.</p></li>
<li><p><strong>q_init_seed</strong> (<em>int</em>) – Random seed for quantization noise initialization. Default: 0.</p></li>
<li><p><strong>q_bypass</strong> (<em>bool</em>) – If True, bypasses optical quantization and uses standard
PyTorch attention. Useful for debugging or comparison. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.query_min_max">
<span class="sig-name descname"><span class="pre">query_min_max</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.query_min_max" title="Link to this definition">#</a></dt>
<dd><p>Running min/max statistics for query tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.key_min_max">
<span class="sig-name descname"><span class="pre">key_min_max</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.key_min_max" title="Link to this definition">#</a></dt>
<dd><p>Running min/max statistics for key tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.value_min_max">
<span class="sig-name descname"><span class="pre">value_min_max</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.value_min_max" title="Link to this definition">#</a></dt>
<dd><p>Running min/max statistics for value tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.qk_min_max">
<span class="sig-name descname"><span class="pre">qk_min_max</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.qk_min_max" title="Link to this definition">#</a></dt>
<dd><p>Running min/max statistics for query-key products.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.attn_min_max">
<span class="sig-name descname"><span class="pre">attn_min_max</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.attn_min_max" title="Link to this definition">#</a></dt>
<dd><p>Min/max range for attention weights (fixed at [0, 1]).</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.av_min_max">
<span class="sig-name descname"><span class="pre">av_min_max</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.av_min_max" title="Link to this definition">#</a></dt>
<dd><p>Running min/max statistics for attention-value products.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.seed">
<span class="sig-name descname"><span class="pre">seed</span></span><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.seed" title="Link to this definition">#</a></dt>
<dd><p>Current random seed state for quantization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.module.transforms.onn.layers.attn</span><span class="w"> </span><span class="kn">import</span> <span class="n">OtLlamaAttention</span>

<span class="c1"># Create from existing HuggingFace attention layer</span>
<span class="n">ot_attn</span> <span class="o">=</span> <span class="n">OtLlamaAttention</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">hf_attention_layer</span><span class="p">,</span>
    <span class="n">layer_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">q_levels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">q_bypass</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LlamaConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_levels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_lut_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.02004</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_quantiles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_smooth_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_init_seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_bypass</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/chop/passes/module/transforms/onn/layers/attn.html#OtLlamaAttention.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_key_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_position</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../../_modules/chop/passes/module/transforms/onn/layers/attn.html#OtLlamaAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.from_pretrained">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LlamaAttention</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_levels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_lut_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.02004</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_quantiles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_smooth_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_init_seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_bypass</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention" title="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention"><span class="pre">OtLlamaAttention</span></a></span></span><a class="reference internal" href="../../../_modules/chop/passes/module/transforms/onn/layers/attn.html#OtLlamaAttention.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.from_pretrained" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
</section>
<section id="functional-api">
<h2>Functional API<a class="headerlink" href="#functional-api" title="Link to this heading">#</a></h2>
<section id="ot-eager-attention-forward">
<h3>ot_eager_attention_forward<a class="headerlink" href="#ot-eager-attention-forward" title="Link to this heading">#</a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="chop.passes.module.transforms.onn.layers.attn.ot_eager_attention_forward">
<span class="sig-prename descclassname"><span class="pre">chop.passes.module.transforms.onn.layers.attn.</span></span><span class="sig-name descname"><span class="pre">ot_eager_attention_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention" title="chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention"><span class="pre">OtLlamaAttention</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaling</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/chop/passes/module/transforms/onn/layers/attn.html#ot_eager_attention_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#chop.passes.module.transforms.onn.layers.attn.ot_eager_attention_forward" title="Link to this definition">#</a></dt>
<dd><p>Optical Transformer Scaled Dot-Product Attention.</p>
<p>Computes scaled dot-product attention with quantized matrix multiplications
to simulate optical neural network hardware constraints. This function applies
quantization to both the query-key and attention-value matrix products.</p>
<p>The quantization statistics (min/max values) are updated in-place during training
using an exponential moving average controlled by <code class="docutils literal notranslate"><span class="pre">q_smooth_factor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<em>Tensor</em>) – Query tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">heads,</span> <span class="pre">seq_len,</span> <span class="pre">head_dim)</span></code>.</p></li>
<li><p><strong>key</strong> (<em>Tensor</em>) – Key tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">kv_heads,</span> <span class="pre">seq_len,</span> <span class="pre">head_dim)</span></code>.</p></li>
<li><p><strong>value</strong> (<em>Tensor</em>) – Value tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">kv_heads,</span> <span class="pre">seq_len,</span> <span class="pre">head_dim)</span></code>.</p></li>
<li><p><strong>attention_mask</strong> (<em>Tensor</em><em>, </em><em>optional</em>) – Attention mask. Default: None.</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – Dropout probability. Default: 0.0.</p></li>
<li><p><strong>scaling</strong> (<em>float</em><em>, </em><em>optional</em>) – Scaling factor. If None, uses <code class="docutils literal notranslate"><span class="pre">1/sqrt(head_dim)</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Attention output of shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">heads,</span> <span class="pre">seq_len,</span> <span class="pre">head_dim)</span></code>.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="usage-example">
<h2>Usage Example<a class="headerlink" href="#usage-example" title="Link to this heading">#</a></h2>
<p>Basic usage with a LLaMA model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.module.transforms.onn</span><span class="w"> </span><span class="kn">import</span> <span class="n">optical_transformer_module_transform_pass</span>

<span class="c1"># Load a pretrained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">)</span>

<span class="c1"># Define transformation configuration</span>
<span class="n">pass_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;by&quot;</span><span class="p">:</span> <span class="s2">&quot;regex_name&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;q_levels&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;q_lut_min&quot;</span><span class="p">:</span> <span class="mf">0.020040</span><span class="p">,</span>
        <span class="s2">&quot;q_smooth_factor&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="s2">&quot;q_init_seed&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;q_bypass&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>

<span class="c1"># Apply the optical transformer transform</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">optical_transformer_module_transform_pass</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pass_args</span><span class="p">)</span>

<span class="c1"># The model now uses OtLinear and OtLlamaAttention layers</span>
<span class="c1"># Continue with training or inference as usual</span>
</pre></div>
</div>
<section id="selective-layer-transformation">
<h3>Selective Layer Transformation<a class="headerlink" href="#selective-layer-transformation" title="Link to this heading">#</a></h3>
<p>Transform only specific layers using regex patterns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pass_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;by&quot;</span><span class="p">:</span> <span class="s2">&quot;regex_name&quot;</span><span class="p">,</span>
    <span class="c1"># Only transform attention layers</span>
    <span class="s2">&quot;.*self_attn.*&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;q_levels&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;q_bypass&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="c1"># Transform MLP with different settings</span>
    <span class="s2">&quot;.*mlp.*&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;q_levels&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;q_bypass&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="bypass-mode-for-debugging">
<h3>Bypass Mode for Debugging<a class="headerlink" href="#bypass-mode-for-debugging" title="Link to this heading">#</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">q_bypass=True</span></code> to disable quantization while keeping the module structure:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pass_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;by&quot;</span><span class="p">:</span> <span class="s2">&quot;regex_name&quot;</span><span class="p">,</span>
    <span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;q_levels&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
        <span class="s2">&quot;q_bypass&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># Disable quantization</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../module_transform/quantization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">chop.passes.module.transform.quantize</p>
      </div>
    </a>
    <a class="right-next"
       href="../passes_graph.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">chop.passes.graph</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transform-pass">Transform Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optical-transformer-module-transform-pass">optical_transformer_module_transform_pass</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.optical_transformer_module_transform_pass"><code class="docutils literal notranslate"><span class="pre">optical_transformer_module_transform_pass()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration">Configuration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration-parameters">Configuration Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layers">Layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#otlinear">OtLinear</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.linear.OtLinear"><code class="docutils literal notranslate"><span class="pre">chop.passes.module.transforms.onn.layers.linear.OtLinear</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.linear.from_linear"><code class="docutils literal notranslate"><span class="pre">chop.passes.module.transforms.onn.layers.linear.from_linear()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#otllamaattention">OtLlamaAttention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.query_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.query_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.key_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.key_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.value_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.value_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.qk_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.qk_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.attn_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.attn_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.av_min_max"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.av_min_max</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.seed"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.seed</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.__init__"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.__init__()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.forward"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.forward()</span></code></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.OtLlamaAttention.from_pretrained"><code class="docutils literal notranslate"><span class="pre">OtLlamaAttention.from_pretrained()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#functional-api">Functional API</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ot-eager-attention-forward">ot_eager_attention_forward</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chop.passes.module.transforms.onn.layers.attn.ot_eager_attention_forward"><code class="docutils literal notranslate"><span class="pre">ot_eager_attention_forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#usage-example">Usage Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selective-layer-transformation">Selective Layer Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bypass-mode-for-debugging">Bypass Mode for Debugging</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DeepWok
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, DeepWok.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>