
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Advanced: ONNX Runtime Tutorial &#8212; MASE 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-data-viewer/jsonview.bundle.css?v=f6ef2277" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-needs/libs/html/datatables.min.css?v=4b4fd840" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-needs/common_css/need_style.css?v=92936fa5" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-needs/common_css/need_core.css?v=f5b60a78" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-needs/common_css/needstable.css?v=5e1b6797" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-needs/common_css/need_links.css?v=2150a916" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-needs/common_css/need_toggle.css?v=5c6620df" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-needs/modern.css?v=803738c0" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../../_static/documentation_options.js?v=e645c8fa"></script>
    <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../../_static/sphinx-data-viewer/jsonview.bundle.js?v=18cd53c5"></script>
    <script src="../../../../_static/sphinx-data-viewer/jsonview_loader.js?v=f7ff7e7d"></script>
    <script src="../../../../_static/sphinx-needs/libs/html/datatables.min.js?v=8a4aee21"></script>
    <script src="../../../../_static/sphinx-needs/libs/html/datatables_loader.js?v=a2cae175"></script>
    <script src="../../../../_static/sphinx-needs/libs/html/sphinx_needs_collapse.js?v=dca66431"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/documentation/tutorials/advanced/onnxrt_quantization_tutorial';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Advanced: Using Mase CLI" href="cli.html" />
    <link rel="prev" title="Advanced: TensorRT Quantization Tutorial" href="tensorRT_quantization_tutorial.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">MASE 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../installation.html">Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/Get-started-using-Anaconda.html">Getting Started using Conda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/Get-started-using-Docker.html">Getting Started using Docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/Get-started-using-Nix.html">Getting Started using Nix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../getting_started/Get-started-students.html">Additional Instructions for Imperial College Students</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../tutorials.html">Tutorials</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../tutorial_1_introduction_to_mase.html">Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_2_lora_finetune.html">Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_3_qat.html">Tutorial 3: Running Quantization-Aware Training (QAT) on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_4_pruning.html">Tutorial 4: Unstructured Pruning on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_5_nas_optuna.html">Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_6_mixed_precision_search.html">Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_7_distributed_deployment.html">Tutorial 7: Deploying a Model for Inference on Distributed Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_8_emit_verilog.html">Tutorial 8: Autogenerating an FPGA accelerator for a Transformer Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_9_kernel_fusion.html">Tutorial 9: Running Kernel Fusion for Inference Acceleration on GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorRT_quantization_tutorial.html">Advanced: TensorRT Quantization Tutorial</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Advanced: ONNX Runtime Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="cli.html">Advanced: Using Mase CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../developer/Add-model-to-machop.html">Developer: Guide on how to add a new model into Machop</a></li>
<li class="toctree-l2"><a class="reference internal" href="../developer/doc_writing.html">Developer: How to write documentations in MASE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../developer/how_to_extend_search.html">Developer: How to extend search</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../health.html">Repository Health</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../specifications.html">Coding Style Specifications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../specifications/C-coding-style-specifications.html">C/C++ Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../specifications/Python-coding-style-specifications.html">Python Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../specifications/Verilog-coding-style-specifications.html">Verilog Coding Style Specifications</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machop API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../machop.html">Machop Documentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/actions.html">chop.actions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/datasets.html">chop.datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/distributed.html">chop.distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/ir.html">chop.ir</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/models.html">chop.models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/nn.html">chop.nn</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../chop/nn_quantized.html">chop.nn.quantized</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../chop/nn_quantized_functional.html">chop.nn.quantized.functional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../chop/nn_quantized_modules.html">chop.nn.quantized.modules</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../chop/passes.html">chop.passes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../chop/passes_module.html">chop.passes.module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/module_analysis/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/module_transform/quantization.html">chop.passes.module.transform.quantize</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../chop/passes_graph.html">chop.passes.graph</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/add_metadata.html">chop.passes.graph.analysis.add_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/autosharding.html">chop.passes.graph.analysis.autosharding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/init_metadata.html">chop.passes.graph.analysis.init_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/report.html">chop.passes.graph.analysis.report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/statistical_profiler.html">chop.passes.graph.analysis.statistical_profiler.profile_statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/verify.html">chop.passes.graph.analysis.verify.verify</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/quantization.html">chop.passes.graph.calculate_avg_bits_mg_analysis_pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/pruning.html">chop.passes.graph.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/analysis/runtime.html">chop.passes.graph.analysis.runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/transform/pruning.html">chop.passes.transform.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/transform/quantize.html">chop.passes.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/transform/verilog.html">chop.passes.transform.verilog</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/transform/utils.html">chop.passes.transform.utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/transform/tensorrt.html">chop.passes.transform.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/interface/save_and_load.html">chop.passes.interface.save_and_load</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/interface/tensorrt.html">chop.passes.interface.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../chop/interface/onnxrt.html">chop.passes.interface.onnxrt</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/pipelines.html">chop.pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../chop/tools.html">chop.tools</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mase Components</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../hardware/hardware_documentation.html">Hardware Documentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/activations.html">Activations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/activations/gelu.html">GELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/activations/selu.html">SELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/activations/softplus.html">SoftPlus</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/activations/softsign.html">SoftSign</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/activations/tanh.html">Tanh</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/arithmetic.html">Arithmetic Units</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/arithmetic/mac.html">Multiply-Accumulate (MAC) Unit</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/axi.html">AXI Components</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/axi/read_master.html">AXI Read Master</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/buffers.html">Buffers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/buffers/hybrid_buffer.html">Hybrid Buffer</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/linear.html">Linear Layer</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/linear/fixed_linear.html">Fixed-Point Linear Layer</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/memory.html">Memory Components</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/memory/matrix_bank.html">Matrix Bank</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/norm.html">Normalization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/norm/batch_norm_2d.html">Batch Norm 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/norm/group_norm_2d.html">Group Norm 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/norm/norm.html">Normalization Module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/norm/rms_norm_2d.html">RMS Norm 2D</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../hardware/systolic_modules.html">Systolic Modules</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../hardware/systolic_modules/output_stationary.html">Output Stationary Systolic Module</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../adls_2024.html">Advanced Deep Learning Systems: 2024/2025</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2024/lab_0_introduction.html">Lab 0: Introduction to Mase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2024/lab_1_compression.html">Lab 1: Model Compression (Quantization and Pruning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2024/lab_2_nas.html">Lab 2: Neural Architecture Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2024/lab_3_mixed_precision_search.html">Lab 3: Mixed Precision Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2024/lab4-hardware.html">Lab 4 (Hardware Stream) Emitting Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2024/lab4-software.html">Lab 4 (Software Stream) Performance Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2024/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../adls_2023.html">Advanced Deep Learning Systems: 2023/2024</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2023/lab1.html">Lab 1 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2023/lab2.html">Lab 2 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2023/lab3.html">Lab 3 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2023/lab4-hardware.html">Lab 4 (Hardware Stream) for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2023/lab4-software.html">Lab 4 (Software Stream) for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../labs_2023/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/modules/documentation/tutorials/advanced/onnxrt_quantization_tutorial.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Advanced: ONNX Runtime Tutorial</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-onnx-runtime-optimizations">Section 1. ONNX Runtime Optimizations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-quantization">Section 2. Quantization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="advanced-onnx-runtime-tutorial">
<h1>Advanced: ONNX Runtime Tutorial<a class="headerlink" href="#advanced-onnx-runtime-tutorial" title="Link to this heading">#</a></h1>
<p>This notebook is designed to demonstrate the features of the ONNXRT passes integrated into MASE as part of the MASERT framework. The following demonstrations were run on a NVIDIA RTX A2000 GPU with a Intel® Xeon® CPU E5-2690 v4 &#64; 2.60GHz CPU.</p>
<section id="section-1-onnx-runtime-optimizations">
<h2>Section 1. ONNX Runtime Optimizations<a class="headerlink" href="#section-1-onnx-runtime-optimizations" title="Link to this heading">#</a></h2>
<p>Firstly, we will show you how we can utilise the ONNX RT optimizations. We expect to see a speed up without a loss in model accuracy. We will use a simple model, <code class="docutils literal notranslate"><span class="pre">jsc-toy</span></code>, and compare the optimized model to the original model using the <code class="docutils literal notranslate"><span class="pre">Machop</span> <span class="pre">API</span></code>.</p>
<p>First, we load the machop requirements by running the cell below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">toml</span>

<span class="c1"># Figure out the correct path</span>
<span class="n">machop_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">parent</span> <span class="o">/</span><span class="s2">&quot;machop&quot;</span>
<span class="k">assert</span> <span class="n">machop_path</span><span class="o">.</span><span class="n">exists</span><span class="p">(),</span> <span class="s2">&quot;Failed to find machop at: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">machop_path</span><span class="p">)</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">machop_path</span><span class="p">))</span>

<span class="c1"># Add directory to the PATH so that chop can be called</span>
<span class="n">new_path</span> <span class="o">=</span> <span class="s2">&quot;../../../machop&quot;</span>
<span class="n">full_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">new_path</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">os</span><span class="o">.</span><span class="n">pathsep</span> <span class="o">+</span> <span class="n">full_path</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_numpy_if_tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools.logger</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_logging_verbosity</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_cf_args</span><span class="p">,</span> <span class="n">get_dummy_input</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.graph.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy_mase_graph</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools.get_input</span><span class="w"> </span><span class="kn">import</span> <span class="n">InputGenerator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools.checkpoint_load</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.ir</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaseGraph</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_model_info</span><span class="p">,</span> <span class="n">get_model</span><span class="p">,</span> <span class="n">get_tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.dataset</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaseDataModule</span><span class="p">,</span> <span class="n">get_dataset_info</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.graph.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">metadata_value_type_cast_transform_pass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.graph</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">summarize_quantization_analysis_pass</span><span class="p">,</span>
    <span class="n">add_common_metadata_analysis_pass</span><span class="p">,</span>
    <span class="n">init_metadata_analysis_pass</span><span class="p">,</span>
    <span class="n">add_software_metadata_analysis_pass</span><span class="p">,</span>
    <span class="n">runtime_analysis_pass</span><span class="p">,</span>
    <span class="n">onnx_runtime_interface_pass</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">set_logging_verbosity</span><span class="p">(</span><span class="s2">&quot;info&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[2024-03-29 13:46:19,035] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)


[32mINFO    [0m [34mSet logging level to info[0m
WARNING: Logging before flag parsing goes to stderr.
I0329 13:46:21.531338 140128553666368 logger.py:44] Set logging level to info
</pre></div>
</div>
<p>We then load in a demonstration toml file and set the relevant pass arguments (this is all done automatically if we were to use the command line, see Section 2).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">JSC_TOML_PATH</span> <span class="o">=</span> <span class="s2">&quot;../../../machop/configs/onnx/jsc_gpu_ort.toml&quot;</span>

<span class="c1"># Reading TOML file and converting it into a Python dictionary</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">JSC_TOML_PATH</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">toml_file</span><span class="p">:</span>
    <span class="n">pass_args</span> <span class="o">=</span> <span class="n">toml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">toml_file</span><span class="p">)</span>

<span class="c1"># Extract the &#39;passes.tensorrt&#39; section and its children</span>
<span class="n">onnx_config</span> <span class="o">=</span> <span class="n">pass_args</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;passes&#39;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;onnxruntime&#39;</span><span class="p">,</span> <span class="p">{})</span>
<span class="c1"># Extract the &#39;passes.runtime_analysis&#39; section and its children</span>
<span class="n">runtime_analysis_config</span> <span class="o">=</span> <span class="n">pass_args</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;passes&#39;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;runtime_analysis&#39;</span><span class="p">,</span> <span class="p">{})</span>

<span class="c1"># Load the basics in</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span>
<span class="n">dataset_name</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;dataset&#39;</span><span class="p">]</span>
<span class="n">max_epochs</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span>
<span class="n">accelerator</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;accelerator&#39;</span><span class="p">]</span>

<span class="n">data_module</span> <span class="o">=</span> <span class="n">MaseDataModule</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="n">dataset_name</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data_module</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">data_module</span><span class="o">.</span><span class="n">setup</span><span class="p">()</span>

<span class="c1"># Add the data_module and other necessary information to the configs</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">[</span><span class="n">onnx_config</span><span class="p">,</span> <span class="n">runtime_analysis_config</span><span class="p">]</span>
<span class="k">for</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">configs</span><span class="p">:</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;task&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;task&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;data_module&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_module</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;dataset&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;dataset&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;accelerator&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;accelerator&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;gpu&#39;</span> <span class="k">else</span> <span class="n">pass_args</span><span class="p">[</span><span class="s1">&#39;accelerator&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;accelerator&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;gpu&#39;</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_MODULE_LOADING&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;LAZY&#39;</span>

<span class="n">model_info</span> <span class="o">=</span> <span class="n">get_model_info</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span>
    <span class="n">dataset_info</span><span class="o">=</span><span class="n">data_module</span><span class="o">.</span><span class="n">dataset_info</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">input_generator</span> <span class="o">=</span> <span class="n">InputGenerator</span><span class="p">(</span>
    <span class="n">data_module</span><span class="o">=</span><span class="n">data_module</span><span class="p">,</span>
    <span class="n">model_info</span><span class="o">=</span><span class="n">model_info</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span>
    <span class="n">which_dataloader</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># generate the mase graph and initialize node metadata</span>
<span class="n">mg</span> <span class="o">=</span> <span class="n">MaseGraph</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, we train the <code class="docutils literal notranslate"><span class="pre">jsc-toy</span></code> model using the machop <code class="docutils literal notranslate"><span class="pre">train</span></code> action with the config from the toml file. You may want to switch to GPU for this task - it will not affect the cpu optimizations later on.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ch<span class="w"> </span>train<span class="w"> </span>--config<span class="w"> </span><span class="o">{</span>JSC_TOML_PATH<span class="o">}</span><span class="w"> </span>--accelerator<span class="w"> </span>gpu
</pre></div>
</div>
<p>Then we load in the checkpoint. You will have to adjust this according to where it has been stored in the mase_output directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load in the trained checkpoint - change this accordingly</span>
<span class="n">JSC_CHECKPOINT_PATH</span> <span class="o">=</span> <span class="s2">&quot;../../../mase_output/jsc-toy_cls_jsc/software/training_ckpts/best.ckpt&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">load_name</span><span class="o">=</span><span class="n">JSC_CHECKPOINT_PATH</span><span class="p">,</span> <span class="n">load_type</span><span class="o">=</span><span class="s2">&quot;pl&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Initiate metadata</span>
<span class="n">dummy_in</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">input_generator</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">dummy_in</span><span class="p">)</span>
<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">init_metadata_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="c1"># Copy original graph for analysis later</span>
<span class="n">mg_original</span> <span class="o">=</span> <span class="n">deepcopy_mase_graph</span><span class="p">(</span><span class="n">mg</span><span class="p">)</span>

<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">add_common_metadata_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;dummy_in&quot;</span><span class="p">:</span> <span class="n">dummy_in</span><span class="p">})</span>
<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">add_software_metadata_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">metadata_value_type_cast_transform_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">,</span> <span class="n">pass_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fn&quot;</span><span class="p">:</span> <span class="n">to_numpy_if_tensor</span><span class="p">})</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[32mINFO    [0m [34mLoaded pytorch lightning checkpoint from ../../../mase_output/jsc-toy_cls_jsc/software/training_ckpts/best.ckpt[0m
I0327 14:20:09.068645 140012160939840 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from ../../../mase_output/jsc-toy_cls_jsc/software/training_ckpts/best.ckpt
</pre></div>
</div>
<p>We then run the <code class="docutils literal notranslate"><span class="pre">onnx_runtime_interface_pass</span></code> which completes the optimizations using the dataloader and <code class="docutils literal notranslate"><span class="pre">jsc-toy</span></code> model. This returns metadata containing the paths to the models:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">onnx_path</span></code> (the optimized model)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">onnx_dynamic_quantized_path</span></code> (the dynamically )</p></li>
</ul>
<p>In this case, since we are not quantizing the model, only the <code class="docutils literal notranslate"><span class="pre">onnx_path</span></code> is available.</p>
<p>The models are also stored in the directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>mase_output
└── onnxrt
    └── model_task_dataset_date
        ├── optimized
        ├── pre_processed
        ├── static_quantized
        └── dynamic_quantized
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mg</span><span class="p">,</span> <span class="n">onnx_meta</span> <span class="o">=</span> <span class="n">onnx_runtime_interface_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">,</span> <span class="n">pass_args</span><span class="o">=</span><span class="n">onnx_config</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[32mINFO    [0m [34mConverting PyTorch model to ONNX...[0m
I0327 14:20:12.535338 140012160939840 onnx_runtime.py:48] Converting PyTorch model to ONNX...
[32mINFO    [0m [34mProject will be created at /root/mase/mase_output/onnxrt/jsc-toy_cls_jsc_2024-03-27[0m
I0327 14:20:12.539771 140012160939840 onnx_runtime.py:50] Project will be created at /root/mase/mase_output/onnxrt/jsc-toy_cls_jsc_2024-03-27
[32mINFO    [0m [34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/onnxrt/jsc-toy_cls_jsc_2024-03-27/optimized/version_1/model.onnx[0m
I0327 14:20:12.751212 140012160939840 onnx_runtime.py:68] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/onnxrt/jsc-toy_cls_jsc_2024-03-27/optimized/version_1/model.onnx
[32mINFO    [0m [34mONNX Model Summary:
+-------+----------------------------------+--------------------+--------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+---------------------+
| Index |               Name               |        Type        |                                                          Inputs                                                          |                  Outputs                  |      Attributes     |
+-------+----------------------------------+--------------------+--------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+---------------------+
|   0   | /seq_blocks.0/BatchNormalization | BatchNormalization |            input, seq_blocks.0.weight, seq_blocks.0.bias, seq_blocks.0.running_mean, seq_blocks.0.running_var            | /seq_blocks.0/BatchNormalization_output_0 |  epsilon, momentum  |
|   1   |        /seq_blocks.1/Relu        |        Relu        |                                        /seq_blocks.0/BatchNormalization_output_0                                         |        /seq_blocks.1/Relu_output_0        |                     |
|   2   |        /seq_blocks.2/Gemm        |        Gemm        |                           /seq_blocks.1/Relu_output_0, seq_blocks.2.weight, seq_blocks.2.bias                            |        /seq_blocks.2/Gemm_output_0        | alpha, beta, transB |
|   3   | /seq_blocks.3/BatchNormalization | BatchNormalization | /seq_blocks.2/Gemm_output_0, seq_blocks.3.weight, seq_blocks.3.bias, seq_blocks.3.running_mean, seq_blocks.3.running_var | /seq_blocks.3/BatchNormalization_output_0 |  epsilon, momentum  |
|   4   |        /seq_blocks.4/Relu        |        Relu        |                                        /seq_blocks.3/BatchNormalization_output_0                                         |        /seq_blocks.4/Relu_output_0        |                     |
|   5   |        /seq_blocks.5/Gemm        |        Gemm        |                           /seq_blocks.4/Relu_output_0, seq_blocks.5.weight, seq_blocks.5.bias                            |        /seq_blocks.5/Gemm_output_0        | alpha, beta, transB |
|   6   | /seq_blocks.6/BatchNormalization | BatchNormalization | /seq_blocks.5/Gemm_output_0, seq_blocks.6.weight, seq_blocks.6.bias, seq_blocks.6.running_mean, seq_blocks.6.running_var | /seq_blocks.6/BatchNormalization_output_0 |  epsilon, momentum  |
|   7   |        /seq_blocks.7/Relu        |        Relu        |                                        /seq_blocks.6/BatchNormalization_output_0                                         |        /seq_blocks.7/Relu_output_0        |                     |
|   8   |        /seq_blocks.8/Gemm        |        Gemm        |                           /seq_blocks.7/Relu_output_0, seq_blocks.8.weight, seq_blocks.8.bias                            |        /seq_blocks.8/Gemm_output_0        | alpha, beta, transB |
|   9   | /seq_blocks.9/BatchNormalization | BatchNormalization | /seq_blocks.8/Gemm_output_0, seq_blocks.9.weight, seq_blocks.9.bias, seq_blocks.9.running_mean, seq_blocks.9.running_var | /seq_blocks.9/BatchNormalization_output_0 |  epsilon, momentum  |
|   10  |       /seq_blocks.10/Relu        |        Relu        |                                        /seq_blocks.9/BatchNormalization_output_0                                         |                     37                    |                     |
+-------+----------------------------------+--------------------+--------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+---------------------+[0m
I0327 14:20:12.757548 140012160939840 onnx_runtime.py:90] ONNX Model Summary:
+-------+----------------------------------+--------------------+--------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+---------------------+
| Index |               Name               |        Type        |                                                          Inputs                                                          |                  Outputs                  |      Attributes     |
+-------+----------------------------------+--------------------+--------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+---------------------+
|   0   | /seq_blocks.0/BatchNormalization | BatchNormalization |            input, seq_blocks.0.weight, seq_blocks.0.bias, seq_blocks.0.running_mean, seq_blocks.0.running_var            | /seq_blocks.0/BatchNormalization_output_0 |  epsilon, momentum  |
|   1   |        /seq_blocks.1/Relu        |        Relu        |                                        /seq_blocks.0/BatchNormalization_output_0                                         |        /seq_blocks.1/Relu_output_0        |                     |
|   2   |        /seq_blocks.2/Gemm        |        Gemm        |                           /seq_blocks.1/Relu_output_0, seq_blocks.2.weight, seq_blocks.2.bias                            |        /seq_blocks.2/Gemm_output_0        | alpha, beta, transB |
|   3   | /seq_blocks.3/BatchNormalization | BatchNormalization | /seq_blocks.2/Gemm_output_0, seq_blocks.3.weight, seq_blocks.3.bias, seq_blocks.3.running_mean, seq_blocks.3.running_var | /seq_blocks.3/BatchNormalization_output_0 |  epsilon, momentum  |
|   4   |        /seq_blocks.4/Relu        |        Relu        |                                        /seq_blocks.3/BatchNormalization_output_0                                         |        /seq_blocks.4/Relu_output_0        |                     |
|   5   |        /seq_blocks.5/Gemm        |        Gemm        |                           /seq_blocks.4/Relu_output_0, seq_blocks.5.weight, seq_blocks.5.bias                            |        /seq_blocks.5/Gemm_output_0        | alpha, beta, transB |
|   6   | /seq_blocks.6/BatchNormalization | BatchNormalization | /seq_blocks.5/Gemm_output_0, seq_blocks.6.weight, seq_blocks.6.bias, seq_blocks.6.running_mean, seq_blocks.6.running_var | /seq_blocks.6/BatchNormalization_output_0 |  epsilon, momentum  |
|   7   |        /seq_blocks.7/Relu        |        Relu        |                                        /seq_blocks.6/BatchNormalization_output_0                                         |        /seq_blocks.7/Relu_output_0        |                     |
|   8   |        /seq_blocks.8/Gemm        |        Gemm        |                           /seq_blocks.7/Relu_output_0, seq_blocks.8.weight, seq_blocks.8.bias                            |        /seq_blocks.8/Gemm_output_0        | alpha, beta, transB |
|   9   | /seq_blocks.9/BatchNormalization | BatchNormalization | /seq_blocks.8/Gemm_output_0, seq_blocks.9.weight, seq_blocks.9.bias, seq_blocks.9.running_mean, seq_blocks.9.running_var | /seq_blocks.9/BatchNormalization_output_0 |  epsilon, momentum  |
|   10  |       /seq_blocks.10/Relu        |        Relu        |                                        /seq_blocks.9/BatchNormalization_output_0                                         |                     37                    |                     |
+-------+----------------------------------+--------------------+--------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+---------------------+
[33mWARNING [0m [34mQuantization is not set in default config. Skipping quantization.[0m
W0327 14:20:12.758648 140012160939840 onnx_runtime.py:97] Quantization is not set in default config. Skipping quantization.
</pre></div>
</div>
<p>We can view a summary of the ONNX model (which is the unmodified from the Pytorch one), however it should be optimized. Let’s run an analysis path on both the original <code class="docutils literal notranslate"><span class="pre">MaseGraph</span></code> and the <code class="docutils literal notranslate"><span class="pre">.onnx</span></code> optimized model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">runtime_analysis_pass</span><span class="p">(</span><span class="n">mg_original</span><span class="p">,</span> <span class="n">pass_args</span><span class="o">=</span><span class="n">runtime_analysis_config</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[32mINFO    [0m [34mStarting transformation analysis on jsc-toy[0m
I0327 14:20:16.984423 140012160939840 analysis.py:270] Starting transformation analysis on jsc-toy
[32mINFO    [0m [34m
Results jsc-toy:
+------------------------------+---------------+
|      Metric (Per Batch)      |     Value     |
+------------------------------+---------------+
|    Average Test Accuracy     |    0.73159    |
|      Average Precision       |    0.74429    |
|        Average Recall        |    0.73023    |
|       Average F1 Score       |    0.73347    |
|         Average Loss         |    0.76373    |
|       Average Latency        |  0.79688 ms   |
|   Average GPU Power Usage    |   21.816 W    |
| Inference Energy Consumption | 0.0048292 mWh |
+------------------------------+---------------+[0m
I0327 14:20:19.793779 140012160939840 analysis.py:398]
Results jsc-toy:
+------------------------------+---------------+
|      Metric (Per Batch)      |     Value     |
+------------------------------+---------------+
|    Average Test Accuracy     |    0.73159    |
|      Average Precision       |    0.74429    |
|        Average Recall        |    0.73023    |
|       Average F1 Score       |    0.73347    |
|         Average Loss         |    0.76373    |
|       Average Latency        |  0.79688 ms   |
|   Average GPU Power Usage    |   21.816 W    |
| Inference Energy Consumption | 0.0048292 mWh |
+------------------------------+---------------+
[32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-27/mase_graph/version_1/model.json[0m
I0327 14:20:19.796502 140012160939840 analysis.py:84] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-27/mase_graph/version_1/model.json
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">runtime_analysis_pass</span><span class="p">(</span><span class="n">onnx_meta</span><span class="p">[</span><span class="s1">&#39;onnx_path&#39;</span><span class="p">],</span> <span class="n">pass_args</span><span class="o">=</span><span class="n">runtime_analysis_config</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>/root/anaconda3/envs/mase/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider &#39;CUDAExecutionProvider&#39; is not in available provider names.Available providers: &#39;AzureExecutionProvider, CPUExecutionProvider&#39;
  warnings.warn(
[32mINFO    [0m [34mStarting transformation analysis on jsc-toy-onnx[0m
I0327 14:20:33.337222 140012160939840 analysis.py:270] Starting transformation analysis on jsc-toy-onnx


[32mINFO    [0m [34m
Results jsc-toy-onnx:
+------------------------------+---------------+
|      Metric (Per Batch)      |     Value     |
+------------------------------+---------------+
|    Average Test Accuracy     |    0.73412    |
|      Average Precision       |    0.74875    |
|        Average Recall        |    0.73435    |
|       Average F1 Score       |    0.73761    |
|         Average Loss         |    0.74954    |
|       Average Latency        |   0.2215 ms   |
|   Average GPU Power Usage    |   21.575 W    |
| Inference Energy Consumption | 0.0013275 mWh |
+------------------------------+---------------+[0m
I0327 14:20:35.876071 140012160939840 analysis.py:398]
Results jsc-toy-onnx:
+------------------------------+---------------+
|      Metric (Per Batch)      |     Value     |
+------------------------------+---------------+
|    Average Test Accuracy     |    0.73412    |
|      Average Precision       |    0.74875    |
|        Average Recall        |    0.73435    |
|       Average F1 Score       |    0.73761    |
|         Average Loss         |    0.74954    |
|       Average Latency        |   0.2215 ms   |
|   Average GPU Power Usage    |   21.575 W    |
| Inference Energy Consumption | 0.0013275 mWh |
+------------------------------+---------------+
[32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-27/onnx/version_0/model.json[0m
I0327 14:20:35.878773 140012160939840 analysis.py:84] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-27/onnx/version_0/model.json
</pre></div>
</div>
<p>As shown above, the latency of the cpu inference is around 3.5x less with the <code class="docutils literal notranslate"><span class="pre">jsc-toy</span></code> model without compromising accuracy simply by using the optimizations of ONNXRT.</p>
<p>Lets now run the same optimzations, this time using a GPU and a larger model - the <code class="docutils literal notranslate"><span class="pre">vgg7</span></code>.  We will also utilse the chop action from the terminal which runs the same <code class="docutils literal notranslate"><span class="pre">onnx_runtime_interface_pass</span></code> pass.</p>
<p>First lets train the <code class="docutils literal notranslate"><span class="pre">vgg7</span></code> model using the machop <code class="docutils literal notranslate"><span class="pre">train</span></code> action with the config from the new toml file and then load the trained checkpoint it into the <code class="docutils literal notranslate"><span class="pre">transform</span></code> pass.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">VGG_TOML_PATH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;../../../machop/configs/onnx/vgg7_gpu_quant.toml&quot;</span>

<span class="c1"># !ch train --config {VGG_TOML_PATH}</span>

<span class="c1"># Load in the checkpoint from the previous train - modify accordingly</span>
<span class="nv">VGG_CHECKPOINT_PATH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;../../../mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt&quot;</span>

ch<span class="w"> </span>transform<span class="w"> </span>--config<span class="w"> </span><span class="o">{</span>VGG_TOML_PATH<span class="o">}</span><span class="w"> </span>--load<span class="w"> </span><span class="o">{</span>VGG_CHECKPOINT_PATH<span class="o">}</span><span class="w"> </span>--load-type<span class="w"> </span>pl
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[2024-03-28 23:09:44,122] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO: Seed set to 0
WARNING: Logging before flag parsing goes to stderr.
I0328 23:09:47.151937 140014036379456 seed.py:54] Seed set to 0
+-------------------------+------------------------+--------------------------+--------------------------+--------------------------+
| Name                    |        Default         |       Config. File       |     Manual Override      |        Effective         |
+-------------------------+------------------------+--------------------------+--------------------------+--------------------------+
| task                    |     [38;5;8mclassification[0m     |           cls            |                          |           cls            |
| load_name               |          [38;5;8mNone[0m          | [38;5;8m../mase_output/vgg7-pre-[0m | /root/mase/mase_output/v | /root/mase/mase_output/v |
|                         |                        |      [38;5;8mtrained/test-[0m       |  gg7-pre-trained/test-   |  gg7-pre-trained/test-   |
|                         |                        |     [38;5;8maccu-0.9332.ckpt[0m     |     accu-0.9332.ckpt     |     accu-0.9332.ckpt     |
| load_type               |           [38;5;8mmz[0m           |            [38;5;8mpl[0m            |            pl            |            pl            |
| batch_size              |          [38;5;8m128[0m           |            64            |                          |            64            |
| to_debug                |         False          |                          |                          |          False           |
| log_level               |          info          |                          |                          |           info           |
| report_to               |      tensorboard       |                          |                          |       tensorboard        |
| seed                    |           0            |                          |                          |            0             |
| quant_config            |          None          |                          |                          |           None           |
| training_optimizer      |          adam          |                          |                          |           adam           |
| trainer_precision       |        16-mixed        |                          |                          |         16-mixed         |
| learning_rate           |         [38;5;8m1e-05[0m          |          0.001           |                          |          0.001           |
| weight_decay            |           0            |                          |                          |            0             |
| max_epochs              |           [38;5;8m20[0m           |            10            |                          |            10            |
| max_steps               |           -1           |                          |                          |            -1            |
| accumulate_grad_batches |           1            |                          |                          |            1             |
| log_every_n_steps       |           50           |                          |                          |            50            |
| num_workers             |           28           |                          |                          |            28            |
| num_devices             |           1            |                          |                          |            1             |
| num_nodes               |           1            |                          |                          |            1             |
| accelerator             |          [38;5;8mauto[0m          |           gpu            |                          |           gpu            |
| strategy                |          auto          |                          |                          |           auto           |
| is_to_auto_requeue      |         False          |                          |                          |          False           |
| github_ci               |         False          |                          |                          |          False           |
| disable_dataset_cache   |         False          |                          |                          |          False           |
| target                  |  xcu250-figd2104-2L-e  |                          |                          |   xcu250-figd2104-2L-e   |
| num_targets             |          100           |                          |                          |           100            |
| is_pretrained           |         False          |                          |                          |          False           |
| max_token_len           |          512           |                          |                          |           512            |
| project_dir             | /root/mase/mase_output |                          |                          |  /root/mase/mase_output  |
| project                 |          None          |                          |                          |           None           |
| model                   |          [38;5;8mNone[0m          |           vgg7           |                          |           vgg7           |
| dataset                 |          [38;5;8mNone[0m          |         cifar10          |                          |         cifar10          |
| t_max                   |           20           |                          |                          |            20            |
| eta_min                 |         1e-06          |                          |                          |          1e-06           |
+-------------------------+------------------------+--------------------------+--------------------------+--------------------------+
[32mINFO    [0m [34mInitialising model &#39;vgg7&#39;...[0m
I0328 23:09:47.162513 140014036379456 cli.py:846] Initialising model &#39;vgg7&#39;...
[32mINFO    [0m [34mInitialising dataset &#39;cifar10&#39;...[0m
I0328 23:09:47.270153 140014036379456 cli.py:874] Initialising dataset &#39;cifar10&#39;...
[32mINFO    [0m [34mProject will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-28[0m
I0328 23:09:47.270543 140014036379456 cli.py:910] Project will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-28
[32mINFO    [0m [34mTransforming model &#39;vgg7&#39;...[0m
I0328 23:09:47.398360 140014036379456 cli.py:370] Transforming model &#39;vgg7&#39;...
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[32mINFO    [0m [34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt[0m
I0328 23:09:53.493722 140014036379456 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt
[32mINFO    [0m [34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt[0m
I0328 23:09:53.612564 140014036379456 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt
[32mINFO    [0m [34mConverting PyTorch model to ONNX...[0m
I0328 23:10:35.118083 140014036379456 onnx_runtime.py:88] Converting PyTorch model to ONNX...
[32mINFO    [0m [34mProject will be created at /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-28[0m
I0328 23:10:35.119032 140014036379456 onnx_runtime.py:90] Project will be created at /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-28
[32mINFO    [0m [34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-28/optimized/version_3/model.onnx[0m
I0328 23:10:43.779212 140014036379456 onnx_runtime.py:108] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-28/optimized/version_3/model.onnx
[32mINFO    [0m [34mONNX Model Summary:
+-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
| Index |            Name            |   Type   |                                Inputs                               |               Outputs               |                     Attributes                    |
+-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
|   0   |   /feature_layers.0/Conv   |   Conv   |                 input, onnx::Conv_78, onnx::Conv_79                 |   /feature_layers.0/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
|   1   |   /feature_layers.2/Relu   |   Relu   |                   /feature_layers.0/Conv_output_0                   |   /feature_layers.2/Relu_output_0   |                                                   |
|   2   |   /feature_layers.3/Conv   |   Conv   |    /feature_layers.2/Relu_output_0, onnx::Conv_81, onnx::Conv_82    |   /feature_layers.3/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
|   3   |   /feature_layers.5/Relu   |   Relu   |                   /feature_layers.3/Conv_output_0                   |   /feature_layers.5/Relu_output_0   |                                                   |
|   4   | /feature_layers.6/MaxPool  | MaxPool  |                   /feature_layers.5/Relu_output_0                   |  /feature_layers.6/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
|   5   |   /feature_layers.7/Conv   |   Conv   |   /feature_layers.6/MaxPool_output_0, onnx::Conv_84, onnx::Conv_85  |   /feature_layers.7/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
|   6   |   /feature_layers.9/Relu   |   Relu   |                   /feature_layers.7/Conv_output_0                   |   /feature_layers.9/Relu_output_0   |                                                   |
|   7   |  /feature_layers.10/Conv   |   Conv   |    /feature_layers.9/Relu_output_0, onnx::Conv_87, onnx::Conv_88    |   /feature_layers.10/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
|   8   |  /feature_layers.12/Relu   |   Relu   |                   /feature_layers.10/Conv_output_0                  |   /feature_layers.12/Relu_output_0  |                                                   |
|   9   | /feature_layers.13/MaxPool | MaxPool  |                   /feature_layers.12/Relu_output_0                  | /feature_layers.13/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
|   10  |  /feature_layers.14/Conv   |   Conv   |  /feature_layers.13/MaxPool_output_0, onnx::Conv_90, onnx::Conv_91  |   /feature_layers.14/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
|   11  |  /feature_layers.16/Relu   |   Relu   |                   /feature_layers.14/Conv_output_0                  |   /feature_layers.16/Relu_output_0  |                                                   |
|   12  |  /feature_layers.17/Conv   |   Conv   |    /feature_layers.16/Relu_output_0, onnx::Conv_93, onnx::Conv_94   |   /feature_layers.17/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
|   13  |  /feature_layers.19/Relu   |   Relu   |                   /feature_layers.17/Conv_output_0                  |   /feature_layers.19/Relu_output_0  |                                                   |
|   14  | /feature_layers.20/MaxPool | MaxPool  |                   /feature_layers.19/Relu_output_0                  | /feature_layers.20/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
|   15  |         /Constant          | Constant |                                                                     |          /Constant_output_0         |                       value                       |
|   16  |          /Reshape          | Reshape  |       /feature_layers.20/MaxPool_output_0, /Constant_output_0       |          /Reshape_output_0          |                                                   |
|   17  |     /classifier.0/Gemm     |   Gemm   |      /Reshape_output_0, classifier.0.weight, classifier.0.bias      |     /classifier.0/Gemm_output_0     |                alpha, beta, transB                |
|   18  |     /classifier.1/Relu     |   Relu   |                     /classifier.0/Gemm_output_0                     |     /classifier.1/Relu_output_0     |                                                   |
|   19  |     /classifier.2/Gemm     |   Gemm   | /classifier.1/Relu_output_0, classifier.2.weight, classifier.2.bias |     /classifier.2/Gemm_output_0     |                alpha, beta, transB                |
|   20  |     /classifier.3/Relu     |   Relu   |                     /classifier.2/Gemm_output_0                     |     /classifier.3/Relu_output_0     |                                                   |
|   21  |      /last_layer/Gemm      |   Gemm   |   /classifier.3/Relu_output_0, last_layer.weight, last_layer.bias   |                  76                 |                alpha, beta, transB                |
+-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+[0m
I0328 23:10:43.897069 140014036379456 onnx_runtime.py:146] ONNX Model Summary:
+-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
| Index |            Name            |   Type   |                                Inputs                               |               Outputs               |                     Attributes                    |
+-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
|   0   |   /feature_layers.0/Conv   |   Conv   |                 input, onnx::Conv_78, onnx::Conv_79                 |   /feature_layers.0/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
|   1   |   /feature_layers.2/Relu   |   Relu   |                   /feature_layers.0/Conv_output_0                   |   /feature_layers.2/Relu_output_0   |                                                   |
|   2   |   /feature_layers.3/Conv   |   Conv   |    /feature_layers.2/Relu_output_0, onnx::Conv_81, onnx::Conv_82    |   /feature_layers.3/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
|   3   |   /feature_layers.5/Relu   |   Relu   |                   /feature_layers.3/Conv_output_0                   |   /feature_layers.5/Relu_output_0   |                                                   |
|   4   | /feature_layers.6/MaxPool  | MaxPool  |                   /feature_layers.5/Relu_output_0                   |  /feature_layers.6/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
|   5   |   /feature_layers.7/Conv   |   Conv   |   /feature_layers.6/MaxPool_output_0, onnx::Conv_84, onnx::Conv_85  |   /feature_layers.7/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
|   6   |   /feature_layers.9/Relu   |   Relu   |                   /feature_layers.7/Conv_output_0                   |   /feature_layers.9/Relu_output_0   |                                                   |
|   7   |  /feature_layers.10/Conv   |   Conv   |    /feature_layers.9/Relu_output_0, onnx::Conv_87, onnx::Conv_88    |   /feature_layers.10/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
|   8   |  /feature_layers.12/Relu   |   Relu   |                   /feature_layers.10/Conv_output_0                  |   /feature_layers.12/Relu_output_0  |                                                   |
|   9   | /feature_layers.13/MaxPool | MaxPool  |                   /feature_layers.12/Relu_output_0                  | /feature_layers.13/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
|   10  |  /feature_layers.14/Conv   |   Conv   |  /feature_layers.13/MaxPool_output_0, onnx::Conv_90, onnx::Conv_91  |   /feature_layers.14/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
|   11  |  /feature_layers.16/Relu   |   Relu   |                   /feature_layers.14/Conv_output_0                  |   /feature_layers.16/Relu_output_0  |                                                   |
|   12  |  /feature_layers.17/Conv   |   Conv   |    /feature_layers.16/Relu_output_0, onnx::Conv_93, onnx::Conv_94   |   /feature_layers.17/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
|   13  |  /feature_layers.19/Relu   |   Relu   |                   /feature_layers.17/Conv_output_0                  |   /feature_layers.19/Relu_output_0  |                                                   |
|   14  | /feature_layers.20/MaxPool | MaxPool  |                   /feature_layers.19/Relu_output_0                  | /feature_layers.20/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
|   15  |         /Constant          | Constant |                                                                     |          /Constant_output_0         |                       value                       |
|   16  |          /Reshape          | Reshape  |       /feature_layers.20/MaxPool_output_0, /Constant_output_0       |          /Reshape_output_0          |                                                   |
|   17  |     /classifier.0/Gemm     |   Gemm   |      /Reshape_output_0, classifier.0.weight, classifier.0.bias      |     /classifier.0/Gemm_output_0     |                alpha, beta, transB                |
|   18  |     /classifier.1/Relu     |   Relu   |                     /classifier.0/Gemm_output_0                     |     /classifier.1/Relu_output_0     |                                                   |
|   19  |     /classifier.2/Gemm     |   Gemm   | /classifier.1/Relu_output_0, classifier.2.weight, classifier.2.bias |     /classifier.2/Gemm_output_0     |                alpha, beta, transB                |
|   20  |     /classifier.3/Relu     |   Relu   |                     /classifier.2/Gemm_output_0                     |     /classifier.3/Relu_output_0     |                                                   |
|   21  |      /last_layer/Gemm      |   Gemm   |   /classifier.3/Relu_output_0, last_layer.weight, last_layer.bias   |                  76                 |                alpha, beta, transB                |
+-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
[32mINFO    [0m [34mQuantizing model using static quantization with calibration...[0m
I0328 23:10:44.711065 140014036379456 quantize.py:68] Quantizing model using static quantization with calibration...
[32mINFO    [0m [34mUsing CUDA as ONNX execution provider.[0m
I0328 23:10:44.711297 140014036379456 utils.py:9] Using CUDA as ONNX execution provider.
[32mINFO    [0m [34mQuantization complete. Model is now calibrated and statically quantized.[0m
I0328 23:15:18.611725 140014036379456 quantize.py:122] Quantization complete. Model is now calibrated and statically quantized.
[32mINFO    [0m [34mQuantizing model using dynamic quantization...[0m
I0328 23:15:18.623580 140014036379456 quantize.py:45] Quantizing model using dynamic quantization...
[32mINFO    [0m [34mQuantization complete. Model is now dynamically quantized.[0m
I0328 23:15:19.266244 140014036379456 quantize.py:62] Quantization complete. Model is now dynamically quantized.
[32mINFO    [0m [34mQuantizing model using automatic mixed precision quantization...[0m
I0328 23:15:19.266812 140014036379456 quantize.py:132] Quantizing model using automatic mixed precision quantization...
Adding missing dtypes for 0 outputs
[&#39;/feature_layers.0/Conv&#39;, &#39;/feature_layers.2/Relu&#39;, &#39;/feature_layers.3/Conv&#39;, &#39;/feature_layers.5/Relu&#39;, &#39;/feature_layers.6/MaxPool&#39;, &#39;/feature_layers.7/Conv&#39;, &#39;/feature_layers.9/Relu&#39;, &#39;/feature_layers.10/Conv&#39;, &#39;/feature_layers.12/Relu&#39;, &#39;/feature_layers.13/MaxPool&#39;, &#39;/feature_layers.14/Conv&#39;, &#39;/feature_layers.16/Relu&#39;, &#39;/feature_layers.17/Conv&#39;, &#39;/feature_layers.19/Relu&#39;, &#39;/feature_layers.20/MaxPool&#39;, &#39;/Reshape&#39;, &#39;/classifier.0/Gemm&#39;, &#39;/classifier.1/Relu&#39;, &#39;/classifier.2/Gemm&#39;, &#39;/classifier.3/Relu&#39;, &#39;/last_layer/Gemm&#39;]
True
Sanity checks passed. Starting autoconvert.
Running attempt 1 excluding conversion of 0 nodes
[]
True
Attempt succeeded.
[*21*]
Done: []
[]
Final model validated successfully.
[32mINFO    [0m [34mQuantization complete. Model is now quantized using automatic mixed precision.[0m
I0328 23:15:33.080978 140014036379456 quantize.py:164] Quantization complete. Model is now quantized using automatic mixed precision.
[32mINFO    [0m [34mPerforming runtime analysis on original graph...[0m
I0328 23:15:33.103024 140014036379456 transform.py:257] Performing runtime analysis on original graph...
[32mINFO    [0m [34mStarting transformation analysis on vgg7[0m
I0328 23:15:33.103300 140014036379456 runtime_analysis.py:357] Starting transformation analysis on vgg7
[32mINFO    [0m [34m
Results vgg7:
+------------------------------+-------------+
|      Metric (Per Batch)      |    Value    |
+------------------------------+-------------+
|    Average Test Accuracy     |   0.91831   |
|      Average Precision       |   0.91791   |
|        Average Recall        |   0.91793   |
|       Average F1 Score       |   0.91778   |
|         Average Loss         |   0.24676   |
|       Average Latency        |  8.6022 ms  |
|   Average GPU Power Usage    |  50.352 W   |
| Inference Energy Consumption | 0.12032 mWh |
+------------------------------+-------------+[0m
I0328 23:15:43.756563 140014036379456 runtime_analysis.py:521]
Results vgg7:
+------------------------------+-------------+
|      Metric (Per Batch)      |    Value    |
+------------------------------+-------------+
|    Average Test Accuracy     |   0.91831   |
|      Average Precision       |   0.91791   |
|        Average Recall        |   0.91793   |
|       Average F1 Score       |   0.91778   |
|         Average Loss         |   0.24676   |
|       Average Latency        |  8.6022 ms  |
|   Average GPU Power Usage    |  50.352 W   |
| Inference Energy Consumption | 0.12032 mWh |
+------------------------------+-------------+
[32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_47/model.json[0m
I0328 23:15:43.758996 140014036379456 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/mase_graph/version_47/model.json
[32mINFO    [0m [34mPerforming runtime analysis on onnx-optimized graph...[0m
I0328 23:15:43.759325 140014036379456 transform.py:263] Performing runtime analysis on onnx-optimized graph...
[32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
I0328 23:15:43.759659 140014036379456 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
[32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
I0328 23:15:43.972420 140014036379456 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
[32mINFO    [0m [34m
Results vgg7-onnx:
+------------------------------+--------------+
|      Metric (Per Batch)      |    Value     |
+------------------------------+--------------+
|    Average Test Accuracy     |   0.93054    |
|      Average Precision       |   0.93154    |
|        Average Recall        |   0.93158    |
|       Average F1 Score       |   0.93138    |
|         Average Loss         |   0.22327    |
|       Average Latency        |  6.0678 ms   |
|   Average GPU Power Usage    |   55.829 W   |
| Inference Energy Consumption | 0.094099 mWh |
+------------------------------+--------------+[0m
I0328 23:15:53.476423 140014036379456 runtime_analysis.py:521]
Results vgg7-onnx:
+------------------------------+--------------+
|      Metric (Per Batch)      |    Value     |
+------------------------------+--------------+
|    Average Test Accuracy     |   0.93054    |
|      Average Precision       |   0.93154    |
|        Average Recall        |   0.93158    |
|       Average F1 Score       |   0.93138    |
|         Average Loss         |   0.22327    |
|       Average Latency        |  6.0678 ms   |
|   Average GPU Power Usage    |   55.829 W   |
| Inference Energy Consumption | 0.094099 mWh |
+------------------------------+--------------+
[32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_9/model.json[0m
I0328 23:15:53.478985 140014036379456 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_9/model.json
[32mINFO    [0m [34mPerforming runtime analysis on static quantized graph...[0m
I0328 23:15:53.539209 140014036379456 transform.py:282] Performing runtime analysis on static quantized graph...
[32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
I0328 23:15:53.539841 140014036379456 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
[0;93m2024-03-28 23:15:53.640481870 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 9 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.[m
[0;93m2024-03-28 23:15:53.641186374 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.[m
[0;93m2024-03-28 23:15:53.641201703 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.[m
[32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
I0328 23:15:53.654182 140014036379456 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
[32mINFO    [0m [34m
Results vgg7-onnx:
+------------------------------+-------------+
|      Metric (Per Batch)      |    Value    |
+------------------------------+-------------+
|    Average Test Accuracy     |   0.93175   |
|      Average Precision       |   0.93244   |
|        Average Recall        |   0.93224   |
|       Average F1 Score       |   0.93212   |
|         Average Loss         |   0.22437   |
|       Average Latency        |  3.8347 ms  |
|   Average GPU Power Usage    |  54.555 W   |
| Inference Energy Consumption | 0.14388 mWh |
+------------------------------+-------------+[0m
I0328 23:16:03.469463 140014036379456 runtime_analysis.py:521]
Results vgg7-onnx:
+------------------------------+-------------+
|      Metric (Per Batch)      |    Value    |
+------------------------------+-------------+
|    Average Test Accuracy     |   0.93175   |
|      Average Precision       |   0.93244   |
|        Average Recall        |   0.93224   |
|       Average F1 Score       |   0.93212   |
|         Average Loss         |   0.22437   |
|       Average Latency        |  3.8347 ms  |
|   Average GPU Power Usage    |  54.555 W   |
| Inference Energy Consumption | 0.14388 mWh |
+------------------------------+-------------+
[32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_10/model.json[0m
I0328 23:16:03.472026 140014036379456 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_10/model.json
[32mINFO    [0m [34mPerforming runtime analysis on dynamic quantized graph...[0m
I0328 23:16:03.488069 140014036379456 transform.py:290] Performing runtime analysis on dynamic quantized graph...
[32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
I0328 23:16:03.489004 140014036379456 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
[0;93m2024-03-28 23:16:03.624021594 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 26 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.[m
[0;93m2024-03-28 23:16:03.624989637 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.[m
[0;93m2024-03-28 23:16:03.625007282 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.[m
[32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
I0328 23:16:03.641838 140014036379456 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
[32mINFO    [0m [34m
Results vgg7-onnx:
+------------------------------+------------+
|      Metric (Per Batch)      |   Value    |
+------------------------------+------------+
|    Average Test Accuracy     |  0.93162   |
|      Average Precision       |  0.93261   |
|        Average Recall        |  0.93257   |
|       Average F1 Score       |  0.93241   |
|         Average Loss         |  0.22253   |
|       Average Latency        |  5.1273 ms |
|   Average GPU Power Usage    |  22.86 W   |
| Inference Energy Consumption | 0.1748 mWh |
+------------------------------+------------+[0m
I0328 23:18:23.964464 140014036379456 runtime_analysis.py:521]
Results vgg7-onnx:
+------------------------------+------------+
|      Metric (Per Batch)      |   Value    |
+------------------------------+------------+
|    Average Test Accuracy     |  0.93162   |
|      Average Precision       |  0.93261   |
|        Average Recall        |  0.93257   |
|       Average F1 Score       |  0.93241   |
|         Average Loss         |  0.22253   |
|       Average Latency        |  5.1273 ms |
|   Average GPU Power Usage    |  22.86 W   |
| Inference Energy Consumption | 0.1748 mWh |
+------------------------------+------------+
[32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_11/model.json[0m
I0328 23:18:23.966642 140014036379456 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_11/model.json
[32mINFO    [0m [34mPerforming runtime analysis on auto mixed precision quantized graph...[0m
I0328 23:18:23.987104 140014036379456 transform.py:298] Performing runtime analysis on auto mixed precision quantized graph...
[32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
I0328 23:18:23.987448 140014036379456 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
[32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
I0328 23:18:24.154293 140014036379456 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
[32mINFO    [0m [34m
Results vgg7-onnx:
+------------------------------+--------------+
|      Metric (Per Batch)      |    Value     |
+------------------------------+--------------+
|    Average Test Accuracy     |   0.93087    |
|      Average Precision       |   0.93188    |
|        Average Recall        |   0.93191    |
|       Average F1 Score       |   0.93172    |
|         Average Loss         |   0.22324    |
|       Average Latency        |  5.4846 ms   |
|   Average GPU Power Usage    |   50.354 W   |
| Inference Energy Consumption | 0.076714 mWh |
+------------------------------+--------------+[0m
I0328 23:18:33.854084 140014036379456 runtime_analysis.py:521]
Results vgg7-onnx:
+------------------------------+--------------+
|      Metric (Per Batch)      |    Value     |
+------------------------------+--------------+
|    Average Test Accuracy     |   0.93087    |
|      Average Precision       |   0.93188    |
|        Average Recall        |   0.93191    |
|       Average F1 Score       |   0.93172    |
|         Average Loss         |   0.22324    |
|       Average Latency        |  5.4846 ms   |
|   Average GPU Power Usage    |   50.354 W   |
| Inference Energy Consumption | 0.076714 mWh |
+------------------------------+--------------+
[32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_12/model.json[0m
I0328 23:18:33.855542 140014036379456 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-28/onnx/version_12/model.json
</pre></div>
</div>
<p>As shown above, the latency of the gpu inference is 30% less with the <code class="docutils literal notranslate"><span class="pre">vgg7</span></code> model without compromising accuracy simply by using the optimizations of ONNXRT.</p>
<p>We will now look at quantization to further speed up the model.</p>
</section>
<section id="section-2-quantization">
<h2>Section 2. Quantization<a class="headerlink" href="#section-2-quantization" title="Link to this heading">#</a></h2>
<p>We may quantize either using FP16 or INT8 by setting the <code class="docutils literal notranslate"><span class="pre">precision</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">passes.onnxruntime.default.config</span></code> to <code class="docutils literal notranslate"><span class="pre">'fp16'</span></code> or <code class="docutils literal notranslate"><span class="pre">'int8'</span></code> respectively. INT8 quantization will show the most notable latency improvements but is more likely to lower performance.</p>
<p>There are three types of quantization for ONNXRT and can be set in <code class="docutils literal notranslate"><span class="pre">onnxruntime.default.config</span></code> under <code class="docutils literal notranslate"><span class="pre">quantization_types</span></code>. The differences of the first two are for how they calibrate i.e. set the scale and zero points which are only relevant for integer based quantization:</p>
<ul class="simple">
<li><p><strong>Static Quantization</strong>:</p>
<ul>
<li><p>The scale and zero point of activations are calculated in advance (offline) using a calibration data set.</p></li>
<li><p>The activations have the same scale and zero point during each forward pass.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">num_calibration_batches</span></code> parameter must also be set to ensure calibration is tested on a subset of the training dataset. A larger subset will be beneficial for calibrating the amaxes and may improve accuracy, however it will result in a longer calibration time.</p></li>
</ul>
</li>
<li><p><strong>Dynamic Quantization</strong>:</p>
<ul>
<li><p>The scale and zero point of activations are calculated on-the-fly (online) and are specific for each forward pass.</p></li>
<li><p>This approach is more accurate but introduces extra computational overhead</p></li>
</ul>
</li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">onnx_runtime_interface_pass</span></code> pass also supports mixed precision. This is an automatic only procedure, where ONNXRT finds a minimal set of ops to skip while retaining a certain level of accuracy, converting most of the ops to float16 but leaving some in float32.</p>
<ul class="simple">
<li><p><strong>Auto Mixed Precision Quantization</strong>:</p>
<ul>
<li><p>Automatically adjusts between FP16 and FP32 precisions to retain certain level of accuracy</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">precision</span></code> parameter does not need to be set in the config since the whole process is automatic.</p></li>
<li><p>Unfortunately, this process is currently only supported on GPU.</p></li>
<li><p>This approach is most beneficial when INT8 or FP16 exclusive quantizations (static or dynamic) are giving poor results.</p></li>
</ul>
</li>
</ul>
<p>All three methodolgies first pre-procsses the model before quantization adding further optimizations. This intermidate model is stored to the <code class="docutils literal notranslate"><span class="pre">pre-processed</span></code> directory.</p>
<p>For this example, we will set the <code class="docutils literal notranslate"><span class="pre">precision</span></code> to <code class="docutils literal notranslate"><span class="pre">'uint8'</span></code> (since <code class="docutils literal notranslate"><span class="pre">ConvInteger</span></code> node is not currently supported for <code class="docutils literal notranslate"><span class="pre">'int8'</span></code> on ONNXRT GPU execution provider).</p>
<p>We will also set the <code class="docutils literal notranslate"><span class="pre">precision_types</span></code> to <code class="docutils literal notranslate"><span class="pre">['static',</span> <span class="pre">'dynamic',</span> <span class="pre">'auto']</span></code> to compare all three quantization methods, whilst keeping the other settings the exact same for a fair comparison against the optimized <code class="docutils literal notranslate"><span class="pre">vgg7</span></code> model used in the previous section.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">VGG_TOML_PATH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;../../../machop/configs/onnx/vgg7_gpu_quant.toml&quot;</span>
<span class="nv">VGG_CHECKPOINT_PATH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;../../../mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt&quot;</span>
!ch<span class="w"> </span>transform<span class="w"> </span>--config<span class="w"> </span><span class="o">{</span>VGG_TOML_PATH<span class="o">}</span><span class="w"> </span>--load<span class="w"> </span><span class="o">{</span>VGG_CHECKPOINT_PATH<span class="o">}</span><span class="w"> </span>--load-type<span class="w"> </span>pl
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>    [2024-03-29 13:49:26,029] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
    INFO: Seed set to 0
    WARNING: Logging before flag parsing goes to stderr.
    I0329 13:49:29.166126 139783521261376 seed.py:54] Seed set to 0
    +-------------------------+------------------------+--------------------------+--------------------------+--------------------------+
    | Name                    |        Default         |       Config. File       |     Manual Override      |        Effective         |
    +-------------------------+------------------------+--------------------------+--------------------------+--------------------------+
    | task                    |     [38;5;8mclassification[0m     |           cls            |                          |           cls            |
    | load_name               |          [38;5;8mNone[0m          | [38;5;8m../mase_output/vgg7-pre-[0m | /root/mase/mase_output/v | /root/mase/mase_output/v |
    |                         |                        |      [38;5;8mtrained/test-[0m       |  gg7-pre-trained/test-   |  gg7-pre-trained/test-   |
    |                         |                        |     [38;5;8maccu-0.9332.ckpt[0m     |     accu-0.9332.ckpt     |     accu-0.9332.ckpt     |
    | load_type               |           [38;5;8mmz[0m           |            [38;5;8mpl[0m            |            pl            |            pl            |
    | batch_size              |          [38;5;8m128[0m           |            64            |                          |            64            |
    | to_debug                |         False          |                          |                          |          False           |
    | log_level               |          info          |                          |                          |           info           |
    | report_to               |      tensorboard       |                          |                          |       tensorboard        |
    | seed                    |           0            |                          |                          |            0             |
    | quant_config            |          None          |                          |                          |           None           |
    | training_optimizer      |          adam          |                          |                          |           adam           |
    | trainer_precision       |        16-mixed        |                          |                          |         16-mixed         |
    | learning_rate           |         [38;5;8m1e-05[0m          |          0.001           |                          |          0.001           |
    | weight_decay            |           0            |                          |                          |            0             |
    | max_epochs              |           [38;5;8m20[0m           |            10            |                          |            10            |
    | max_steps               |           -1           |                          |                          |            -1            |
    | accumulate_grad_batches |           1            |                          |                          |            1             |
    | log_every_n_steps       |           50           |                          |                          |            50            |
    | num_workers             |           28           |                          |                          |            28            |
    | num_devices             |           1            |                          |                          |            1             |
    | num_nodes               |           1            |                          |                          |            1             |
    | accelerator             |          [38;5;8mauto[0m          |           gpu            |                          |           gpu            |
    | strategy                |          auto          |                          |                          |           auto           |
    | is_to_auto_requeue      |         False          |                          |                          |          False           |
    | github_ci               |         False          |                          |                          |          False           |
    | disable_dataset_cache   |         False          |                          |                          |          False           |
    | target                  |  xcu250-figd2104-2L-e  |                          |                          |   xcu250-figd2104-2L-e   |
    | num_targets             |          100           |                          |                          |           100            |
    | is_pretrained           |         False          |                          |                          |          False           |
    | max_token_len           |          512           |                          |                          |           512            |
    | project_dir             | /root/mase/mase_output |                          |                          |  /root/mase/mase_output  |
    | project                 |          None          |                          |                          |           None           |
    | model                   |          [38;5;8mNone[0m          |           vgg7           |                          |           vgg7           |
    | dataset                 |          [38;5;8mNone[0m          |         cifar10          |                          |         cifar10          |
    | t_max                   |           20           |                          |                          |            20            |
    | eta_min                 |         1e-06          |                          |                          |          1e-06           |
    +-------------------------+------------------------+--------------------------+--------------------------+--------------------------+
    [32mINFO    [0m [34mInitialising model &#39;vgg7&#39;...[0m
    I0329 13:49:29.176632 139783521261376 cli.py:846] Initialising model &#39;vgg7&#39;...
    [32mINFO    [0m [34mInitialising dataset &#39;cifar10&#39;...[0m
    I0329 13:49:29.286156 139783521261376 cli.py:874] Initialising dataset &#39;cifar10&#39;...
    [32mINFO    [0m [34mProject will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-29[0m
    I0329 13:49:29.287228 139783521261376 cli.py:910] Project will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-29
    [32mINFO    [0m [34mTransforming model &#39;vgg7&#39;...[0m
    I0329 13:49:29.419221 139783521261376 cli.py:370] Transforming model &#39;vgg7&#39;...
    Files already downloaded and verified
    Files already downloaded and verified
    Files already downloaded and verified
    Files already downloaded and verified
    Files already downloaded and verified
    Files already downloaded and verified
    Files already downloaded and verified
    Files already downloaded and verified
    [32mINFO    [0m [34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt[0m
    I0329 13:49:35.331030 139783521261376 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt
    [32mINFO    [0m [34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt[0m
    I0329 13:49:35.445306 139783521261376 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt
    [32mINFO    [0m [34mConverting PyTorch model to ONNX...[0m
    I0329 13:50:32.507291 139783521261376 onnx_runtime.py:88] Converting PyTorch model to ONNX...
    [32mINFO    [0m [34mProject will be created at /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-29[0m
    I0329 13:50:32.508896 139783521261376 onnx_runtime.py:90] Project will be created at /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-29
    [32mINFO    [0m [34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-29/optimized/version_0/model.onnx[0m
    I0329 13:50:53.587861 139783521261376 onnx_runtime.py:108] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/onnxrt/vgg7_cls_cifar10_2024-03-29/optimized/version_0/model.onnx
    [32mINFO    [0m [34mONNX Model Summary:
    +-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
    | Index |            Name            |   Type   |                                Inputs                               |               Outputs               |                     Attributes                    |
    +-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
    |   0   |   /feature_layers.0/Conv   |   Conv   |                 input, onnx::Conv_78, onnx::Conv_79                 |   /feature_layers.0/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
    |   1   |   /feature_layers.2/Relu   |   Relu   |                   /feature_layers.0/Conv_output_0                   |   /feature_layers.2/Relu_output_0   |                                                   |
    |   2   |   /feature_layers.3/Conv   |   Conv   |    /feature_layers.2/Relu_output_0, onnx::Conv_81, onnx::Conv_82    |   /feature_layers.3/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
    |   3   |   /feature_layers.5/Relu   |   Relu   |                   /feature_layers.3/Conv_output_0                   |   /feature_layers.5/Relu_output_0   |                                                   |
    |   4   | /feature_layers.6/MaxPool  | MaxPool  |                   /feature_layers.5/Relu_output_0                   |  /feature_layers.6/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
    |   5   |   /feature_layers.7/Conv   |   Conv   |   /feature_layers.6/MaxPool_output_0, onnx::Conv_84, onnx::Conv_85  |   /feature_layers.7/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
    |   6   |   /feature_layers.9/Relu   |   Relu   |                   /feature_layers.7/Conv_output_0                   |   /feature_layers.9/Relu_output_0   |                                                   |
    |   7   |  /feature_layers.10/Conv   |   Conv   |    /feature_layers.9/Relu_output_0, onnx::Conv_87, onnx::Conv_88    |   /feature_layers.10/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
    |   8   |  /feature_layers.12/Relu   |   Relu   |                   /feature_layers.10/Conv_output_0                  |   /feature_layers.12/Relu_output_0  |                                                   |
    |   9   | /feature_layers.13/MaxPool | MaxPool  |                   /feature_layers.12/Relu_output_0                  | /feature_layers.13/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
    |   10  |  /feature_layers.14/Conv   |   Conv   |  /feature_layers.13/MaxPool_output_0, onnx::Conv_90, onnx::Conv_91  |   /feature_layers.14/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
    |   11  |  /feature_layers.16/Relu   |   Relu   |                   /feature_layers.14/Conv_output_0                  |   /feature_layers.16/Relu_output_0  |                                                   |
    |   12  |  /feature_layers.17/Conv   |   Conv   |    /feature_layers.16/Relu_output_0, onnx::Conv_93, onnx::Conv_94   |   /feature_layers.17/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
    |   13  |  /feature_layers.19/Relu   |   Relu   |                   /feature_layers.17/Conv_output_0                  |   /feature_layers.19/Relu_output_0  |                                                   |
    |   14  | /feature_layers.20/MaxPool | MaxPool  |                   /feature_layers.19/Relu_output_0                  | /feature_layers.20/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
    |   15  |         /Constant          | Constant |                                                                     |          /Constant_output_0         |                       value                       |
    |   16  |          /Reshape          | Reshape  |       /feature_layers.20/MaxPool_output_0, /Constant_output_0       |          /Reshape_output_0          |                                                   |
    |   17  |     /classifier.0/Gemm     |   Gemm   |      /Reshape_output_0, classifier.0.weight, classifier.0.bias      |     /classifier.0/Gemm_output_0     |                alpha, beta, transB                |
    |   18  |     /classifier.1/Relu     |   Relu   |                     /classifier.0/Gemm_output_0                     |     /classifier.1/Relu_output_0     |                                                   |
    |   19  |     /classifier.2/Gemm     |   Gemm   | /classifier.1/Relu_output_0, classifier.2.weight, classifier.2.bias |     /classifier.2/Gemm_output_0     |                alpha, beta, transB                |
    |   20  |     /classifier.3/Relu     |   Relu   |                     /classifier.2/Gemm_output_0                     |     /classifier.3/Relu_output_0     |                                                   |
    |   21  |      /last_layer/Gemm      |   Gemm   |   /classifier.3/Relu_output_0, last_layer.weight, last_layer.bias   |                  76                 |                alpha, beta, transB                |
    +-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+[0m
    I0329 13:50:53.719763 139783521261376 onnx_runtime.py:146] ONNX Model Summary:
    +-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
    | Index |            Name            |   Type   |                                Inputs                               |               Outputs               |                     Attributes                    |
    +-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
    |   0   |   /feature_layers.0/Conv   |   Conv   |                 input, onnx::Conv_78, onnx::Conv_79                 |   /feature_layers.0/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
    |   1   |   /feature_layers.2/Relu   |   Relu   |                   /feature_layers.0/Conv_output_0                   |   /feature_layers.2/Relu_output_0   |                                                   |
    |   2   |   /feature_layers.3/Conv   |   Conv   |    /feature_layers.2/Relu_output_0, onnx::Conv_81, onnx::Conv_82    |   /feature_layers.3/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
    |   3   |   /feature_layers.5/Relu   |   Relu   |                   /feature_layers.3/Conv_output_0                   |   /feature_layers.5/Relu_output_0   |                                                   |
    |   4   | /feature_layers.6/MaxPool  | MaxPool  |                   /feature_layers.5/Relu_output_0                   |  /feature_layers.6/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
    |   5   |   /feature_layers.7/Conv   |   Conv   |   /feature_layers.6/MaxPool_output_0, onnx::Conv_84, onnx::Conv_85  |   /feature_layers.7/Conv_output_0   |   dilations, group, kernel_shape, pads, strides   |
    |   6   |   /feature_layers.9/Relu   |   Relu   |                   /feature_layers.7/Conv_output_0                   |   /feature_layers.9/Relu_output_0   |                                                   |
    |   7   |  /feature_layers.10/Conv   |   Conv   |    /feature_layers.9/Relu_output_0, onnx::Conv_87, onnx::Conv_88    |   /feature_layers.10/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
    |   8   |  /feature_layers.12/Relu   |   Relu   |                   /feature_layers.10/Conv_output_0                  |   /feature_layers.12/Relu_output_0  |                                                   |
    |   9   | /feature_layers.13/MaxPool | MaxPool  |                   /feature_layers.12/Relu_output_0                  | /feature_layers.13/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
    |   10  |  /feature_layers.14/Conv   |   Conv   |  /feature_layers.13/MaxPool_output_0, onnx::Conv_90, onnx::Conv_91  |   /feature_layers.14/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
    |   11  |  /feature_layers.16/Relu   |   Relu   |                   /feature_layers.14/Conv_output_0                  |   /feature_layers.16/Relu_output_0  |                                                   |
    |   12  |  /feature_layers.17/Conv   |   Conv   |    /feature_layers.16/Relu_output_0, onnx::Conv_93, onnx::Conv_94   |   /feature_layers.17/Conv_output_0  |   dilations, group, kernel_shape, pads, strides   |
    |   13  |  /feature_layers.19/Relu   |   Relu   |                   /feature_layers.17/Conv_output_0                  |   /feature_layers.19/Relu_output_0  |                                                   |
    |   14  | /feature_layers.20/MaxPool | MaxPool  |                   /feature_layers.19/Relu_output_0                  | /feature_layers.20/MaxPool_output_0 | ceil_mode, dilations, kernel_shape, pads, strides |
    |   15  |         /Constant          | Constant |                                                                     |          /Constant_output_0         |                       value                       |
    |   16  |          /Reshape          | Reshape  |       /feature_layers.20/MaxPool_output_0, /Constant_output_0       |          /Reshape_output_0          |                                                   |
    |   17  |     /classifier.0/Gemm     |   Gemm   |      /Reshape_output_0, classifier.0.weight, classifier.0.bias      |     /classifier.0/Gemm_output_0     |                alpha, beta, transB                |
    |   18  |     /classifier.1/Relu     |   Relu   |                     /classifier.0/Gemm_output_0                     |     /classifier.1/Relu_output_0     |                                                   |
    |   19  |     /classifier.2/Gemm     |   Gemm   | /classifier.1/Relu_output_0, classifier.2.weight, classifier.2.bias |     /classifier.2/Gemm_output_0     |                alpha, beta, transB                |
    |   20  |     /classifier.3/Relu     |   Relu   |                     /classifier.2/Gemm_output_0                     |     /classifier.3/Relu_output_0     |                                                   |
    |   21  |      /last_layer/Gemm      |   Gemm   |   /classifier.3/Relu_output_0, last_layer.weight, last_layer.bias   |                  76                 |                alpha, beta, transB                |
    +-------+----------------------------+----------+---------------------------------------------------------------------+-------------------------------------+---------------------------------------------------+
    [32mINFO    [0m [34mQuantizing model using static quantization with calibration...[0m
    I0329 13:50:54.538058 139783521261376 quantize.py:68] Quantizing model using static quantization with calibration...
    [32mINFO    [0m [34mUsing CUDA as ONNX execution provider.[0m
    I0329 13:50:54.538278 139783521261376 utils.py:9] Using CUDA as ONNX execution provider.
    [32mINFO    [0m [34mQuantization complete. Model is now calibrated and statically quantized.[0m
    I0329 13:55:09.302043 139783521261376 quantize.py:122] Quantization complete. Model is now calibrated and statically quantized.
    [32mINFO    [0m [34mQuantizing model using dynamic quantization...[0m
    I0329 13:55:09.317632 139783521261376 quantize.py:45] Quantizing model using dynamic quantization...
    [32mINFO    [0m [34mQuantization complete. Model is now dynamically quantized.[0m
    I0329 13:55:10.003557 139783521261376 quantize.py:62] Quantization complete. Model is now dynamically quantized.
    [32mINFO    [0m [34mQuantizing model using automatic mixed precision quantization...[0m
    I0329 13:55:10.004111 139783521261376 quantize.py:132] Quantizing model using automatic mixed precision quantization...
    Adding missing dtypes for 0 outputs
    [&#39;/feature_layers.0/Conv&#39;, &#39;/feature_layers.2/Relu&#39;, &#39;/feature_layers.3/Conv&#39;, &#39;/feature_layers.5/Relu&#39;, &#39;/feature_layers.6/MaxPool&#39;, &#39;/feature_layers.7/Conv&#39;, &#39;/feature_layers.9/Relu&#39;, &#39;/feature_layers.10/Conv&#39;, &#39;/feature_layers.12/Relu&#39;, &#39;/feature_layers.13/MaxPool&#39;, &#39;/feature_layers.14/Conv&#39;, &#39;/feature_layers.16/Relu&#39;, &#39;/feature_layers.17/Conv&#39;, &#39;/feature_layers.19/Relu&#39;, &#39;/feature_layers.20/MaxPool&#39;, &#39;/Reshape&#39;, &#39;/classifier.0/Gemm&#39;, &#39;/classifier.1/Relu&#39;, &#39;/classifier.2/Gemm&#39;, &#39;/classifier.3/Relu&#39;, &#39;/last_layer/Gemm&#39;]
    True
    Sanity checks passed. Starting autoconvert.
    Running attempt 1 excluding conversion of 0 nodes
    []
    True
    Attempt succeeded.
    [*21*]
    Done: []
    []
    Final model validated successfully.
    [32mINFO    [0m [34mQuantization complete. Model is now quantized using automatic mixed precision.[0m
    I0329 13:55:35.887633 139783521261376 quantize.py:164] Quantization complete. Model is now quantized using automatic mixed precision.
    [32mINFO    [0m [34mPerforming runtime analysis on original graph...[0m
    I0329 13:55:35.906447 139783521261376 transform.py:257] Performing runtime analysis on original graph...
    [32mINFO    [0m [34mStarting transformation analysis on vgg7[0m
    I0329 13:55:35.906747 139783521261376 runtime_analysis.py:357] Starting transformation analysis on vgg7
    [32mINFO    [0m [34m
    Results vgg7:
    +------------------------------+-----------+
    |      Metric (Per Batch)      |   Value   |
    +------------------------------+-----------+
    |    Average Test Accuracy     |  0.91831  |
    |      Average Precision       |  0.91791  |
    |        Average Recall        |  0.91793  |
    |       Average F1 Score       |  0.91778  |
    |         Average Loss         |  0.24676  |
    |       Average Latency        | 8.5586 ms |
    |   Average GPU Power Usage    | 52.579 W  |
    | Inference Energy Consumption | 0.125 mWh |
    +------------------------------+-----------+[0m
    I0329 13:55:58.685605 139783521261376 runtime_analysis.py:521]
    Results vgg7:
    +------------------------------+-----------+
    |      Metric (Per Batch)      |   Value   |
    +------------------------------+-----------+
    |    Average Test Accuracy     |  0.91831  |
    |      Average Precision       |  0.91791  |
    |        Average Recall        |  0.91793  |
    |       Average F1 Score       |  0.91778  |
    |         Average Loss         |  0.24676  |
    |       Average Latency        | 8.5586 ms |
    |   Average GPU Power Usage    | 52.579 W  |
    | Inference Energy Consumption | 0.125 mWh |
    +------------------------------+-----------+
    [32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/mase_graph/version_0/model.json[0m
    I0329 13:55:58.687570 139783521261376 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/mase_graph/version_0/model.json
    [32mINFO    [0m [34mPerforming runtime analysis on onnx-optimized graph...[0m
    I0329 13:55:58.687825 139783521261376 transform.py:263] Performing runtime analysis on onnx-optimized graph...
    [32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
    I0329 13:55:58.688159 139783521261376 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
    [32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
    I0329 13:55:58.822749 139783521261376 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
    [32mINFO    [0m [34m
    Results vgg7-onnx:
    +------------------------------+--------------+
    |      Metric (Per Batch)      |    Value     |
    +------------------------------+--------------+
    |    Average Test Accuracy     |   0.93054    |
    |      Average Precision       |   0.93154    |
    |        Average Recall        |   0.93158    |
    |       Average F1 Score       |   0.93138    |
    |         Average Loss         |   0.22327    |
    |       Average Latency        |  6.0628 ms   |
    |   Average GPU Power Usage    |   53.26 W    |
    | Inference Energy Consumption | 0.089695 mWh |
    +------------------------------+--------------+[0m
    I0329 13:56:20.459139 139783521261376 runtime_analysis.py:521]
    Results vgg7-onnx:
    +------------------------------+--------------+
    |      Metric (Per Batch)      |    Value     |
    +------------------------------+--------------+
    |    Average Test Accuracy     |   0.93054    |
    |      Average Precision       |   0.93154    |
    |        Average Recall        |   0.93158    |
    |       Average F1 Score       |   0.93138    |
    |         Average Loss         |   0.22327    |
    |       Average Latency        |  6.0628 ms   |
    |   Average GPU Power Usage    |   53.26 W    |
    | Inference Energy Consumption | 0.089695 mWh |
    +------------------------------+--------------+
    [32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_0/model.json[0m
    I0329 13:56:20.461472 139783521261376 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_0/model.json
    [32mINFO    [0m [34mPerforming runtime analysis on static quantized graph...[0m
    I0329 13:56:20.475034 139783521261376 transform.py:282] Performing runtime analysis on static quantized graph...
    [32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
    I0329 13:56:20.475347 139783521261376 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
    [0;93m2024-03-29 13:56:20.543785227 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 9 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.[m
    [0;93m2024-03-29 13:56:20.544404009 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.[m
    [0;93m2024-03-29 13:56:20.544417333 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.[m
    [32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
    I0329 13:56:20.554211 139783521261376 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
    [32mINFO    [0m [34m
    Results vgg7-onnx:
    +------------------------------+-------------+
    |      Metric (Per Batch)      |    Value    |
    +------------------------------+-------------+
    |    Average Test Accuracy     |   0.9316    |
    |      Average Precision       |   0.93244   |
    |        Average Recall        |   0.93224   |
    |       Average F1 Score       |   0.93212   |
    |         Average Loss         |   0.22435   |
    |       Average Latency        |  3.3334 ms  |
    |   Average GPU Power Usage    |  58.211 W   |
    | Inference Energy Consumption | 0.14364 mWh |
    +------------------------------+-------------+[0m
    I0329 13:56:42.742136 139783521261376 runtime_analysis.py:521]
    Results vgg7-onnx:
    +------------------------------+-------------+
    |      Metric (Per Batch)      |    Value    |
    +------------------------------+-------------+
    |    Average Test Accuracy     |   0.9316    |
    |      Average Precision       |   0.93244   |
    |        Average Recall        |   0.93224   |
    |       Average F1 Score       |   0.93212   |
    |         Average Loss         |   0.22435   |
    |       Average Latency        |  3.3334 ms  |
    |   Average GPU Power Usage    |  58.211 W   |
    | Inference Energy Consumption | 0.14364 mWh |
    +------------------------------+-------------+
    [32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_1/model.json[0m
    I0329 13:56:42.744704 139783521261376 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_1/model.json
    [32mINFO    [0m [34mPerforming runtime analysis on dynamic quantized graph...[0m
    I0329 13:56:42.759592 139783521261376 transform.py:290] Performing runtime analysis on dynamic quantized graph...
    [32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
    I0329 13:56:42.760658 139783521261376 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
    [0;93m2024-03-29 13:56:42.846650769 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 26 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.[m
    [0;93m2024-03-29 13:56:42.847548047 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.[m
    [0;93m2024-03-29 13:56:42.847563259 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.[m
    [32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
    I0329 13:56:42.863932 139783521261376 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
    [32mINFO    [0m [34m
    Results vgg7-onnx:
    +------------------------------+------------+
    |      Metric (Per Batch)      |   Value    |
    +------------------------------+------------+
    |    Average Test Accuracy     |  0.93162   |
    |      Average Precision       |  0.93261   |
    |        Average Recall        |  0.93257   |
    |       Average F1 Score       |  0.93241   |
    |         Average Loss         |  0.22253   |
    |       Average Latency        |  5.2453 ms |
    |   Average GPU Power Usage    |  22.924 W  |
    | Inference Energy Consumption | 0.0742 mWh |
    +------------------------------+------------+[0m
    I0329 13:59:46.169317 139783521261376 runtime_analysis.py:521]
    Results vgg7-onnx:
    +------------------------------+------------+
    |      Metric (Per Batch)      |   Value    |
    +------------------------------+------------+
    |    Average Test Accuracy     |  0.93162   |
    |      Average Precision       |  0.93261   |
    |        Average Recall        |  0.93257   |
    |       Average F1 Score       |  0.93241   |
    |         Average Loss         |  0.22253   |
    |       Average Latency        |  5.2453 ms |
    |   Average GPU Power Usage    |  22.924 W  |
    | Inference Energy Consumption | 0.0742 mWh |
    +------------------------------+------------+
    [32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_2/model.json[0m
    I0329 13:59:46.174946 139783521261376 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_2/model.json
    [32mINFO    [0m [34mPerforming runtime analysis on auto mixed precision quantized graph...[0m
    I0329 13:59:46.208538 139783521261376 transform.py:298] Performing runtime analysis on auto mixed precision quantized graph...
    [32mINFO    [0m [34mUsing [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.[0m
    I0329 13:59:46.209225 139783521261376 runtime_analysis.py:108] Using [&#39;CUDAExecutionProvider&#39;] as ONNX execution provider.
    [32mINFO    [0m [34mStarting transformation analysis on vgg7-onnx[0m
    I0329 13:59:46.289226 139783521261376 runtime_analysis.py:357] Starting transformation analysis on vgg7-onnx
    [32mINFO    [0m [34m
    Results vgg7-onnx:
    +------------------------------+--------------+
    |      Metric (Per Batch)      |    Value     |
    +------------------------------+--------------+
    |    Average Test Accuracy     |   0.93087    |
    |      Average Precision       |   0.93188    |
    |        Average Recall        |   0.93191    |
    |       Average F1 Score       |   0.93172    |
    |         Average Loss         |   0.22324    |
    |       Average Latency        |  4.9185 ms   |
    |   Average GPU Power Usage    |   49.313 W   |
    | Inference Energy Consumption | 0.067374 mWh |
    +------------------------------+--------------+[0m
    I0329 14:00:07.487649 139783521261376 runtime_analysis.py:521]
    Results vgg7-onnx:
    +------------------------------+--------------+
    |      Metric (Per Batch)      |    Value     |
    +------------------------------+--------------+
    |    Average Test Accuracy     |   0.93087    |
    |      Average Precision       |   0.93188    |
    |        Average Recall        |   0.93191    |
    |       Average F1 Score       |   0.93172    |
    |         Average Loss         |   0.22324    |
    |       Average Latency        |  4.9185 ms   |
    |   Average GPU Power Usage    |   49.313 W   |
    | Inference Energy Consumption | 0.067374 mWh |
    +------------------------------+--------------+
    [32mINFO    [0m [34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_3/model.json[0m
    I0329 14:00:07.489081 139783521261376 runtime_analysis.py:143] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/vgg7_cls_cifar10_2024-03-29/onnx/version_3/model.json
    [32mINFO    [0m [34mSaved mase graph to /root/mase/mase_output/vgg7_cls_cifar10_2024-03-29/software/transform/transformed_ckpt[0m
    I0329 14:06:04.342311 139783521261376 save_and_load.py:147] Saved mase graph to /root/mase/mase_output/vgg7_cls_cifar10_2024-03-29/software/transform/transformed_ckpt
    [32mINFO    [0m [34mTransformation is completed[0m
    I0329 14:06:04.342653 139783521261376 cli.py:388] Transformation is completed
</pre></div>
</div>
<p>As we can see, the optimized onnx model still outperforms Pytorch on the VGG model due to it’s runtime optimizations. The static performs the best, then the automatic mixed precision which outperforms the dynamic quantization due to its requirement of calculating activations on-the-fly.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="tensorRT_quantization_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Advanced: TensorRT Quantization Tutorial</p>
      </div>
    </a>
    <a class="right-next"
       href="cli.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Advanced: Using Mase CLI</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-1-onnx-runtime-optimizations">Section 1. ONNX Runtime Optimizations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-2-quantization">Section 2. Quantization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DeepWok
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, DeepWok.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>