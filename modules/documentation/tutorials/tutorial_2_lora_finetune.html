
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter &#8212; MASE 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-data-viewer/jsonview.bundle.css?v=f6ef2277" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/libs/html/datatables.min.css?v=4b4fd840" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_links.css?v=2150a916" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_style.css?v=92936fa5" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/needstable.css?v=5e1b6797" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_toggle.css?v=5c6620df" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_core.css?v=f5b60a78" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/modern.css?v=803738c0" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../_static/documentation_options.js?v=e645c8fa"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/sphinx-data-viewer/jsonview.bundle.js?v=18cd53c5"></script>
    <script src="../../../_static/sphinx-data-viewer/jsonview_loader.js?v=f7ff7e7d"></script>
    <script src="../../../_static/sphinx-needs/libs/html/datatables.min.js?v=8a4aee21"></script>
    <script src="../../../_static/sphinx-needs/libs/html/datatables_loader.js?v=a2cae175"></script>
    <script src="../../../_static/sphinx-needs/libs/html/sphinx_needs_collapse.js?v=dca66431"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/documentation/tutorials/tutorial_2_lora_finetune';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Tutorial 3: Running Quantization-Aware Training (QAT) on Bert" href="tutorial_3_qat.html" />
    <link rel="prev" title="Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes" href="tutorial_1_introduction_to_mase.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">MASE 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../installation.html">Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/Get-started-using-uv.html">Getting Started using uv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/Get-started-using-Docker.html">Getting Started using Docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/Get-started-students.html">Additional Instructions for Imperial College Students</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../tutorials.html">Tutorials</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_1_introduction_to_mase.html">Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_3_qat.html">Tutorial 3: Running Quantization-Aware Training (QAT) on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_4_pruning.html">Tutorial 4: Unstructured Pruning on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_5_nas_optuna.html">Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6_mixed_precision_search.html">Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_7_distributed_deployment.html">Tutorial 7: Deploying a Model for Inference on Distributed Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9_kernel_fusion.html">Tutorial 9: Running Kernel Fusion for Inference Acceleration on GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced/tensorRT_quantization_tutorial.html">Advanced: TensorRT Quantization Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced/onnxrt_quantization_tutorial.html">Advanced: ONNX Runtime Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced/cli.html">Advanced: Using Mase CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer/Add-model-to-machop.html">Developer: Guide on how to add a new model into Machop</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer/doc_writing.html">Developer: How to write documentations in MASE</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer/how_to_extend_search.html">Developer: How to extend search</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../health.html">Repository Health</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../specifications.html">Coding Style Specifications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../specifications/C-coding-style-specifications.html">C/C++ Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../specifications/Python-coding-style-specifications.html">Python Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../specifications/Verilog-coding-style-specifications.html">Verilog Coding Style Specifications</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machop API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machop.html">Machop Documentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chop/actions.html">chop.actions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/datasets.html">chop.datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/distributed.html">chop.distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/ir.html">chop.ir</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/models.html">chop.models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/nn.html">chop.nn</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../chop/nn_quantized.html">chop.nn.quantized</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chop/nn_quantized_functional.html">chop.nn.quantized.functional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chop/nn_quantized_modules.html">chop.nn.quantized.modules</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../chop/passes.html">chop.passes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../chop/passes_module.html">chop.passes.module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../chop/module_analysis/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/module_transform/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/onn.html">chop.passes.module.transforms.onn</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../chop/passes_graph.html">chop.passes.graph</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/add_metadata.html">chop.passes.graph.analysis.add_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/autosharding.html">chop.passes.graph.analysis.autosharding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/init_metadata.html">chop.passes.graph.analysis.init_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/report.html">chop.passes.graph.analysis.report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/statistical_profiler.html">chop.passes.graph.analysis.statistical_profiler.profile_statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/verify.html">chop.passes.graph.analysis.verify.verify</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/quantization.html">chop.passes.graph.calculate_avg_bits_mg_analysis_pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/pruning.html">chop.passes.graph.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/runtime.html">chop.passes.graph.analysis.runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/pruning.html">chop.passes.transform.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/quantize.html">chop.passes.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/utils.html">chop.passes.transform.utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/tensorrt.html">chop.passes.transform.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/interface/save_and_load.html">chop.passes.interface.save_and_load</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/interface/tensorrt.html">chop.passes.interface.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/interface/onnxrt.html">chop.passes.interface.onnxrt</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/pipelines.html">chop.pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/tools.html">chop.tools</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../adls_2024.html">Advanced Deep Learning Systems: 2024/2025</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_0_introduction.html">Lab 0: Introduction to Mase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_1_compression.html">Lab 1: Model Compression (Quantization and Pruning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_2_nas.html">Lab 2: Neural Architecture Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_3_mixed_precision_search.html">Lab 3: Mixed Precision Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab4-software.html">Lab 4 (Software Stream) Performance Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../adls_2023.html">Advanced Deep Learning Systems: 2023/2024</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab1.html">Lab 1 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab2.html">Lab 2 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab3.html">Lab 3 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab4-software.html">Lab 4 (Software Stream) for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/modules/documentation/tutorials/tutorial_2_lora_finetune.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis-with-the-imdb-dataset">Sentiment Analysis with the IMDb Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-a-masegraph-with-custom-arguments">Generate a MaseGraph with Custom Arguments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#full-supervised-finetuning-sft">Full Supervised Finetuning (SFT)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-finetuning-peft-with-lora">Parameter Efficient Finetuning (PEFT) with LoRA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tutorial-2-finetuning-bert-for-sequence-classification-using-a-lora-adapter">
<h1>Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter<a class="headerlink" href="#tutorial-2-finetuning-bert-for-sequence-classification-using-a-lora-adapter" title="Link to this heading">#</a></h1>
<p>When we import a pretrained transformer model from HuggingFace, we receive the encoder/decoder weights, which aren’t that useful on their own - to perform a useful task such as sequence classification, we add a classification head on top of the model and train those weights on the required dataset. In this tutorial, we’ll look at fine tuning a Bert model for sequence classification with two approaches. First, we’ll attempt full Supervised Fine Tuning (SFT). Then, we’ll use the Mase stack to add a <a class="reference external" href="https://arxiv.org/abs/2106.09685">LoRA</a> adapter to the model. We’ll look at the effect in memory requirement for training and the achieved accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;DeepWokLab/bert-tiny&quot;</span>
<span class="n">tokenizer_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;DeepWokLab/bert-tiny&quot;</span>
<span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;imdb&quot;</span>
</pre></div>
</div>
</div>
</div>
<section id="sentiment-analysis-with-the-imdb-dataset">
<h2>Sentiment Analysis with the IMDb Dataset<a class="headerlink" href="#sentiment-analysis-with-the-imdb-dataset" title="Link to this heading">#</a></h2>
<p>The IMDB dataset, introduced in <a class="reference external" href="https://aclanthology.org/P11-1015/">this 2011 paper</a> from Stanford, is commonly used for sentiment analysis in the Natural Language Processing (NLP) community. This is a collection of 50k movie reviews from the IMDb website, labelled as either “positive” or “negative”. Here is an example of a positive review:</p>
<blockquote>
<div><p>I turned over to this film in the middle of the night and very nearly skipped right passed it. It was only because there was nothing else on that I decided to watch it. In the end, I thought it was great.<br /><br />An interesting storyline, good characters, a clever script and brilliant directing makes this a fine film to sit down and watch. This was, in fact, the first I’d heard of this movie, but I would have been happy to have paid money to see this at the cinema.<br /><br />My IMDB Rating : 8 out of 10<br /><br /></p>
</div></blockquote>
<p>And a negative review:</p>
<blockquote>
<div><p>its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don’t make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn’t leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don’t make sense as they are added in and seem to have little relevance to the history of van dam’s character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality</p>
</div></blockquote>
<p>The dataset is available from HuggingFace through the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> library. We use the <code class="docutils literal notranslate"><span class="pre">get_tokenized_dataset</span></code> utility in Mase to automatically tokenize it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_tokenized_dataset</span>

<span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenized_dataset</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_name</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="o">=</span><span class="n">tokenizer_checkpoint</span><span class="p">,</span>
    <span class="n">return_tokenizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Tokenizing dataset imdb with AutoTokenizer for DeepWokLab/bert-tiny.</span>
Map: 100%|██████████| 25000/25000 [00:03&lt;00:00, 6535.11 examples/s]
</pre></div>
</div>
</div>
</div>
</section>
<section id="generate-a-masegraph-with-custom-arguments">
<h2>Generate a MaseGraph with Custom Arguments<a class="headerlink" href="#generate-a-masegraph-with-custom-arguments" title="Link to this heading">#</a></h2>
<p>By inspecting the implementation of the Bert model in HuggingFace, we can see the forward function has a signature similar to the following.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">class</span><span class="w"> </span><span class="nc">BertForSequenceClassification</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="o">...</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">SequenceClassifierOutput</span><span class="p">]:</span>
            <span class="o">...</span>
</pre></div>
</div>
<p>By default, the MaseGraph constructor chooses to use the <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> argument, ignoring the other optional arguments. However, you can specify which inputs to drive during symbolic tracing using the <code class="docutils literal notranslate"><span class="pre">hf_input_names</span></code> argument. In the following cell, we also drive the <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">labels</span></code> inputs. By specifying the <code class="docutils literal notranslate"><span class="pre">labels</span></code> argument, we include a <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> module at the end of the model to calculate the loss directly.</p>
<blockquote>
<div><p><strong>Task:</strong> Remove the <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">labels</span></code> arguments from the <code class="docutils literal notranslate"><span class="pre">hf_input_names</span></code> list and re-run the following cell. Use <code class="docutils literal notranslate"><span class="pre">mg.draw()</span></code> to visualize the graph in each case. Can you see any changes in the graph topology? Can you explain why this happens?</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">chop</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaseGraph</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">chop.passes</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">passes</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;single_label_classification&quot;</span>

<span class="n">mg</span> <span class="o">=</span> <span class="n">MaseGraph</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">hf_input_names</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
        <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span>
        <span class="s2">&quot;labels&quot;</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>

<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">passes</span><span class="o">.</span><span class="n">init_metadata_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">)</span>
<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">passes</span><span class="o">.</span><span class="n">add_common_metadata_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepWokLab/bert-tiny and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Getting dummy input for DeepWokLab/bert-tiny.</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],
        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],
        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],


        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])
tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],


        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]])
tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])
tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],


        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])
tensor([[[ 0.7973,  0.0109, -8.8405,  ...,  1.4170,  0.1046, -0.1551],
         [-1.1766,  1.2879, -1.0986,  ...,  0.4749, -0.5899,  0.8746],
         [-2.0560,  0.7748, -0.8909,  ..., -0.4034,  0.5352, -1.3657],
         ...,
         [ 0.2317, -0.7896,  0.9634,  ..., -0.8037,  0.4834, -0.5868],
         [ 0.0243, -1.0235, -1.2771,  ..., -2.2378,  1.8530,  0.1558],
         [-1.3637,  0.7055, -0.2177,  ...,  0.3557, -0.3971, -0.3107]],

        [[ 0.7973,  0.0109, -8.8405,  ...,  1.4170,  0.1046, -0.1551],
         [-2.6940,  0.6198, -0.4564,  ..., -1.4367, -1.5705, -3.1260],
         [-1.7524,  0.8535, -0.2155,  ..., -0.5222, -1.2430, -1.7199],
         ...,
         [-0.0347,  0.7446,  1.4462,  ..., -1.1578, -2.6197,  0.2612],
         [ 2.4334, -0.3068,  0.8250,  ...,  0.1475,  0.1790,  2.2907],
         [-1.3637,  0.7055, -0.2177,  ...,  0.3557, -0.3971, -0.3107]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
tensor([[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,
           1.8900e-01,  2.8282e-01],
         [-3.5020e-02, -6.1047e-02, -1.0465e-01,  ..., -8.1892e-01,
           1.1978e+00,  2.1808e+00],
         [ 4.3982e-01, -1.9602e+00, -6.8830e-01,  ..., -6.3025e-01,
          -1.5967e-01,  1.3284e+00],
         ...,
         [ 1.4783e+00,  1.0907e-01, -1.5222e+00,  ..., -3.0983e-01,
          -1.2971e-01,  1.1265e+00],
         [ 1.5890e+00, -1.6859e+00,  7.8703e-01,  ..., -1.3174e+00,
           2.2258e-01,  8.8157e-01],
         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,
           7.2677e-02,  1.1724e-01]],

        [[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,
           1.8900e-01,  2.8282e-01],
         [-4.4781e-01, -7.9224e-01, -2.1741e+00,  ..., -5.9181e-01,
           1.4373e+00,  2.4267e+00],
         [-2.5942e-01,  9.7163e-01, -3.2928e+00,  ..., -5.9773e-01,
          -3.0482e-01,  1.4038e+00],
         ...,
         [ 5.1575e-02,  3.5218e-01, -3.8926e-01,  ..., -1.1508e+00,
           7.5490e-01,  8.2911e-01],
         [ 1.6107e+00,  6.8170e-02,  9.2537e-01,  ..., -1.5233e+00,
          -6.0733e-01,  3.3097e-01],
         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,
           7.2677e-02,  1.1724e-01]]], grad_fn=&lt;ViewBackward0&gt;)
tensor([[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,
           1.8900e-01,  2.8282e-01],
         [-3.5020e-02, -6.1047e-02, -1.0465e-01,  ..., -8.1892e-01,
           1.1978e+00,  2.1808e+00],
         [ 4.3982e-01, -1.9602e+00, -6.8830e-01,  ..., -6.3025e-01,
          -1.5967e-01,  1.3284e+00],
         ...,
         [ 1.4783e+00,  1.0907e-01, -1.5222e+00,  ..., -3.0983e-01,
          -1.2971e-01,  1.1265e+00],
         [ 1.5890e+00, -1.6859e+00,  7.8703e-01,  ..., -1.3174e+00,
           2.2258e-01,  8.8157e-01],
         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,
           7.2677e-02,  1.1724e-01]],

        [[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ...,  1.4757e-01,
           1.8900e-01,  2.8282e-01],
         [-4.4781e-01, -7.9224e-01, -2.1741e+00,  ..., -5.9181e-01,
           1.4373e+00,  2.4267e+00],
         [-2.5942e-01,  9.7163e-01, -3.2928e+00,  ..., -5.9773e-01,
          -3.0482e-01,  1.4038e+00],
         ...,
         [ 5.1575e-02,  3.5218e-01, -3.8926e-01,  ..., -1.1508e+00,
           7.5490e-01,  8.2911e-01],
         [ 1.6107e+00,  6.8170e-02,  9.2537e-01,  ..., -1.5233e+00,
          -6.0733e-01,  3.3097e-01],
         [-3.7517e-01,  1.5191e+00, -2.6796e-01,  ..., -1.6159e+00,
           7.2677e-02,  1.1724e-01]]], grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ..., -5.2614e-01,
           -3.5687e-01, -2.6793e-01],
          [ 2.0335e-01, -5.4534e-01,  3.0686e-01,  ...,  1.4757e-01,
            1.8900e-01,  2.8282e-01]],

         [[-3.5020e-02, -6.1047e-02, -1.0465e-01,  ..., -2.0138e+00,
            4.5529e-01, -7.8171e-01],
          [ 1.1969e+00,  1.6337e+00,  2.5047e-01,  ..., -8.1892e-01,
            1.1978e+00,  2.1808e+00]],

         [[ 4.3982e-01, -1.9602e+00, -6.8830e-01,  ..., -2.2501e-01,
            7.2290e-02, -1.8290e+00],
          [ 8.9952e-01,  1.0029e+00,  7.4536e-04,  ..., -6.3025e-01,
           -1.5967e-01,  1.3284e+00]],

         ...,

         [[ 1.4783e+00,  1.0907e-01, -1.5222e+00,  ...,  1.1867e+00,
           -1.3561e+00,  6.5158e-01],
          [ 9.5466e-01,  4.5887e-01,  7.8078e-01,  ..., -3.0983e-01,
           -1.2971e-01,  1.1265e+00]],

         [[ 1.5890e+00, -1.6859e+00,  7.8703e-01,  ...,  6.5467e-01,
           -6.8451e-01,  6.5081e-01],
          [ 7.0729e-01,  1.4499e+00, -1.5089e-01,  ..., -1.3174e+00,
            2.2258e-01,  8.8157e-01]],

         [[-3.7517e-01,  1.5191e+00, -2.6796e-01,  ...,  3.3130e-01,
           -3.2756e-01, -6.3130e-01],
          [ 8.6773e-01,  2.0996e-01, -3.4332e-01,  ..., -1.6159e+00,
            7.2677e-02,  1.1724e-01]]],


        [[[ 9.7740e-01,  2.5482e-03, -5.2921e-01,  ..., -5.2614e-01,
           -3.5687e-01, -2.6793e-01],
          [ 2.0335e-01, -5.4534e-01,  3.0686e-01,  ...,  1.4757e-01,
            1.8900e-01,  2.8282e-01]],

         [[-4.4781e-01, -7.9224e-01, -2.1741e+00,  ...,  1.7508e+00,
           -3.6708e-01, -1.3251e+00],
          [ 7.9208e-01, -1.3537e-01,  2.3756e-01,  ..., -5.9181e-01,
            1.4373e+00,  2.4267e+00]],

         [[-2.5942e-01,  9.7163e-01, -3.2928e+00,  ..., -9.6646e-01,
           -4.8876e-01, -1.4426e+00],
          [ 1.0250e+00, -6.9093e-01, -1.2734e+00,  ..., -5.9773e-01,
           -3.0482e-01,  1.4038e+00]],

         ...,

         [[ 5.1575e-02,  3.5218e-01, -3.8926e-01,  ..., -1.2252e-02,
            1.0394e+00,  4.2402e-01],
          [-4.7386e-01,  2.6401e+00,  1.7024e+00,  ..., -1.1508e+00,
            7.5490e-01,  8.2911e-01]],

         [[ 1.6107e+00,  6.8170e-02,  9.2537e-01,  ..., -6.1665e-01,
            2.7627e-01, -1.2083e+00],
          [ 9.3395e-01, -9.7541e-01, -2.5442e-02,  ..., -1.5233e+00,
           -6.0733e-01,  3.3097e-01]],

         [[-3.7517e-01,  1.5191e+00, -2.6796e-01,  ...,  3.3130e-01,
           -3.2756e-01, -6.3130e-01],
          [ 8.6773e-01,  2.0996e-01, -3.4332e-01,  ..., -1.6159e+00,
            7.2677e-02,  1.1724e-01]]]], grad_fn=&lt;ViewBackward0&gt;)
tensor([[[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],
         [-0.5842,  0.9588,  1.5642,  ..., -1.0731, -0.7330,  0.3132],
         [-0.8601, -1.3756,  0.5042,  ..., -0.0476,  0.2650,  1.2150],
         ...,
         [ 0.0520,  1.1719, -1.5471,  ..., -0.7894,  0.1419,  1.6964],
         [ 0.7654, -1.5053, -0.4142,  ..., -1.4622, -0.8975,  1.4576],
         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]],

        [[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],
         [-1.3806,  0.2626, -0.5207,  ..., -1.6714, -0.0554,  1.0225],
         [-1.7116,  1.8788, -2.5695,  ..., -0.6958,  0.5728,  0.5461],
         ...,
         [-1.3246,  1.2196, -0.3034,  ..., -1.1955, -0.6708,  0.5128],
         [ 0.9854,  0.8260,  0.2892,  ..., -0.6428,  0.3637,  0.4339],
         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],
         [-0.5842,  0.9588,  1.5642,  ..., -1.0731, -0.7330,  0.3132],
         [-0.8601, -1.3756,  0.5042,  ..., -0.0476,  0.2650,  1.2150],
         ...,
         [ 0.0520,  1.1719, -1.5471,  ..., -0.7894,  0.1419,  1.6964],
         [ 0.7654, -1.5053, -0.4142,  ..., -1.4622, -0.8975,  1.4576],
         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]],

        [[-0.1709,  0.5230, -0.8713,  ..., -1.3382,  0.5892,  0.4026],
         [-1.3806,  0.2626, -0.5207,  ..., -1.6714, -0.0554,  1.0225],
         [-1.7116,  1.8788, -2.5695,  ..., -0.6958,  0.5728,  0.5461],
         ...,
         [-1.3246,  1.2196, -0.3034,  ..., -1.1955, -0.6708,  0.5128],
         [ 0.9854,  0.8260,  0.2892,  ..., -0.6428,  0.3637,  0.4339],
         [-1.2008, -0.6008, -1.4608,  ..., -1.2105, -0.4289,  0.3827]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[-0.1709,  0.5230, -0.8713,  ...,  0.4365,  0.6238, -0.9414],
          [-1.3731,  1.1521,  0.1321,  ..., -1.3382,  0.5892,  0.4026]],

         [[-0.5842,  0.9588,  1.5642,  ..., -1.5431,  0.4999, -1.1350],
          [ 0.9615,  0.8694,  0.0998,  ..., -1.0731, -0.7330,  0.3132]],

         [[-0.8601, -1.3756,  0.5042,  ...,  0.9764, -0.8321, -1.0204],
          [ 1.5175,  1.1454,  0.7791,  ..., -0.0476,  0.2650,  1.2150]],

         ...,

         [[ 0.0520,  1.1719, -1.5471,  ...,  1.9402, -1.1294,  0.4793],
          [ 1.0053,  0.8099,  1.6415,  ..., -0.7894,  0.1419,  1.6964]],

         [[ 0.7654, -1.5053, -0.4142,  ...,  1.7455, -0.7326,  1.5248],
          [ 1.0806,  1.1457,  2.2163,  ..., -1.4622, -0.8975,  1.4576]],

         [[-1.2008, -0.6008, -1.4608,  ...,  2.0905,  1.8849, -1.5708],
          [ 1.9999,  0.3493, -0.8524,  ..., -1.2105, -0.4289,  0.3827]]],


        [[[-0.1709,  0.5230, -0.8713,  ...,  0.4365,  0.6238, -0.9414],
          [-1.3731,  1.1521,  0.1321,  ..., -1.3382,  0.5892,  0.4026]],

         [[-1.3806,  0.2626, -0.5207,  ...,  1.6517, -0.2316, -1.3171],
          [ 0.6812, -0.0090,  0.3803,  ..., -1.6714, -0.0554,  1.0225]],

         [[-1.7116,  1.8788, -2.5695,  ...,  0.4927, -0.4850, -1.0645],
          [ 1.2646,  1.6481,  0.9055,  ..., -0.6958,  0.5728,  0.5461]],

         ...,

         [[-1.3246,  1.2196, -0.3034,  ...,  1.2747,  1.2353,  0.2825],
          [ 1.5373,  0.8648,  0.6062,  ..., -1.1955, -0.6708,  0.5128]],

         [[ 0.9854,  0.8260,  0.2892,  ...,  1.3848, -0.0103, -1.0700],
          [ 1.3827,  2.9809,  0.0276,  ..., -0.6428,  0.3637,  0.4339]],

         [[-1.2008, -0.6008, -1.4608,  ...,  2.0905,  1.8849, -1.5708],
          [ 1.9999,  0.3493, -0.8524,  ..., -1.2105, -0.4289,  0.3827]]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],
         [-1.1465, -1.5578, -0.6984,  ...,  1.0310,  0.4824, -0.2291],
         [-1.0361, -1.8192, -2.3055,  ...,  1.5286, -1.5941,  1.1762],
         ...,
         [-0.7992,  0.0886,  0.4887,  ..., -1.7941,  0.4835,  1.3780],
         [-1.4692, -0.9135, -0.2802,  ..., -0.9691,  0.3500,  1.8863],
         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]],

        [[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],
         [-0.3700, -1.9754, -0.7315,  ...,  0.2293,  0.6996,  3.1299],
         [-0.6252,  0.2879, -1.4036,  ..., -2.0560, -2.4623, -0.9584],
         ...,
         [-1.1306, -1.4343, -1.4422,  ..., -1.6115, -0.0475,  1.3975],
         [-0.9816, -1.4909, -1.0086,  ..., -0.9284,  0.5260,  1.5330],
         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],
         [-1.1465, -1.5578, -0.6984,  ...,  1.0310,  0.4824, -0.2291],
         [-1.0361, -1.8192, -2.3055,  ...,  1.5286, -1.5941,  1.1762],
         ...,
         [-0.7992,  0.0886,  0.4887,  ..., -1.7941,  0.4835,  1.3780],
         [-1.4692, -0.9135, -0.2802,  ..., -0.9691,  0.3500,  1.8863],
         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]],

        [[-0.0123,  0.5761,  0.2209,  ..., -0.1027,  1.1061, -2.5200],
         [-0.3700, -1.9754, -0.7315,  ...,  0.2293,  0.6996,  3.1299],
         [-0.6252,  0.2879, -1.4036,  ..., -2.0560, -2.4623, -0.9584],
         ...,
         [-1.1306, -1.4343, -1.4422,  ..., -1.6115, -0.0475,  1.3975],
         [-0.9816, -1.4909, -1.0086,  ..., -0.9284,  0.5260,  1.5330],
         [-0.5760, -0.0452,  0.4230,  ..., -0.7179, -0.7858,  1.6879]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[-0.0123,  0.5761,  0.2209,  ..., -0.1457, -0.7538,  0.1761],
          [-0.0705,  0.9215,  0.7990,  ..., -0.1027,  1.1061, -2.5200]],

         [[-1.1465, -1.5578, -0.6984,  ...,  0.0289, -2.1112, -0.8728],
          [ 0.6506, -1.6966,  1.4463,  ...,  1.0310,  0.4824, -0.2291]],

         [[-1.0361, -1.8192, -2.3055,  ..., -0.2195, -1.1732,  0.3182],
          [-0.5841, -0.0227,  3.0901,  ...,  1.5286, -1.5941,  1.1762]],

         ...,

         [[-0.7992,  0.0886,  0.4887,  ...,  0.7859, -1.0127, -0.2676],
          [-0.3055,  0.6270, -3.0705,  ..., -1.7941,  0.4835,  1.3780]],

         [[-1.4692, -0.9135, -0.2802,  ...,  0.1197, -0.7532,  0.0731],
          [ 0.6096, -1.0893, -0.6959,  ..., -0.9691,  0.3500,  1.8863]],

         [[-0.5760, -0.0452,  0.4230,  ...,  0.8851,  0.3078,  0.8106],
          [-1.1804,  0.9512,  0.3169,  ..., -0.7179, -0.7858,  1.6879]]],


        [[[-0.0123,  0.5761,  0.2209,  ..., -0.1457, -0.7538,  0.1761],
          [-0.0705,  0.9215,  0.7990,  ..., -0.1027,  1.1061, -2.5200]],

         [[-0.3700, -1.9754, -0.7315,  ...,  0.5756, -1.5559,  0.0326],
          [ 1.4229,  2.3970, -0.4516,  ...,  0.2293,  0.6996,  3.1299]],

         [[-0.6252,  0.2879, -1.4036,  ...,  0.5306, -0.5608,  1.1861],
          [-2.5980,  0.2673,  3.3016,  ..., -2.0560, -2.4623, -0.9584]],

         ...,

         [[-1.1306, -1.4343, -1.4422,  ...,  0.3918, -1.5336, -0.5026],
          [ 1.8587,  0.8501, -1.2402,  ..., -1.6115, -0.0475,  1.3975]],

         [[-0.9816, -1.4909, -1.0086,  ...,  0.2956,  0.0351, -1.0685],
          [-0.6594, -0.0133, -1.1863,  ..., -0.9284,  0.5260,  1.5330]],

         [[-0.5760, -0.0452,  0.4230,  ...,  0.8851,  0.3078,  0.8106],
          [-1.1804,  0.9512,  0.3169,  ..., -0.7179, -0.7858,  1.6879]]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[-0.5911, -0.4682, -0.4314,  ...,  0.0366, -1.0405, -0.0579],
          [-1.1141, -1.4785, -0.6477,  ...,  0.0631, -1.9950, -0.7912],
          [-0.7059, -0.9954, -1.3923,  ..., -0.1557, -0.9998,  0.2774],
          ...,
          [-0.6300,  0.1252,  0.3486,  ...,  0.5692, -0.9417, -0.1399],
          [-1.0593, -0.6395, -0.3343,  ...,  0.0571, -0.7842,  0.1042],
          [-0.2081,  0.2785,  0.0613,  ..., -0.0353, -0.8389, -0.0026]],

         [[-0.3266,  0.6360,  0.0214,  ...,  0.1677,  0.3883,  0.4382],
          [-0.6299,  0.6012,  0.7379,  ...,  0.2989, -0.1569,  0.9508],
          [-0.4097,  0.6374, -0.1589,  ...,  0.0258, -0.0364,  0.9990],
          ...,
          [-0.0088,  0.1378, -0.4819,  ..., -0.1261,  0.2908,  1.0980],
          [-0.5584,  0.9932, -0.1105,  ...,  0.2486,  0.4005,  0.6046],
          [-0.4516,  0.8092, -0.0513,  ...,  0.1182,  0.4294,  0.4781]]],


        [[[-0.6850, -0.2168, -0.6087,  ...,  0.1402, -0.7267, -0.1502],
          [-0.2084, -0.1325, -0.1151,  ...,  0.0938, -0.8963,  0.1296],
          [-0.3665,  0.3408, -0.6133,  ...,  0.1938, -0.6681,  0.5929],
          ...,
          [-1.0985, -1.2861, -1.2531,  ...,  0.3609, -1.4022, -0.4444],
          [-0.9787, -1.4207, -0.9908,  ...,  0.2989, -0.0159, -1.0060],
          [-0.1970,  0.2371, -0.0115,  ...,  0.0049, -0.8135,  0.1201]],

         [[ 0.0907,  0.7927, -0.0524,  ..., -0.4610, -0.4295,  0.4391],
          [-0.2352,  1.0586, -0.1117,  ..., -0.4943, -0.6546,  0.6617],
          [ 0.8269,  1.7861, -1.1207,  ..., -0.0885,  0.2791,  1.4721],
          ...,
          [-0.1038,  0.9623, -0.7062,  ..., -0.3340, -0.3050,  0.8792],
          [ 0.0507,  0.6634, -0.2642,  ..., -0.4959, -0.7944,  0.7687],
          [ 0.3364,  0.9047, -0.2037,  ..., -0.3705, -0.2893,  0.6787]]]],
       grad_fn=&lt;ScaledDotProductFlashAttentionForCpuBackward0&gt;)
tensor([[[[-0.5911, -0.4682, -0.4314,  ...,  0.0366, -1.0405, -0.0579],
          [-0.3266,  0.6360,  0.0214,  ...,  0.1677,  0.3883,  0.4382]],

         [[-1.1141, -1.4785, -0.6477,  ...,  0.0631, -1.9950, -0.7912],
          [-0.6299,  0.6012,  0.7379,  ...,  0.2989, -0.1569,  0.9508]],

         [[-0.7059, -0.9954, -1.3923,  ..., -0.1557, -0.9998,  0.2774],
          [-0.4097,  0.6374, -0.1589,  ...,  0.0258, -0.0364,  0.9990]],

         ...,

         [[-0.6300,  0.1252,  0.3486,  ...,  0.5692, -0.9417, -0.1399],
          [-0.0088,  0.1378, -0.4819,  ..., -0.1261,  0.2908,  1.0980]],

         [[-1.0593, -0.6395, -0.3343,  ...,  0.0571, -0.7842,  0.1042],
          [-0.5584,  0.9932, -0.1105,  ...,  0.2486,  0.4005,  0.6046]],

         [[-0.2081,  0.2785,  0.0613,  ..., -0.0353, -0.8389, -0.0026],
          [-0.4516,  0.8092, -0.0513,  ...,  0.1182,  0.4294,  0.4781]]],


        [[[-0.6850, -0.2168, -0.6087,  ...,  0.1402, -0.7267, -0.1502],
          [ 0.0907,  0.7927, -0.0524,  ..., -0.4610, -0.4295,  0.4391]],

         [[-0.2084, -0.1325, -0.1151,  ...,  0.0938, -0.8963,  0.1296],
          [-0.2352,  1.0586, -0.1117,  ..., -0.4943, -0.6546,  0.6617]],

         [[-0.3665,  0.3408, -0.6133,  ...,  0.1938, -0.6681,  0.5929],
          [ 0.8269,  1.7861, -1.1207,  ..., -0.0885,  0.2791,  1.4721]],

         ...,

         [[-1.0985, -1.2861, -1.2531,  ...,  0.3609, -1.4022, -0.4444],
          [-0.1038,  0.9623, -0.7062,  ..., -0.3340, -0.3050,  0.8792]],

         [[-0.9787, -1.4207, -0.9908,  ...,  0.2989, -0.0159, -1.0060],
          [ 0.0507,  0.6634, -0.2642,  ..., -0.4959, -0.7944,  0.7687]],

         [[-0.1970,  0.2371, -0.0115,  ...,  0.0049, -0.8135,  0.1201],
          [ 0.3364,  0.9047, -0.2037,  ..., -0.3705, -0.2893,  0.6787]]]],
       grad_fn=&lt;TransposeBackward0&gt;)
tensor([[[-0.9552,  0.6594, -6.5403,  ..., -0.7144,  0.0906,  0.3369],
         [-2.5251,  1.3955, -0.8914,  ..., -2.1363,  0.0271,  1.1132],
         [-3.7148,  0.6796, -0.8710,  ..., -2.6492,  0.5694, -0.1085],
         ...,
         [-2.2403, -0.7594,  0.5414,  ..., -3.0426,  0.8895, -0.0546],
         [-1.6945, -0.6326, -0.8632,  ..., -4.0678,  1.7219,  0.6481],
         [-2.9625,  0.7451, -0.8037,  ..., -2.5048,  0.3125,  0.5537]],

        [[-0.5150,  0.8150, -6.5015,  ..., -0.5377, -0.4171,  0.1350],
         [-2.9979,  1.0930, -0.2619,  ..., -3.1811, -1.0048, -1.8349],
         [-2.8788,  0.5405, -0.0789,  ..., -2.3969, -0.7016, -0.7332],
         ...,
         [-1.7194,  1.5158,  1.0070,  ..., -2.8931, -2.3309,  1.1685],
         [ 0.0717, -0.1039,  0.5084,  ..., -2.1932,  0.0751,  2.8236],
         [-2.4774,  0.7563, -0.7502,  ..., -2.1312, -0.0685,  0.8700]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
tensor([[[-0.0455,  0.6529,  0.6297,  ...,  0.4139, -0.9381,  0.6769],
         [-3.1104, -3.7282, -2.3953,  ..., -0.9155, -0.8280, -1.7070],
         [-0.9324, -2.9333, -2.3249,  ..., -0.8455, -0.0326, -0.6998],
         ...,
         [-1.9000, -1.1028, -1.1281,  ..., -0.2809,  2.0206, -1.0802],
         [-1.1088, -1.0420, -2.4026,  ..., -0.4478,  0.7391, -0.0354],
         [ 0.7349,  0.6742, -2.6697,  ..., -0.5114,  1.5155,  2.0246]],

        [[-0.0799,  0.7813,  0.4918,  ...,  0.6888, -0.7680,  0.9805],
         [-2.8720, -1.0602, -2.3610,  ..., -2.1143,  0.9664, -1.1212],
         [-1.4705, -2.1384, -1.9955,  ..., -0.9722,  1.5909, -0.1668],
         ...,
         [-2.9884, -1.1566, -2.5215,  ...,  1.1460,  0.7120, -0.6320],
         [-3.3666, -0.7966, -3.3154,  ...,  0.5316,  1.7058,  2.1950],
         [ 0.6626,  0.8537, -2.7251,  ..., -0.0901,  1.5883,  2.3840]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[-0.0455,  0.6529,  0.6297,  ...,  0.4139, -0.9381,  0.6769],
         [-3.1104, -3.7282, -2.3953,  ..., -0.9155, -0.8280, -1.7070],
         [-0.9324, -2.9333, -2.3249,  ..., -0.8455, -0.0326, -0.6998],
         ...,
         [-1.9000, -1.1028, -1.1281,  ..., -0.2809,  2.0206, -1.0802],
         [-1.1088, -1.0420, -2.4026,  ..., -0.4478,  0.7391, -0.0354],
         [ 0.7349,  0.6742, -2.6697,  ..., -0.5114,  1.5155,  2.0246]],

        [[-0.0799,  0.7813,  0.4918,  ...,  0.6888, -0.7680,  0.9805],
         [-2.8720, -1.0602, -2.3610,  ..., -2.1143,  0.9664, -1.1212],
         [-1.4705, -2.1384, -1.9955,  ..., -0.9722,  1.5909, -0.1668],
         ...,
         [-2.9884, -1.1566, -2.5215,  ...,  1.1460,  0.7120, -0.6320],
         [-3.3666, -0.7966, -3.3154,  ...,  0.5316,  1.7058,  2.1950],
         [ 0.6626,  0.8537, -2.7251,  ..., -0.0901,  1.5883,  2.3840]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[-0.0455,  0.6529,  0.6297,  ...,  1.0165, -1.6055,  0.0557],
          [-0.9141, -1.5101, -0.8415,  ...,  0.4139, -0.9381,  0.6769]],

         [[-3.1104, -3.7282, -2.3953,  ..., -1.6195,  0.7426, -3.2794],
          [-1.2694,  0.3821, -0.5687,  ..., -0.9155, -0.8280, -1.7070]],

         [[-0.9324, -2.9333, -2.3249,  ..., -1.0254,  1.8158, -1.8835],
          [-1.5265, -0.3901,  0.2734,  ..., -0.8455, -0.0326, -0.6998]],

         ...,

         [[-1.9000, -1.1028, -1.1281,  ..., -1.2688, -0.0851, -2.3190],
          [-2.4374,  0.0718, -2.7276,  ..., -0.2809,  2.0206, -1.0802]],

         [[-1.1088, -1.0420, -2.4026,  ..., -1.0658,  0.1932, -1.7012],
          [-2.3622, -0.5291, -1.9931,  ..., -0.4478,  0.7391, -0.0354]],

         [[ 0.7349,  0.6742, -2.6697,  ..., -1.4630, -0.1686, -2.5682],
          [-0.1401, -0.9712, -2.3801,  ..., -0.5114,  1.5155,  2.0246]]],


        [[[-0.0799,  0.7813,  0.4918,  ...,  1.2364, -1.9500, -0.1275],
          [-0.4080, -1.5069, -0.8504,  ...,  0.6888, -0.7680,  0.9805]],

         [[-2.8720, -1.0602, -2.3610,  ..., -2.3225, -0.0351, -2.7432],
          [-0.2305, -0.5940, -1.1570,  ..., -2.1143,  0.9664, -1.1212]],

         [[-1.4705, -2.1384, -1.9955,  ..., -0.6524, -1.8025, -1.8321],
          [-1.7742, -0.6800, -0.2172,  ..., -0.9722,  1.5909, -0.1668]],

         ...,

         [[-2.9884, -1.1566, -2.5215,  ..., -0.5054, -1.0314, -3.4883],
          [-1.9535,  0.5573, -2.1564,  ...,  1.1460,  0.7120, -0.6320]],

         [[-3.3666, -0.7966, -3.3154,  ...,  0.7587, -0.6289, -3.4848],
          [-1.4099, -2.0919, -1.5870,  ...,  0.5316,  1.7058,  2.1950]],

         [[ 0.6626,  0.8537, -2.7251,  ..., -1.1831, -0.7083, -2.7717],
          [ 0.4486, -1.1639, -2.1203,  ..., -0.0901,  1.5883,  2.3840]]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[-0.8947,  0.0412, -1.2359,  ...,  0.4410, -0.3965,  0.0106],
         [-1.9196,  0.3326,  0.8482,  ..., -1.5790, -1.1817, -1.0156],
         [-2.1664,  0.3959,  0.7476,  ..., -2.1767, -0.6488,  0.1889],
         ...,
         [-1.6009,  0.4887, -0.4818,  ..., -1.1268,  0.4111,  0.7892],
         [-0.1528,  1.1728, -0.5164,  ..., -0.4340,  0.1499,  1.6704],
         [ 1.0253,  1.4222, -0.1805,  ..., -0.6130, -0.5380,  1.6164]],

        [[-0.6736, -0.0718, -1.1724,  ...,  0.2001, -0.5481,  0.0232],
         [-1.8698, -1.2184,  0.2913,  ..., -1.1398, -1.3523, -0.7851],
         [-1.3725, -0.8212,  0.1984,  ..., -1.8218, -1.4800, -0.2956],
         ...,
         [-0.5946,  0.5680,  0.8938,  ..., -1.6653,  0.8218,  1.1902],
         [ 1.2800,  1.9566,  0.2540,  ..., -1.2290,  0.5257,  1.2667],
         [ 1.3511,  1.3329, -0.0782,  ..., -0.8454, -0.7400,  1.5966]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[-0.8947,  0.0412, -1.2359,  ...,  0.4410, -0.3965,  0.0106],
         [-1.9196,  0.3326,  0.8482,  ..., -1.5790, -1.1817, -1.0156],
         [-2.1664,  0.3959,  0.7476,  ..., -2.1767, -0.6488,  0.1889],
         ...,
         [-1.6009,  0.4887, -0.4818,  ..., -1.1268,  0.4111,  0.7892],
         [-0.1528,  1.1728, -0.5164,  ..., -0.4340,  0.1499,  1.6704],
         [ 1.0253,  1.4222, -0.1805,  ..., -0.6130, -0.5380,  1.6164]],

        [[-0.6736, -0.0718, -1.1724,  ...,  0.2001, -0.5481,  0.0232],
         [-1.8698, -1.2184,  0.2913,  ..., -1.1398, -1.3523, -0.7851],
         [-1.3725, -0.8212,  0.1984,  ..., -1.8218, -1.4800, -0.2956],
         ...,
         [-0.5946,  0.5680,  0.8938,  ..., -1.6653,  0.8218,  1.1902],
         [ 1.2800,  1.9566,  0.2540,  ..., -1.2290,  0.5257,  1.2667],
         [ 1.3511,  1.3329, -0.0782,  ..., -0.8454, -0.7400,  1.5966]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[-0.8947,  0.0412, -1.2359,  ...,  1.5140, -1.9812, -2.5532],
          [-0.2951, -1.6086, -0.6381,  ...,  0.4410, -0.3965,  0.0106]],

         [[-1.9196,  0.3326,  0.8482,  ..., -2.3348,  1.3935,  1.1452],
          [-0.5277,  0.1234,  0.7865,  ..., -1.5790, -1.1817, -1.0156]],

         [[-2.1664,  0.3959,  0.7476,  ..., -2.0817,  0.2852,  0.8173],
          [-0.8414,  0.5154, -0.4553,  ..., -2.1767, -0.6488,  0.1889]],

         ...,

         [[-1.6009,  0.4887, -0.4818,  ..., -1.8165,  1.4764,  0.5091],
          [-0.6869,  0.4007, -1.5818,  ..., -1.1268,  0.4111,  0.7892]],

         [[-0.1528,  1.1728, -0.5164,  ..., -1.3611,  1.0621,  1.1810],
          [-0.7595, -0.1699, -1.5305,  ..., -0.4340,  0.1499,  1.6704]],

         [[ 1.0253,  1.4222, -0.1805,  ..., -0.6989,  0.4721,  2.6129],
          [-1.2381, -0.4573, -1.7561,  ..., -0.6130, -0.5380,  1.6164]]],


        [[[-0.6736, -0.0718, -1.1724,  ...,  1.4816, -1.7920, -2.5177],
          [-0.3929, -1.5120, -0.5353,  ...,  0.2001, -0.5481,  0.0232]],

         [[-1.8698, -1.2184,  0.2913,  ..., -1.5227,  1.9764,  0.6389],
          [-0.4202,  0.4572, -1.0780,  ..., -1.1398, -1.3523, -0.7851]],

         [[-1.3725, -0.8212,  0.1984,  ..., -2.1553,  1.7041,  0.7166],
          [-1.0124,  0.9351, -0.0954,  ..., -1.8218, -1.4800, -0.2956]],

         ...,

         [[-0.5946,  0.5680,  0.8938,  ..., -2.1904,  1.7986,  1.0902],
          [-1.3820,  1.0268, -1.0041,  ..., -1.6653,  0.8218,  1.1902]],

         [[ 1.2800,  1.9566,  0.2540,  ..., -1.6180,  1.6176,  2.5636],
          [-2.0592,  0.7059, -1.3359,  ..., -1.2290,  0.5257,  1.2667]],

         [[ 1.3511,  1.3329, -0.0782,  ..., -0.5836,  0.6491,  2.6554],
          [-1.3686, -0.2348, -1.7438,  ..., -0.8454, -0.7400,  1.5966]]]],
       grad_fn=&lt;ViewBackward0&gt;)
tensor([[[ 3.8307e-01,  1.8837e-02, -1.0314e+00,  ...,  6.4335e-01,
          -3.1830e-01, -1.7296e+00],
         [ 1.5897e+00,  1.3689e-01, -6.8915e-01,  ...,  1.5973e+00,
           1.1907e+00, -9.0454e-01],
         [-5.8138e-01,  6.7943e-01, -1.3203e+00,  ..., -5.3627e-01,
          -1.0456e+00, -1.6301e+00],
         ...,
         [-1.7466e-01,  3.0707e-02,  7.5225e-01,  ..., -1.3217e+00,
          -1.3415e+00, -3.8328e-01],
         [ 1.5170e-01,  5.1089e-01,  1.3993e-01,  ..., -1.6600e-01,
          -6.5011e-01,  2.1798e-02],
         [-2.4311e-01,  1.6726e+00,  1.6682e-01,  ...,  1.3441e-03,
          -1.6754e+00,  3.1771e-01]],

        [[ 4.7844e-01, -3.1772e-01, -1.0617e+00,  ...,  6.4928e-01,
          -3.2944e-01, -2.4185e+00],
         [ 5.9122e-01, -2.2648e-01,  1.7474e-01,  ..., -1.8623e+00,
          -1.1230e+00,  3.5013e-01],
         [ 1.1642e-01,  1.2460e+00,  7.7941e-02,  ...,  6.4975e-01,
          -7.3862e-01, -2.1510e+00],
         ...,
         [ 1.1291e+00, -1.3637e+00, -1.5779e+00,  ...,  1.7637e+00,
           9.1331e-01, -1.7033e+00],
         [ 1.5909e+00, -1.4922e+00,  1.0060e+00,  ...,  9.8096e-01,
           8.6736e-01, -2.3894e+00],
         [ 1.8451e-01,  1.2740e+00,  4.2857e-01,  ...,  6.2708e-01,
          -1.3601e+00, -3.9984e-01]]], grad_fn=&lt;ViewBackward0&gt;)
tensor([[[ 3.8307e-01,  1.8837e-02, -1.0314e+00,  ...,  6.4335e-01,
          -3.1830e-01, -1.7296e+00],
         [ 1.5897e+00,  1.3689e-01, -6.8915e-01,  ...,  1.5973e+00,
           1.1907e+00, -9.0454e-01],
         [-5.8138e-01,  6.7943e-01, -1.3203e+00,  ..., -5.3627e-01,
          -1.0456e+00, -1.6301e+00],
         ...,
         [-1.7466e-01,  3.0707e-02,  7.5225e-01,  ..., -1.3217e+00,
          -1.3415e+00, -3.8328e-01],
         [ 1.5170e-01,  5.1089e-01,  1.3993e-01,  ..., -1.6600e-01,
          -6.5011e-01,  2.1798e-02],
         [-2.4311e-01,  1.6726e+00,  1.6682e-01,  ...,  1.3441e-03,
          -1.6754e+00,  3.1771e-01]],

        [[ 4.7844e-01, -3.1772e-01, -1.0617e+00,  ...,  6.4928e-01,
          -3.2944e-01, -2.4185e+00],
         [ 5.9122e-01, -2.2648e-01,  1.7474e-01,  ..., -1.8623e+00,
          -1.1230e+00,  3.5013e-01],
         [ 1.1642e-01,  1.2460e+00,  7.7941e-02,  ...,  6.4975e-01,
          -7.3862e-01, -2.1510e+00],
         ...,
         [ 1.1291e+00, -1.3637e+00, -1.5779e+00,  ...,  1.7637e+00,
           9.1331e-01, -1.7033e+00],
         [ 1.5909e+00, -1.4922e+00,  1.0060e+00,  ...,  9.8096e-01,
           8.6736e-01, -2.3894e+00],
         [ 1.8451e-01,  1.2740e+00,  4.2857e-01,  ...,  6.2708e-01,
          -1.3601e+00, -3.9984e-01]]], grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[ 3.8307e-01,  1.8837e-02, -1.0314e+00,  ..., -8.0955e-01,
           -2.8557e-01, -2.3318e-01],
          [-9.9661e-02,  3.1722e-01, -3.0517e-01,  ...,  6.4335e-01,
           -3.1830e-01, -1.7296e+00]],

         [[ 1.5897e+00,  1.3689e-01, -6.8915e-01,  ..., -7.9549e-01,
           -6.9279e-01, -1.8082e-01],
          [-9.9201e-01,  9.4938e-01,  4.4198e-02,  ...,  1.5973e+00,
            1.1907e+00, -9.0454e-01]],

         [[-5.8138e-01,  6.7943e-01, -1.3203e+00,  ..., -1.8905e+00,
            1.6226e-01, -1.2953e+00],
          [-7.0312e-01, -6.4926e-01, -5.0913e-01,  ..., -5.3627e-01,
           -1.0456e+00, -1.6301e+00]],

         ...,

         [[-1.7466e-01,  3.0707e-02,  7.5225e-01,  ..., -1.9281e+00,
            1.1489e+00, -2.4530e-01],
          [-7.6225e-02,  8.5814e-01, -1.5467e+00,  ..., -1.3217e+00,
           -1.3415e+00, -3.8328e-01]],

         [[ 1.5170e-01,  5.1089e-01,  1.3993e-01,  ..., -2.4168e+00,
            3.3385e-01, -6.2115e-02],
          [-1.6390e+00, -1.6085e-01, -1.9118e+00,  ..., -1.6600e-01,
           -6.5011e-01,  2.1798e-02]],

         [[-2.4311e-01,  1.6726e+00,  1.6682e-01,  ..., -1.0481e+00,
           -2.7634e+00,  2.2741e-01],
          [-1.4603e+00,  3.1239e-02,  3.8892e-01,  ...,  1.3441e-03,
           -1.6754e+00,  3.1771e-01]]],


        [[[ 4.7844e-01, -3.1772e-01, -1.0617e+00,  ..., -7.8511e-01,
           -3.2510e-01, -2.1300e-01],
          [-3.9893e-01,  6.1469e-01, -3.9206e-01,  ...,  6.4928e-01,
           -3.2944e-01, -2.4185e+00]],

         [[ 5.9122e-01, -2.2648e-01,  1.7474e-01,  ..., -1.6488e+00,
           -8.6854e-01, -8.0783e-01],
          [-2.1516e+00, -2.4247e-01, -8.1713e-01,  ..., -1.8623e+00,
           -1.1230e+00,  3.5013e-01]],

         [[ 1.1642e-01,  1.2460e+00,  7.7941e-02,  ..., -1.3121e+00,
           -6.7044e-01, -1.1324e+00],
          [ 4.3930e-01,  3.4082e-01, -1.2243e+00,  ...,  6.4975e-01,
           -7.3862e-01, -2.1510e+00]],

         ...,

         [[ 1.1291e+00, -1.3637e+00, -1.5779e+00,  ...,  8.5980e-01,
            3.2796e-01, -1.9442e+00],
          [-8.9502e-01,  9.7357e-01,  8.5447e-01,  ...,  1.7637e+00,
            9.1331e-01, -1.7033e+00]],

         [[ 1.5909e+00, -1.4922e+00,  1.0060e+00,  ..., -1.5965e+00,
           -3.9380e-01, -4.3585e-01],
          [-2.2103e+00,  4.4127e-01,  1.1554e+00,  ...,  9.8096e-01,
            8.6736e-01, -2.3894e+00]],

         [[ 1.8451e-01,  1.2740e+00,  4.2857e-01,  ..., -1.1366e+00,
           -2.8409e+00,  4.6711e-01],
          [-1.9576e+00,  2.0176e-01, -4.1035e-02,  ...,  6.2708e-01,
           -1.3601e+00, -3.9984e-01]]]], grad_fn=&lt;ViewBackward0&gt;)
tensor([[[[ 3.7393e-01,  3.7078e-01, -7.5553e-01,  ..., -9.4280e-01,
           -6.5765e-01, -1.4711e-01],
          [ 3.9396e-01,  4.7658e-02, -1.0277e+00,  ..., -8.4926e-01,
           -2.7815e-01, -2.6627e-01],
          [ 9.9586e-01,  2.4414e-01, -8.3459e-01,  ..., -8.5795e-01,
           -4.3860e-01, -2.1582e-01],
          ...,
          [ 1.4504e+00, -1.2077e-01, -4.8160e-01,  ..., -1.6701e+00,
            6.3807e-01, -8.3788e-02],
          [ 3.8821e-01,  1.4150e-01, -1.5051e-01,  ..., -1.5785e+00,
            4.1589e-01, -1.8024e-01],
          [ 1.0467e-01,  5.5196e-01,  1.0818e-01,  ..., -2.0373e+00,
            8.4395e-02, -6.9034e-02]],

         [[-7.7230e-01, -1.1852e-01, -8.0274e-02,  ..., -1.3794e-03,
           -5.2249e-01, -4.2095e-01],
          [-4.6209e-01, -1.2680e-01, -2.5711e-01,  ...,  1.3235e-01,
           -4.1385e-01, -1.3744e+00],
          [-1.7985e-01, -2.6037e-01,  3.5678e-01,  ...,  2.4736e-01,
           -1.6626e-01, -7.0940e-01],
          ...,
          [-6.2069e-01,  2.5298e-01, -7.8416e-01,  ...,  8.6187e-02,
           -6.9561e-01, -8.5675e-01],
          [-9.1553e-01,  1.4648e-01, -5.5621e-02,  ...,  1.8643e-01,
           -1.0965e+00, -4.8097e-01],
          [-1.0678e+00,  9.0885e-02, -4.5400e-02,  ...,  1.5424e-01,
           -1.1762e+00, -2.9385e-01]]],


        [[[ 4.9445e-01,  9.6665e-03, -5.7117e-01,  ..., -9.7640e-01,
           -9.0948e-01, -1.3761e-01],
          [ 4.7672e-01, -2.3672e-01, -8.9246e-01,  ..., -8.9927e-01,
           -3.9088e-01, -3.1635e-01],
          [ 4.5120e-01, -5.8035e-02, -6.8736e-01,  ..., -1.0352e+00,
           -4.7996e-01, -4.5624e-01],
          ...,
          [ 3.3361e-01,  1.5918e-03, -1.0108e+00,  ..., -1.5704e+00,
           -3.9079e-01, -2.1742e-01],
          [ 1.0123e+00, -1.1147e+00, -1.3032e+00,  ...,  3.4082e-01,
            1.3298e-01, -1.5481e+00],
          [ 1.4273e+00, -1.2951e+00,  5.7719e-01,  ..., -1.2490e+00,
           -4.1355e-01, -5.8558e-01]],

         [[-1.0884e+00,  2.0838e-01, -5.0948e-01,  ...,  2.2845e-01,
           -8.2075e-01, -1.1496e+00],
          [-3.4296e-01,  5.0833e-01, -8.6644e-01,  ...,  3.3008e-01,
           -5.5070e-01, -1.8440e+00],
          [-3.2602e-01,  6.0671e-01, -7.5711e-01,  ...,  4.7493e-01,
           -4.6589e-01, -2.0454e+00],
          ...,
          [-1.7399e+00,  4.6359e-01,  6.5213e-01,  ...,  9.0025e-01,
            3.1643e-01, -2.0784e+00],
          [-1.8375e+00,  2.6226e-01,  3.7726e-02,  ...,  6.6286e-01,
           -1.0588e+00, -7.6781e-01],
          [-1.6402e+00,  3.1250e-01, -6.6194e-04,  ...,  6.6493e-01,
           -9.1110e-01, -1.0382e+00]]]],
       grad_fn=&lt;ScaledDotProductFlashAttentionForCpuBackward0&gt;)
tensor([[[[ 3.7393e-01,  3.7078e-01, -7.5553e-01,  ..., -9.4280e-01,
           -6.5765e-01, -1.4711e-01],
          [-7.7230e-01, -1.1852e-01, -8.0274e-02,  ..., -1.3794e-03,
           -5.2249e-01, -4.2095e-01]],

         [[ 3.9396e-01,  4.7658e-02, -1.0277e+00,  ..., -8.4926e-01,
           -2.7815e-01, -2.6627e-01],
          [-4.6209e-01, -1.2680e-01, -2.5711e-01,  ...,  1.3235e-01,
           -4.1385e-01, -1.3744e+00]],

         [[ 9.9586e-01,  2.4414e-01, -8.3459e-01,  ..., -8.5795e-01,
           -4.3860e-01, -2.1582e-01],
          [-1.7985e-01, -2.6037e-01,  3.5678e-01,  ...,  2.4736e-01,
           -1.6626e-01, -7.0940e-01]],

         ...,

         [[ 1.4504e+00, -1.2077e-01, -4.8160e-01,  ..., -1.6701e+00,
            6.3807e-01, -8.3788e-02],
          [-6.2069e-01,  2.5298e-01, -7.8416e-01,  ...,  8.6187e-02,
           -6.9561e-01, -8.5675e-01]],

         [[ 3.8821e-01,  1.4150e-01, -1.5051e-01,  ..., -1.5785e+00,
            4.1589e-01, -1.8024e-01],
          [-9.1553e-01,  1.4648e-01, -5.5621e-02,  ...,  1.8643e-01,
           -1.0965e+00, -4.8097e-01]],

         [[ 1.0467e-01,  5.5196e-01,  1.0818e-01,  ..., -2.0373e+00,
            8.4395e-02, -6.9034e-02],
          [-1.0678e+00,  9.0885e-02, -4.5400e-02,  ...,  1.5424e-01,
           -1.1762e+00, -2.9385e-01]]],


        [[[ 4.9445e-01,  9.6665e-03, -5.7117e-01,  ..., -9.7640e-01,
           -9.0948e-01, -1.3761e-01],
          [-1.0884e+00,  2.0838e-01, -5.0948e-01,  ...,  2.2845e-01,
           -8.2075e-01, -1.1496e+00]],

         [[ 4.7672e-01, -2.3672e-01, -8.9246e-01,  ..., -8.9927e-01,
           -3.9088e-01, -3.1635e-01],
          [-3.4296e-01,  5.0833e-01, -8.6644e-01,  ...,  3.3008e-01,
           -5.5070e-01, -1.8440e+00]],

         [[ 4.5120e-01, -5.8035e-02, -6.8736e-01,  ..., -1.0352e+00,
           -4.7996e-01, -4.5624e-01],
          [-3.2602e-01,  6.0671e-01, -7.5711e-01,  ...,  4.7493e-01,
           -4.6589e-01, -2.0454e+00]],

         ...,

         [[ 3.3361e-01,  1.5918e-03, -1.0108e+00,  ..., -1.5704e+00,
           -3.9079e-01, -2.1742e-01],
          [-1.7399e+00,  4.6359e-01,  6.5213e-01,  ...,  9.0025e-01,
            3.1643e-01, -2.0784e+00]],

         [[ 1.0123e+00, -1.1147e+00, -1.3032e+00,  ...,  3.4082e-01,
            1.3298e-01, -1.5481e+00],
          [-1.8375e+00,  2.6226e-01,  3.7726e-02,  ...,  6.6286e-01,
           -1.0588e+00, -7.6781e-01]],

         [[ 1.4273e+00, -1.2951e+00,  5.7719e-01,  ..., -1.2490e+00,
           -4.1355e-01, -5.8558e-01],
          [-1.6402e+00,  3.1250e-01, -6.6194e-04,  ...,  6.6493e-01,
           -9.1110e-01, -1.0382e+00]]]], grad_fn=&lt;TransposeBackward0&gt;)
tensor([[-0.3776, -0.2373],
        [-0.3180, -0.2760]], grad_fn=&lt;AddmmBackward0&gt;)
tensor([1, 0])
</pre></div>
</div>
</div>
</div>
</section>
<section id="full-supervised-finetuning-sft">
<h2>Full Supervised Finetuning (SFT)<a class="headerlink" href="#full-supervised-finetuning-sft" title="Link to this heading">#</a></h2>
<p>Before training the model, let’s inspect how many trainable parameters there are. If you’re familiar with Keras, you might have used the <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> API before, but it’s not as easy to do the same in Pytorch - luckily, Mase has a module-level pass with this functionality.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">chop.passes.module</span><span class="w"> </span><span class="kn">import</span> <span class="n">report_trainable_parameters_analysis_pass</span>

<span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">report_trainable_parameters_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-------------------------------------------------+------------------------+
| Submodule                                       |   Trainable Parameters |
+=================================================+========================+
| bert                                            |                4385920 |
+-------------------------------------------------+------------------------+
| bert.embeddings                                 |                3972864 |
+-------------------------------------------------+------------------------+
| bert.embeddings.word_embeddings                 |                3906816 |
+-------------------------------------------------+------------------------+
| bert.embeddings.token_type_embeddings           |                    256 |
+-------------------------------------------------+------------------------+
| bert.embeddings.position_embeddings             |                  65536 |
+-------------------------------------------------+------------------------+
| bert.embeddings.LayerNorm                       |                    256 |
+-------------------------------------------------+------------------------+
| bert.embeddings.dropout                         |                      0 |
+-------------------------------------------------+------------------------+
| bert.encoder                                    |                 396544 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer                              |                 396544 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0                            |                 198272 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention                  |                  66304 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self             |                  49536 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.query       |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.key         |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.value       |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output           |                  16768 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dense     |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dropout   |                      0 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.LayerNorm |                    256 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate               |                  66048 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate.dense         |                  66048 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.output                     |                  65920 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dense               |                  65664 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dropout             |                      0 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.LayerNorm           |                    256 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1                            |                 198272 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention                  |                  66304 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self             |                  49536 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.query       |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.key         |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.value       |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output           |                  16768 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dense     |                  16512 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dropout   |                      0 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.LayerNorm |                    256 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate               |                  66048 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate.dense         |                  66048 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.output                     |                  65920 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dense               |                  65664 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dropout             |                      0 |
+-------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.LayerNorm           |                    256 |
+-------------------------------------------------+------------------------+
| bert.pooler                                     |                  16512 |
+-------------------------------------------------+------------------------+
| bert.pooler.dense                               |                  16512 |
+-------------------------------------------------+------------------------+
| bert.pooler.activation                          |                      0 |
+-------------------------------------------------+------------------------+
| dropout                                         |                      0 |
+-------------------------------------------------+------------------------+
| classifier                                      |                    258 |
+-------------------------------------------------+------------------------+
| crossentropyloss_0                              |                      0 |
+-------------------------------------------------+------------------------+

Total Trainable Parameters: 14480258
</pre></div>
</div>
</div>
</div>
<p>From this, we can see the majority of the trainable parameters are in the <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> layer. We don’t need to train this, so we freeze those parameters in the cell below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">mg</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<p>To train the model, we rely on the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class from the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library, which makes it easy to set up a training loop with any hardware configuration. The <code class="docutils literal notranslate"><span class="pre">get_trainer</span></code> utility in Mase handles assigning the training arguments to the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class for common use cases, such as in this tutorial.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">get_trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">mg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenized_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">evaluate_metric</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/zz7522/Projects/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
</pre></div>
</div>
</div>
</div>
<p>Before running any fine tuning, let’s see how the model performs out of the box. Without any fine-tuning, we can see the model just performs a random guess - there are two labels in the dataset, so this corresponds to an accuracy of around 50%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate accuracy</span>
<span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation accuracy: </span><span class="si">{</span><span class="n">eval_results</span><span class="p">[</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
</pre></div>
</div>
<div class="output text_html">
    <div>
      
      <progress value='1564' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [782/782 01:29]
    </div>
    </div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation accuracy: 0.49944
</pre></div>
</div>
</div>
</div>
<p>Now, run the cell below to execute a single training epoch with the current setup.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
</pre></div>
</div>
<div class="output text_html">
    <div>
      
      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [782/782 00:34, Epoch 1/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>500</td>
      <td>0.577800</td>
    </tr>
  </tbody>
</table><p></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TrainOutput(global_step=782, training_loss=0.5414889596612252, metrics={&#39;train_runtime&#39;: 34.4035, &#39;train_samples_per_second&#39;: 726.671, &#39;train_steps_per_second&#39;: 22.73, &#39;total_flos&#39;: 0.0, &#39;train_loss&#39;: 0.5414889596612252, &#39;epoch&#39;: 1.0})
</pre></div>
</div>
</div>
</div>
<p>Let’s see how much accuracy we get after a single training epoch of full finetuning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation accuracy: </span><span class="si">{</span><span class="n">eval_results</span><span class="p">[</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation accuracy: 0.788
</pre></div>
</div>
</div>
</div>
<p>We can now export the SFT version of the model to be used in later tutorials.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="n">mg</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">()</span><span class="si">}</span><span class="s2">/tutorial_2_sft&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Exporting MaseGraph to /home/zz7522/tutorial_2_sft.pt, /home/zz7522/tutorial_2_sft.mz</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Exporting GraphModule to /home/zz7522/tutorial_2_sft.pt</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Exporting MaseMetadata to /home/zz7522/tutorial_2_sft.mz</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">Failed to pickle call_function node: finfo</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">cannot pickle &#39;torch.finfo&#39; object</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">Failed to pickle call_function node: getattr_2</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">cannot pickle &#39;torch.finfo&#39; object</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="parameter-efficient-finetuning-peft-with-lora">
<h2>Parameter Efficient Finetuning (PEFT) with LoRA<a class="headerlink" href="#parameter-efficient-finetuning-peft-with-lora" title="Link to this heading">#</a></h2>
<p>An alternative to full fine-tuning is Parameter Efficient Fine Tuning (PEFT), which uses a small number of trainable parameters to achieve similar performance. LoRA was proposed by a research team at Microsoft in 2021, as an efficient technique for PEFT.</p>
<div style="text-align: center;">
    <img src="imgs/lora_adapter.png" alt="drawing" width="400"/>
</div>
<p>Consider the standard equation of a linear layer:</p>
<div class="math notranslate nohighlight">
\[
y = X W + b
\]</div>
<p>The LoRA method involves replacing this with the following, where A and B are low-rank matrices. We freeze the <span class="math notranslate nohighlight">\(W\)</span> parameters, and only allow the optimizer to train the parameters in <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<div class="math notranslate nohighlight">
\[
y = X (W + AB) + b
\]</div>
<p>This enables us to achieve accuracies comparable to full fine tuning, while only training a fraction of the parameters. See <a class="reference external" href="https://arxiv.org/abs/2106.09685">the paper</a> for more details. We can inject the LoRA adapter into the existing model using the <code class="docutils literal notranslate"><span class="pre">insert_lora_adapter_transform_pass</span></code> pass in Mase, as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">passes</span><span class="o">.</span><span class="n">insert_lora_adapter_transform_pass</span><span class="p">(</span>
    <span class="n">mg</span><span class="p">,</span>
    <span class="n">pass_args</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;rank&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_0_attention_self_query, target: bert.encoder.layer.0.attention.self.query with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_0_attention_self_key, target: bert.encoder.layer.0.attention.self.key with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_0_attention_self_value, target: bert.encoder.layer.0.attention.self.value with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_0_attention_output_dense, target: bert.encoder.layer.0.attention.output.dense with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_0_intermediate_dense, target: bert.encoder.layer.0.intermediate.dense with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_0_output_dense, target: bert.encoder.layer.0.output.dense with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_1_attention_self_query, target: bert.encoder.layer.1.attention.self.query with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_1_attention_self_key, target: bert.encoder.layer.1.attention.self.key with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_1_attention_self_value, target: bert.encoder.layer.1.attention.self.value with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_1_attention_output_dense, target: bert.encoder.layer.1.attention.output.dense with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_1_intermediate_dense, target: bert.encoder.layer.1.intermediate.dense with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_encoder_layer_1_output_dense, target: bert.encoder.layer.1.output.dense with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: bert_pooler_dense, target: bert.pooler.dense with LoRALinear module.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Replaced node: classifier, target: classifier with LoRALinear module.</span>
</pre></div>
</div>
</div>
</div>
<p>Similar to before, let’s report the number of trainable parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">report_trainable_parameters_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-----------------------------------------------------+------------------------+
| Submodule                                           |   Trainable Parameters |
+=====================================================+========================+
| bert                                                |                 439808 |
+-----------------------------------------------------+------------------------+
| bert.embeddings                                     |                      0 |
+-----------------------------------------------------+------------------------+
| bert.embeddings.word_embeddings                     |                      0 |
+-----------------------------------------------------+------------------------+
| bert.embeddings.token_type_embeddings               |                      0 |
+-----------------------------------------------------+------------------------+
| bert.embeddings.position_embeddings                 |                      0 |
+-----------------------------------------------------+------------------------+
| bert.embeddings.LayerNorm                           |                      0 |
+-----------------------------------------------------+------------------------+
| bert.embeddings.dropout                             |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder                                        |                 421888 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer                                  |                 421888 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0                                |                 210944 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention                      |                  71936 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self                 |                  53760 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.query           |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.query.linear    |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.query.lora_a    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.query.lora_b    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.query.dropout   |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.key             |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.key.linear      |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.key.lora_a      |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.key.lora_b      |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.key.dropout     |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.value           |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.value.linear    |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.value.lora_a    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.value.lora_b    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.self.value.dropout   |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output               |                  18176 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dense         |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dense.linear  |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dense.lora_a  |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dense.lora_b  |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dense.dropout |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.dropout       |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.attention.output.LayerNorm     |                    256 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate                   |                  69376 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate.dense             |                  69376 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate.dense.linear      |                  65536 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate.dense.lora_a      |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate.dense.lora_b      |                   3072 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.intermediate.dense.dropout     |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output                         |                  69632 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dense                   |                  69376 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dense.linear            |                  65536 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dense.lora_a            |                   3072 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dense.lora_b            |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dense.dropout           |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.dropout                 |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.0.output.LayerNorm               |                    256 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1                                |                 210944 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention                      |                  71936 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self                 |                  53760 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.query           |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.query.linear    |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.query.lora_a    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.query.lora_b    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.query.dropout   |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.key             |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.key.linear      |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.key.lora_a      |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.key.lora_b      |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.key.dropout     |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.value           |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.value.linear    |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.value.lora_a    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.value.lora_b    |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.self.value.dropout   |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output               |                  18176 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dense         |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dense.linear  |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dense.lora_a  |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dense.lora_b  |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dense.dropout |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.dropout       |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.attention.output.LayerNorm     |                    256 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate                   |                  69376 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate.dense             |                  69376 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate.dense.linear      |                  65536 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate.dense.lora_a      |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate.dense.lora_b      |                   3072 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.intermediate.dense.dropout     |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output                         |                  69632 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dense                   |                  69376 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dense.linear            |                  65536 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dense.lora_a            |                   3072 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dense.lora_b            |                    768 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dense.dropout           |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.dropout                 |                      0 |
+-----------------------------------------------------+------------------------+
| bert.encoder.layer.1.output.LayerNorm               |                    256 |
+-----------------------------------------------------+------------------------+
| bert.pooler                                         |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.pooler.dense                                   |                  17920 |
+-----------------------------------------------------+------------------------+
| bert.pooler.dense.linear                            |                  16384 |
+-----------------------------------------------------+------------------------+
| bert.pooler.dense.lora_a                            |                    768 |
+-----------------------------------------------------+------------------------+
| bert.pooler.dense.lora_b                            |                    768 |
+-----------------------------------------------------+------------------------+
| bert.pooler.dense.dropout                           |                      0 |
+-----------------------------------------------------+------------------------+
| bert.pooler.activation                              |                      0 |
+-----------------------------------------------------+------------------------+
| dropout                                             |                      0 |
+-----------------------------------------------------+------------------------+
| classifier                                          |                   1036 |
+-----------------------------------------------------+------------------------+
| classifier.linear                                   |                    256 |
+-----------------------------------------------------+------------------------+
| classifier.lora_a                                   |                    768 |
+-----------------------------------------------------+------------------------+
| classifier.lora_b                                   |                     12 |
+-----------------------------------------------------+------------------------+
| classifier.dropout                                  |                      0 |
+-----------------------------------------------------+------------------------+
| crossentropyloss_0                                  |                      0 |
+-----------------------------------------------------+------------------------+

Total Trainable Parameters: 3169816
</pre></div>
</div>
</div>
</div>
<p>In this case, LoRA reduces the number of trainable parameters by <span class="math notranslate nohighlight">\(4.5\times\)</span>! We’ll run a few more training epochs and evaluate the resulting accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">get_trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">mg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenized_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">evaluate_metric</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Evaluate accuracy</span>
<span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation accuracy: </span><span class="si">{</span><span class="n">eval_results</span><span class="p">[</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/zz7522/Projects/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
</pre></div>
</div>
<div class="output text_html">
    <div>
      
      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [782/782 00:44, Epoch 1/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>500</td>
      <td>0.441300</td>
    </tr>
  </tbody>
</table><p></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
</pre></div>
</div>
<div class="output text_html">
    <div>
      
      <progress value='1564' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [782/782 01:02]
    </div>
    </div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation accuracy: 0.82264
</pre></div>
</div>
</div>
</div>
<p>After training is finished, we can run the <code class="docutils literal notranslate"><span class="pre">fuse_lora_weights_transform_pass</span></code> pass to optimize the model for inference. This pass replaces each <code class="docutils literal notranslate"><span class="pre">LoRALinear</span></code> instance with an <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> module, where the <span class="math notranslate nohighlight">\(AB\)</span> product added to the original weights matrix. This incurs less kernel invocations when deploying the model, which reduces inference runtime.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">passes</span><span class="o">.</span><span class="n">fuse_lora_weights_transform_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">)</span>
<span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.0.attention.self.query.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.0.attention.self.key.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.0.attention.self.value.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.0.attention.output.dense.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.0.intermediate.dense.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.0.output.dense.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.1.attention.self.query.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.1.attention.self.key.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.1.attention.self.value.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.1.attention.output.dense.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.1.intermediate.dense.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.encoder.layer.1.output.dense.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for bert.pooler.dense.</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Fusing LoRALinear weights for classifier.</span>
/mnt/data/zz7522/miniconda/envs/mase/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation accuracy: </span><span class="si">{</span><span class="n">eval_results</span><span class="p">[</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation accuracy: 0.82264
</pre></div>
</div>
</div>
</div>
<p>Finally, export the finetuned model to be used in future tutorials.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="n">mg</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">()</span><span class="si">}</span><span class="s2">/tutorial_2_lora&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Exporting MaseGraph to /home/zz7522/tutorial_2_lora.pt, /home/zz7522/tutorial_2_lora.mz</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Exporting GraphModule to /home/zz7522/tutorial_2_lora.pt</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Exporting MaseMetadata to /home/zz7522/tutorial_2_lora.mz</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">Failed to pickle call_function node: finfo</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">cannot pickle &#39;torch.finfo&#39; object</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">Failed to pickle call_function node: getattr_2</span>
<span class=" -Color -Color-Yellow">WARNING </span> <span class=" -Color -Color-Blue">cannot pickle &#39;torch.finfo&#39; object</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>By adjusting the rank number of LoRA, we can control the trade-off between memory usage and fine-tuned accuracy. Such parameter-efficient fine-tuning techniques are very useful in the area of large language models (LLMs), where the memory requirement for training is a significant bottleneck.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="tutorial_1_introduction_to_mase.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes</p>
      </div>
    </a>
    <a class="right-next"
       href="tutorial_3_qat.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tutorial 3: Running Quantization-Aware Training (QAT) on Bert</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis-with-the-imdb-dataset">Sentiment Analysis with the IMDb Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-a-masegraph-with-custom-arguments">Generate a MaseGraph with Custom Arguments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#full-supervised-finetuning-sft">Full Supervised Finetuning (SFT)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-finetuning-peft-with-lora">Parameter Efficient Finetuning (PEFT) with LoRA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DeepWok
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, DeepWok.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>