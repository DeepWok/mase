
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tutorial 4: Unstructured Pruning on Bert &#8212; MASE 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-data-viewer/jsonview.bundle.css?v=f6ef2277" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/libs/html/datatables.min.css?v=4b4fd840" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_links.css?v=2150a916" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_style.css?v=92936fa5" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/needstable.css?v=5e1b6797" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_toggle.css?v=5c6620df" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/common_css/need_core.css?v=f5b60a78" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-needs/modern.css?v=803738c0" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../_static/documentation_options.js?v=e645c8fa"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/sphinx-data-viewer/jsonview.bundle.js?v=18cd53c5"></script>
    <script src="../../../_static/sphinx-data-viewer/jsonview_loader.js?v=f7ff7e7d"></script>
    <script src="../../../_static/sphinx-needs/libs/html/datatables.min.js?v=8a4aee21"></script>
    <script src="../../../_static/sphinx-needs/libs/html/datatables_loader.js?v=a2cae175"></script>
    <script src="../../../_static/sphinx-needs/libs/html/sphinx_needs_collapse.js?v=dca66431"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/documentation/tutorials/tutorial_4_pruning';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna" href="tutorial_5_nas_optuna.html" />
    <link rel="prev" title="Tutorial 3: Running Quantization-Aware Training (QAT) on Bert" href="tutorial_3_qat.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">MASE 0.0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../installation.html">Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/Get-started-using-uv.html">Getting Started using uv</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/Get-started-using-Docker.html">Getting Started using Docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getting_started/Get-started-students.html">Additional Instructions for Imperial College Students</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../tutorials.html">Tutorials</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tutorial_1_introduction_to_mase.html">Tutorial 1: Introduction to the Mase IR, MaseGraph and Torch FX passes</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_2_lora_finetune.html">Tutorial 2: Finetuning Bert for Sequence Classification using a LoRA adapter</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_3_qat.html">Tutorial 3: Running Quantization-Aware Training (QAT) on Bert</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Tutorial 4: Unstructured Pruning on Bert</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_5_nas_optuna.html">Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_6_mixed_precision_search.html">Tutorial 6: Mixed Precision Quantization Search with Mase and Optuna</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_7_distributed_deployment.html">Tutorial 7: Deploying a Model for Inference on Distributed Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_9_kernel_fusion.html">Tutorial 9: Running Kernel Fusion for Inference Acceleration on GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced/tensorRT_quantization_tutorial.html">Advanced: TensorRT Quantization Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced/onnxrt_quantization_tutorial.html">Advanced: ONNX Runtime Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced/cli.html">Advanced: Using Mase CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer/Add-model-to-machop.html">Developer: Guide on how to add a new model into Machop</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer/doc_writing.html">Developer: How to write documentations in MASE</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer/how_to_extend_search.html">Developer: How to extend search</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../health.html">Repository Health</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../specifications.html">Coding Style Specifications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../specifications/C-coding-style-specifications.html">C/C++ Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../specifications/Python-coding-style-specifications.html">Python Coding Style Specifications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../specifications/Verilog-coding-style-specifications.html">Verilog Coding Style Specifications</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machop API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../machop.html">Machop Documentation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chop/actions.html">chop.actions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/datasets.html">chop.datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/distributed.html">chop.distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/ir.html">chop.ir</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/models.html">chop.models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/nn.html">chop.nn</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../chop/nn_quantized.html">chop.nn.quantized</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../chop/nn_quantized_functional.html">chop.nn.quantized.functional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../chop/nn_quantized_modules.html">chop.nn.quantized.modules</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../chop/passes.html">chop.passes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../chop/passes_module.html">chop.passes.module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../chop/module_analysis/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/module_transform/quantization.html">chop.passes.module.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/onn.html">chop.passes.module.transforms.onn</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../chop/passes_graph.html">chop.passes.graph</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/add_metadata.html">chop.passes.graph.analysis.add_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/autosharding.html">chop.passes.graph.analysis.autosharding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/init_metadata.html">chop.passes.graph.analysis.init_metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/report.html">chop.passes.graph.analysis.report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/statistical_profiler.html">chop.passes.graph.analysis.statistical_profiler.profile_statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/verify.html">chop.passes.graph.analysis.verify.verify</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/quantization.html">chop.passes.graph.calculate_avg_bits_mg_analysis_pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/pruning.html">chop.passes.graph.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/analysis/runtime.html">chop.passes.graph.analysis.runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/pruning.html">chop.passes.transform.pruning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/quantize.html">chop.passes.transform.quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/utils.html">chop.passes.transform.utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/transform/tensorrt.html">chop.passes.transform.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/interface/save_and_load.html">chop.passes.interface.save_and_load</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/interface/tensorrt.html">chop.passes.interface.tensorrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../chop/interface/onnxrt.html">chop.passes.interface.onnxrt</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/pipelines.html">chop.pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chop/tools.html">chop.tools</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../adls_2024.html">Advanced Deep Learning Systems: 2024/2025</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_0_introduction.html">Lab 0: Introduction to Mase</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_1_compression.html">Lab 1: Model Compression (Quantization and Pruning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_2_nas.html">Lab 2: Neural Architecture Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab_3_mixed_precision_search.html">Lab 3: Mixed Precision Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/lab4-software.html">Lab 4 (Software Stream) Performance Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2024/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../adls_2023.html">Advanced Deep Learning Systems: 2023/2024</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab1.html">Lab 1 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab2.html">Lab 2 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab3.html">Lab 3 for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/lab4-software.html">Lab 4 (Software Stream) for Advanced Deep Learning Systems (ADLS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labs_2023/setup_docker_env.html">ADLS Docker Environment Setup</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/modules/documentation/tutorials/tutorial_4_pruning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tutorial 4: Unstructured Pruning on Bert</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-the-model">Importing the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unstructured-pruning">Unstructured Pruning</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tutorial-4-unstructured-pruning-on-bert">
<h1>Tutorial 4: Unstructured Pruning on Bert<a class="headerlink" href="#tutorial-4-unstructured-pruning-on-bert" title="Link to this heading">#</a></h1>
<p>Pruning is a technique used to reduce the size and complexity of neural networks by removing unnecessary parameters (weights and connections) or structural components (neurons, filters, or layers). The goal is to create a smaller, more efficient model that maintains most of the original model’s performance. The following benefits can be seen from pruning neural networks:</p>
<ul class="simple">
<li><p><strong>Reduce model size</strong>: Deep neural networks often have millions of parameters, leading to large storage requirements.</p></li>
<li><p><strong>Decrease inference time</strong>: Fewer parameters mean fewer computations, resulting in faster predictions.</p></li>
<li><p><strong>Improve generalization</strong>: Removing unnecessary connections can help prevent overfitting.</p></li>
<li><p><strong>Energy efficiency</strong>: Smaller models require less energy to run, which is crucial for edge devices and mobile applications.</p></li>
</ul>
<p>Structured pruning removes entire structures (e.g., channels, filters, or layers) from the network, while unstructured pruning removes individual weights or connections from the network, regardless of their location. In this tutorial, we’ll build on top of Tutorial 3 by taking the quantized Bert model and running Mase’s unstructured pruning pass. After pruning, we’ll run further fine tuning iterations to retain sequence classification accuracy in the pruned model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;prajjwal1/bert-tiny&quot;</span>
<span class="n">tokenizer_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;imdb&quot;</span>
</pre></div>
</div>
</div>
</div>
<section id="importing-the-model">
<h2>Importing the model<a class="headerlink" href="#importing-the-model" title="Link to this heading">#</a></h2>
<p>If you are starting from scratch, you can create a MaseGraph for Bert by running the following cell.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">chop</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaseGraph</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">chop.passes</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">passes</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">problem_type</span> <span class="o">=</span> <span class="s2">&quot;single_label_classification&quot;</span>

<span class="n">mg</span> <span class="o">=</span> <span class="n">MaseGraph</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">hf_input_names</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
        <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span>
        <span class="s2">&quot;labels&quot;</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>

<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">passes</span><span class="o">.</span><span class="n">init_metadata_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">)</span>
<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">passes</span><span class="o">.</span><span class="n">add_common_metadata_analysis_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Getting dummy input for prajjwal1/bert-tiny.</span>
/Users/yz10513/anaconda3/envs/mase/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
<p>If you have previously ran the tutorial on Quantization-Aware Training (QAT), run the following cell to import the fine tuned checkpoint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">chop</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaseGraph</span>

<span class="n">mg</span> <span class="o">=</span> <span class="n">MaseGraph</span><span class="o">.</span><span class="n">from_checkpoint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">()</span><span class="si">}</span><span class="s2">/tutorial_3_qat&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="unstructured-pruning">
<h2>Unstructured Pruning<a class="headerlink" href="#unstructured-pruning" title="Link to this heading">#</a></h2>
<p>Before running pruning, let’s evaluate the model accuracy on the IMDb dataset. If you’re coming from Tutorial, this would be the same as the accuracy after Quantization Aware Training (QAT). If you’ve just initialized the model, this will likely be a random guess (i.e. around 50%), in which case pruning wouldn’t have a significant effect on the accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">chop.tools</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_tokenized_dataset</span><span class="p">,</span> <span class="n">get_trainer</span>

<span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenized_dataset</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_name</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="o">=</span><span class="n">tokenizer_checkpoint</span><span class="p">,</span>
    <span class="n">return_tokenizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">get_trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">mg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenized_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">evaluate_metric</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Evaluate accuracy</span>
<span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation accuracy: </span><span class="si">{</span><span class="n">eval_results</span><span class="p">[</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Tokenizing dataset imdb with AutoTokenizer for bert-base-uncased.</span>
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2024-12-01 15:14:08,830] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to mps (auto detect)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1201 15:14:09.744000 8580182592 torch/distributed/elastic/multiprocessing/redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
100%|██████████| 3125/3125 [04:39&lt;00:00, 11.16it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation accuracy: 0.84232
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>To run the pruning pass, we pass the following pruning configuration dictionary, which defines the following parameters.</p>
<ul class="simple">
<li><p><strong>Sparsity</strong>: a value between 0 and 1, expressing the proportion of elements in the model that should be pruned (i.e. set to 0).</p></li>
<li><p><strong>Method</strong>: several pruning methods are supported, including <code class="docutils literal notranslate"><span class="pre">Random</span></code> and <code class="docutils literal notranslate"><span class="pre">L1-Norm</span></code>.</p></li>
<li><p><strong>Scope</strong>: defines whether to consider each weight/activation tensor individually (<code class="docutils literal notranslate"><span class="pre">local</span></code>) or all tensors in the model (<code class="docutils literal notranslate"><span class="pre">global</span></code>) when obtaining statistics for pruning (e.g. absolute value threshold for pruning)</p></li>
</ul>
<p>We’ll start by running random pruning with local scope, at a fixed sparsity. This may be suboptimal, but in future tutorials we’ll see how to find optimal pruning and quantization configurations for a given model on a specified dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">chop.passes</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">passes</span>

<span class="n">pruning_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;sparsity&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;l1-norm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;scope&quot;</span><span class="p">:</span> <span class="s2">&quot;local&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;activation&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;sparsity&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;l1-norm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;scope&quot;</span><span class="p">:</span> <span class="s2">&quot;local&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>

<span class="n">mg</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">passes</span><span class="o">.</span><span class="n">prune_transform_pass</span><span class="p">(</span><span class="n">mg</span><span class="p">,</span> <span class="n">pass_args</span><span class="o">=</span><span class="n">pruning_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_0_attention_self_query</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_0_attention_self_key</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_0_attention_self_value</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_0_attention_output_dense</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_0_intermediate_dense</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_0_output_dense</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_1_attention_self_query</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_1_attention_self_key</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_1_attention_self_value</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_1_attention_output_dense</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_1_intermediate_dense</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_encoder_layer_1_output_dense</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: bert_pooler_dense</span>
<span class=" -Color -Color-Green">INFO    </span> <span class=" -Color -Color-Blue">Pruning module: classifier</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s evaluate the effect of pruning on accuracy. It’s likely to observe drops of around 10% or more.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">get_trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">mg</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenized_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">evaluate_metric</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Evaluate accuracy</span>
<span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation accuracy: </span><span class="si">{</span><span class="n">eval_results</span><span class="p">[</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 3125/3125 [04:47&lt;00:00, 10.88it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation accuracy: 0.55512
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<p>To overcome the drop in accuracy, we’ll run a few finetuning epochs. This allows the model to adapt to the new pruning mask.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  3%|▎         | 500/15625 [00:56&lt;21:55, 11.50it/s]  
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.459, &#39;grad_norm&#39;: 1.4026139974594116, &#39;learning_rate&#39;: 4.8400000000000004e-05, &#39;epoch&#39;: 0.16}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  6%|▋         | 1001/15625 [01:36&lt;31:03,  7.85it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4056, &#39;grad_norm&#39;: 0.9277871251106262, &#39;learning_rate&#39;: 4.6800000000000006e-05, &#39;epoch&#39;: 0.32}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 10%|▉         | 1500/15625 [02:15&lt;12:55, 18.21it/s]  
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4219, &#39;grad_norm&#39;: 1.443852186203003, &#39;learning_rate&#39;: 4.52e-05, &#39;epoch&#39;: 0.48}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 13%|█▎        | 2000/15625 [02:52&lt;34:12,  6.64it/s]  
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4059, &#39;grad_norm&#39;: 1.2503076791763306, &#39;learning_rate&#39;: 4.36e-05, &#39;epoch&#39;: 0.64}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 16%|█▌        | 2500/15625 [03:25&lt;11:55, 18.35it/s]  
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4015, &#39;grad_norm&#39;: 0.6023377776145935, &#39;learning_rate&#39;: 4.2e-05, &#39;epoch&#39;: 0.8}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 19%|█▉        | 3000/15625 [03:53&lt;11:18, 18.61it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4032, &#39;grad_norm&#39;: 1.3447505235671997, &#39;learning_rate&#39;: 4.0400000000000006e-05, &#39;epoch&#39;: 0.96}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 22%|██▏       | 3501/15625 [04:20&lt;12:08, 16.64it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4193, &#39;grad_norm&#39;: 0.6158122420310974, &#39;learning_rate&#39;: 3.88e-05, &#39;epoch&#39;: 1.12}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 26%|██▌       | 4002/15625 [04:46&lt;12:50, 15.09it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4004, &#39;grad_norm&#39;: 1.2009944915771484, &#39;learning_rate&#39;: 3.72e-05, &#39;epoch&#39;: 1.28}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 29%|██▉       | 4503/15625 [05:16&lt;10:15, 18.07it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3783, &#39;grad_norm&#39;: 0.8180735111236572, &#39;learning_rate&#39;: 3.56e-05, &#39;epoch&#39;: 1.44}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 32%|███▏      | 5002/15625 [05:44&lt;10:17, 17.21it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3933, &#39;grad_norm&#39;: 0.59749835729599, &#39;learning_rate&#39;: 3.4000000000000007e-05, &#39;epoch&#39;: 1.6}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 35%|███▌      | 5502/15625 [06:12&lt;11:11, 15.07it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3883, &#39;grad_norm&#39;: 0.9686317443847656, &#39;learning_rate&#39;: 3.24e-05, &#39;epoch&#39;: 1.76}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 38%|███▊      | 6002/15625 [06:41&lt;09:48, 16.35it/s]  
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3871, &#39;grad_norm&#39;: 1.6825438737869263, &#39;learning_rate&#39;: 3.08e-05, &#39;epoch&#39;: 1.92}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 42%|████▏     | 6503/15625 [07:08&lt;08:24, 18.08it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3808, &#39;grad_norm&#39;: 1.0123984813690186, &#39;learning_rate&#39;: 2.9199999999999998e-05, &#39;epoch&#39;: 2.08}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 45%|████▍     | 7001/15625 [07:33&lt;08:33, 16.79it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3938, &#39;grad_norm&#39;: 0.5268100500106812, &#39;learning_rate&#39;: 2.7600000000000003e-05, &#39;epoch&#39;: 2.24}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 48%|████▊     | 7502/15625 [08:01&lt;07:36, 17.81it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.391, &#39;grad_norm&#39;: 0.721001148223877, &#39;learning_rate&#39;: 2.6000000000000002e-05, &#39;epoch&#39;: 2.4}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 51%|█████     | 8001/15625 [08:27&lt;08:42, 14.59it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3842, &#39;grad_norm&#39;: 0.9280937314033508, &#39;learning_rate&#39;: 2.44e-05, &#39;epoch&#39;: 2.56}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 54%|█████▍    | 8502/15625 [08:52&lt;07:08, 16.61it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.4128, &#39;grad_norm&#39;: 1.1052242517471313, &#39;learning_rate&#39;: 2.2800000000000002e-05, &#39;epoch&#39;: 2.72}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 58%|█████▊    | 9003/15625 [09:20&lt;05:59, 18.43it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.379, &#39;grad_norm&#39;: 0.6635761260986328, &#39;learning_rate&#39;: 2.12e-05, &#39;epoch&#39;: 2.88}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 61%|██████    | 9502/15625 [09:46&lt;05:28, 18.65it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3885, &#39;grad_norm&#39;: 1.7871322631835938, &#39;learning_rate&#39;: 1.9600000000000002e-05, &#39;epoch&#39;: 3.04}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 64%|██████▍   | 10002/15625 [10:11&lt;05:16, 17.76it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3713, &#39;grad_norm&#39;: 1.0901461839675903, &#39;learning_rate&#39;: 1.8e-05, &#39;epoch&#39;: 3.2}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 67%|██████▋   | 10502/15625 [10:36&lt;04:28, 19.10it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.389, &#39;grad_norm&#39;: 0.6938749551773071, &#39;learning_rate&#39;: 1.6400000000000002e-05, &#39;epoch&#39;: 3.36}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 70%|███████   | 11003/15625 [11:02&lt;04:19, 17.79it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3849, &#39;grad_norm&#39;: 0.6419057250022888, &#39;learning_rate&#39;: 1.48e-05, &#39;epoch&#39;: 3.52}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 74%|███████▎  | 11502/15625 [11:33&lt;04:09, 16.54it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3755, &#39;grad_norm&#39;: 0.9091131687164307, &#39;learning_rate&#39;: 1.32e-05, &#39;epoch&#39;: 3.68}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 77%|███████▋  | 12001/15625 [11:58&lt;03:40, 16.44it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3765, &#39;grad_norm&#39;: 0.7711085081100464, &#39;learning_rate&#39;: 1.16e-05, &#39;epoch&#39;: 3.84}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 80%|████████  | 12503/15625 [12:24&lt;02:43, 19.11it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3713, &#39;grad_norm&#39;: 0.4314064383506775, &#39;learning_rate&#39;: 1e-05, &#39;epoch&#39;: 4.0}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 83%|████████▎ | 13001/15625 [12:51&lt;02:35, 16.88it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.375, &#39;grad_norm&#39;: 0.8700340390205383, &#39;learning_rate&#39;: 8.400000000000001e-06, &#39;epoch&#39;: 4.16}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 86%|████████▋ | 13502/15625 [13:16&lt;02:01, 17.52it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3822, &#39;grad_norm&#39;: 0.7520729899406433, &#39;learning_rate&#39;: 6.800000000000001e-06, &#39;epoch&#39;: 4.32}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 90%|████████▉ | 14002/15625 [13:41&lt;01:30, 17.97it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3715, &#39;grad_norm&#39;: 0.5653247833251953, &#39;learning_rate&#39;: 5.2e-06, &#39;epoch&#39;: 4.48}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 93%|█████████▎| 14500/15625 [14:07&lt;00:55, 20.33it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3871, &#39;grad_norm&#39;: 1.1256822347640991, &#39;learning_rate&#39;: 3.6e-06, &#39;epoch&#39;: 4.64}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 96%|█████████▌| 15002/15625 [14:36&lt;00:38, 16.01it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3831, &#39;grad_norm&#39;: 0.8478624820709229, &#39;learning_rate&#39;: 2.0000000000000003e-06, &#39;epoch&#39;: 4.8}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 99%|█████████▉| 15502/15625 [15:02&lt;00:06, 17.95it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;loss&#39;: 0.3749, &#39;grad_norm&#39;: 0.9598965644836426, &#39;learning_rate&#39;: 4.0000000000000003e-07, &#39;epoch&#39;: 4.96}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 15625/15625 [15:08&lt;00:00, 17.20it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;train_runtime&#39;: 908.3575, &#39;train_samples_per_second&#39;: 137.611, &#39;train_steps_per_second&#39;: 17.201, &#39;train_loss&#39;: 0.3912585158691406, &#39;epoch&#39;: 5.0}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TrainOutput(global_step=15625, training_loss=0.3912585158691406, metrics={&#39;train_runtime&#39;: 908.3575, &#39;train_samples_per_second&#39;: 137.611, &#39;train_steps_per_second&#39;: 17.201, &#39;total_flos&#39;: 0.0, &#39;train_loss&#39;: 0.3912585158691406, &#39;epoch&#39;: 5.0})
</pre></div>
</div>
</div>
</div>
<p>Let’s evaluate the model accuracy after finetuning. We should see that the accuracy is reverted back to the original level.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluation accuracy: </span><span class="si">{</span><span class="n">eval_results</span><span class="p">[</span><span class="s1">&#39;eval_accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 3125/3125 [02:02&lt;00:00, 25.45it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Evaluation accuracy: 0.83624
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="tutorial_3_qat.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Tutorial 3: Running Quantization-Aware Training (QAT) on Bert</p>
      </div>
    </a>
    <a class="right-next"
       href="tutorial_5_nas_optuna.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importing-the-model">Importing the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unstructured-pruning">Unstructured Pruning</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DeepWok
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023, DeepWok.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>