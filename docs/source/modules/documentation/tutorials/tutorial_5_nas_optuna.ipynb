{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads, inspired by the [NAS-BERT paper](https://arxiv.org/abs/2105.14444)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\n",
    "        nn.Linear,\n",
    "        Identity,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_int` and `trial.suggest_categorical` functions to trigger the chosen sampler to choose parameter choices and layer types. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Update the paramaters in the config\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "    ]:\n",
    "        chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n",
    "        setattr(config, param, search_space[param][chosen_idx])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == nn.Linear:\n",
    "                continue\n",
    "            elif new_layer_cls == Identity:\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Set the model as an attribute so we can fetch it later\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = RandomSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the model associated with the best trial as follows, and export to be used in future tutorials. In Tutorial 6, we'll see how to run mixed-precision quantization search on top of the model we've just found through NAS to further find the optimal quantization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "\n",
    "with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Optimized Model with CompressionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the CompressionPipeline in Mase to run uniform quantization and pruning over the searched model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "\n",
    "mg = MaseGraph(model)\n",
    "pipe = CompressionPipeline()\n",
    "\n",
    "quantization_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\n",
    "        \"config\": {\n",
    "            \"name\": None,\n",
    "        }\n",
    "    },\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            # data\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            # weight\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            # bias\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "pruning_config = {\n",
    "    \"weight\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mg, _ = pipe(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"quantize_transform_pass\": quantization_config,\n",
    "        \"prune_transform_pass\": pruning_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the MaseGraph for the compressed checkpoint to be used in future tutorials for hardware generation and distributed deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.export(f\"{Path.home()}/tutorial_5_nas_compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 5a, full code for random method ###\n",
    "\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_tokenized_dataset, get_trainer\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler, TPESampler, RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define your checkpoints & dataset\n",
    "# -----------------------------\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# Load tokenized dataset and tokenizer\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define search space\n",
    "# -----------------------------\n",
    "grid_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "# Same discrete sets for TPE/Random:\n",
    "tpe_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Construct the model given a trial\n",
    "# -----------------------------\n",
    "def construct_model(trial):\n",
    "    \"\"\"\n",
    "    Creates a BERT model with the hyperparameters suggested by the trial.\n",
    "    We manually map each parameter to the correct config attribute or layer override.\n",
    "    \"\"\"\n",
    "    # Load the default config from a small BERT checkpoint\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Map parameters to config\n",
    "    config.num_hidden_layers = trial.suggest_categorical(\n",
    "        \"num_layers\", tpe_search_space[\"num_layers\"]\n",
    "    )\n",
    "    config.num_attention_heads = trial.suggest_categorical(\n",
    "        \"num_heads\", tpe_search_space[\"num_heads\"]\n",
    "    )\n",
    "    config.hidden_size = trial.suggest_categorical(\n",
    "        \"hidden_size\", tpe_search_space[\"hidden_size\"]\n",
    "    )\n",
    "    config.intermediate_size = trial.suggest_categorical(\n",
    "        \"intermediate_size\", tpe_search_space[\"intermediate_size\"]\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Handle linear layer choice\n",
    "    linear_choice = trial.suggest_categorical(\n",
    "        \"linear_layer_choices\", tpe_search_space[\"linear_layer_choices\"]\n",
    "    )\n",
    "    if linear_choice == \"identity\":\n",
    "        # For each linear layer that is square (in_features == out_features),\n",
    "        # replace it with Identity\n",
    "        for name, layer in model.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "                deepsetattr(model, name, Identity())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define the objective function\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    "    # Build the model given the trial\n",
    "    trial_model = construct_model(trial)\n",
    "\n",
    "    # Create a Trainer (from chop) that handles fine-tuning\n",
    "    trainer = get_trainer(\n",
    "        model=trial_model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=3,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Save the model to the trial user attributes for later retrieval if needed\n",
    "    trial.set_user_attr(\"model\", trial_model)\n",
    "\n",
    "    # Return the metric you want to optimize\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Helper to run a study and collect \"running best accuracies\"\n",
    "# -----------------------------\n",
    "def run_study_and_get_curve(sampler, n_trials=None, study_name=\"study\"):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study with the provided sampler and returns:\n",
    "      - the study object\n",
    "      - a list of best accuracies up to each trial (running max)\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=study_name,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        timeout=60 * 60 * 24,  # or specify a time limit\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    # Build a list of max accuracies up to each trial\n",
    "    running_max_accuracies = []\n",
    "    current_max = 0.0\n",
    "    for t in study.trials:\n",
    "        if t.value is not None and t.value > current_max:\n",
    "            current_max = t.value\n",
    "        running_max_accuracies.append(current_max)\n",
    "\n",
    "    return study, running_max_accuracies\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Function to save trials data to CSV\n",
    "# -----------------------------\n",
    "def save_study_results_to_csv(study, filename):\n",
    "    \"\"\"\n",
    "    Saves each trial's results into a CSV, including:\n",
    "      - trial number\n",
    "      - objective value (accuracy)\n",
    "      - parameters\n",
    "      - model config parameters\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        row = {\n",
    "            \"trial_number\": t.number,\n",
    "            \"accuracy\": t.value,\n",
    "        }\n",
    "        # Merge in parameter key-value pairs directly\n",
    "        row.update(t.params)\n",
    "\n",
    "        # Add model config if it exists in user attributes\n",
    "        if \"model\" in t.user_attrs:\n",
    "            model_config = t.user_attrs[\"model\"].config.to_dict()\n",
    "            for key, value in model_config.items():\n",
    "                row[f\"config_{key}\"] = value\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(rows)} trials with model configs to {filename}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random_sampler = RandomSampler()\n",
    "    random_study, random_max_curve = run_study_and_get_curve(\n",
    "        sampler=random_sampler,\n",
    "        n_trials=10,\n",
    "        study_name=\"bert-random-study\",\n",
    "    )\n",
    "    print(f\"[RandomSampler] Number of trials: {len(random_study.trials)}\")\n",
    "    print(f\"[RandomSampler] Best accuracy: {random_study.best_value:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(random_max_curve)+1), random_max_curve, label=\"RandomSampler\")\n",
    "    plt.xlabel(\"Number of Trials\")\n",
    "    plt.ylabel(\"Max Accuracy So Far\")\n",
    "    plt.title(\"Comparison of GridSampler vs TPESampler vs RandomSampler\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    best_random_model = random_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    with open(\"best_random_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(best_random_model, f)\n",
    "\n",
    "    save_study_results_to_csv(random_study, \"random_study_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part a, full code for grid method ###\n",
    "\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_tokenized_dataset, get_trainer\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler, TPESampler, RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define your checkpoints & dataset\n",
    "# -----------------------------\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# Load tokenized dataset and tokenizer\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define search space\n",
    "# -----------------------------\n",
    "grid_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "# Same discrete sets for TPE/Random:\n",
    "tpe_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Construct the model given a trial\n",
    "# -----------------------------\n",
    "def construct_model(trial):\n",
    "    \"\"\"\n",
    "    Creates a BERT model with the hyperparameters suggested by the trial.\n",
    "    We manually map each parameter to the correct config attribute or layer override.\n",
    "    \"\"\"\n",
    "    # Load the default config from a small BERT checkpoint\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Map parameters to config\n",
    "    config.num_hidden_layers = trial.suggest_categorical(\n",
    "        \"num_layers\", grid_search_space[\"num_layers\"]\n",
    "    )\n",
    "    config.num_attention_heads = trial.suggest_categorical(\n",
    "        \"num_heads\", grid_search_space[\"num_heads\"]\n",
    "    )\n",
    "    config.hidden_size = trial.suggest_categorical(\n",
    "        \"hidden_size\", grid_search_space[\"hidden_size\"]\n",
    "    )\n",
    "    config.intermediate_size = trial.suggest_categorical(\n",
    "        \"intermediate_size\", grid_search_space[\"intermediate_size\"]\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Handle linear layer choice\n",
    "    linear_choice = trial.suggest_categorical(\n",
    "        \"linear_layer_choices\", grid_search_space[\"linear_layer_choices\"]\n",
    "    )\n",
    "    if linear_choice == \"identity\":\n",
    "        # For each linear layer that is square (in_features == out_features),\n",
    "        # replace it with Identity\n",
    "        for name, layer in model.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "                deepsetattr(model, name, Identity())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define the objective function\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    "    # Build the model given the trial\n",
    "    trial_model = construct_model(trial)\n",
    "\n",
    "    # Create a Trainer (from chop) that handles fine-tuning\n",
    "    trainer = get_trainer(\n",
    "        model=trial_model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=3,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Save the model to the trial user attributes for later retrieval if needed\n",
    "    trial.set_user_attr(\"model\", trial_model)\n",
    "\n",
    "    # Return the metric you want to optimize\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Helper to run a study and collect \"running best accuracies\"\n",
    "# -----------------------------\n",
    "def run_study_and_get_curve(sampler, n_trials=None, study_name=\"study\"):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study with the provided sampler and returns:\n",
    "      - the study object\n",
    "      - a list of best accuracies up to each trial (running max)\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=study_name,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        timeout=60 * 60 * 24,  # or specify a time limit\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    # Build a list of max accuracies up to each trial\n",
    "    running_max_accuracies = []\n",
    "    current_max = 0.0\n",
    "    for t in study.trials:\n",
    "        if t.value is not None and t.value > current_max:\n",
    "            current_max = t.value\n",
    "        running_max_accuracies.append(current_max)\n",
    "\n",
    "    return study, running_max_accuracies\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Function to save trials data to CSV\n",
    "# -----------------------------\n",
    "def save_study_results_to_csv(study, filename):\n",
    "    \"\"\"\n",
    "    Saves each trial's results into a CSV, including:\n",
    "      - trial number\n",
    "      - objective value (accuracy)\n",
    "      - parameters\n",
    "      - model config parameters\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        row = {\n",
    "            \"trial_number\": t.number,\n",
    "            \"accuracy\": t.value,\n",
    "        }\n",
    "        # Merge in parameter key-value pairs directly\n",
    "        row.update(t.params)\n",
    "\n",
    "        # Add model config if it exists in user attributes\n",
    "        if \"model\" in t.user_attrs:\n",
    "            model_config = t.user_attrs[\"model\"].config.to_dict()\n",
    "            for key, value in model_config.items():\n",
    "                row[f\"config_{key}\"] = value\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(rows)} trials with model configs to {filename}.\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Compare GridSampler vs TPESampler vs RandomSampler\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # For GridSampler, define discrete search space\n",
    "    grid_sampler = GridSampler(\n",
    "       search_space={\n",
    "           \"num_layers\": grid_search_space[\"num_layers\"],\n",
    "           \"num_heads\": grid_search_space[\"num_heads\"],\n",
    "           \"hidden_size\": grid_search_space[\"hidden_size\"],\n",
    "           \"intermediate_size\": grid_search_space[\"intermediate_size\"],\n",
    "           \"linear_layer_choices\": grid_search_space[\"linear_layer_choices\"],\n",
    "       }\n",
    "    )\n",
    "\n",
    "    #tpe_sampler = TPESampler()\n",
    "    # random_sampler = RandomSampler()\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7a. Run with GridSampler\n",
    "    # ---------------------------------------\n",
    "    grid_study, grid_max_curve = run_study_and_get_curve(\n",
    "       sampler=grid_sampler,\n",
    "       n_trials=10,  # product of all combos if you set this None or a bigger number\n",
    "       study_name=\"bert-grid-study\",\n",
    "    )\n",
    "    print(f\"[GridSampler] Number of trials: {len(grid_study.trials)}\")\n",
    "    print(f\"[GridSampler] Best accuracy: {grid_study.best_value:.4f}\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7b. Run with TPESampler\n",
    "    # ---------------------------------------\n",
    "    #tpe_study, tpe_max_curve = run_study_and_get_curve(\n",
    "    #    sampler=tpe_sampler,\n",
    "    #    n_trials=10,  # pick more if desired\n",
    "    #    study_name=\"bert-tpe-study\",\n",
    "    #)\n",
    "    #print(f\"[TPESampler] Number of trials: {len(tpe_study.trials)}\")\n",
    "    #print(f\"[TPESampler] Best accuracy: {tpe_study.best_value:.4f}\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7c. Run with RandomSampler\n",
    "    # ---------------------------------------\n",
    "    # random_study, random_max_curve = run_study_and_get_curve(\n",
    "    #     sampler=random_sampler,\n",
    "    #     n_trials=10,  # pick more if desired\n",
    "    #     study_name=\"bert-random-study\",\n",
    "    # )\n",
    "    # print(f\"[RandomSampler] Number of trials: {len(random_study.trials)}\")\n",
    "    # print(f\"[RandomSampler] Best accuracy: {random_study.best_value:.4f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8. Plot the results\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(grid_max_curve)+1), grid_max_curve, label=\"GridSampler\")\n",
    "    #plt.plot(range(1, len(tpe_max_curve)+1), tpe_max_curve, label=\"TPESampler\")\n",
    "    # plt.plot(range(1, len(random_max_curve)+1), random_max_curve, label=\"RandomSampler\")\n",
    "    plt.xlabel(\"Number of Trials\")\n",
    "    plt.ylabel(\"Max Accuracy So Far\")\n",
    "    plt.title(\"Comparison of GridSampler vs TPESampler vs RandomSampler\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 9. (Optional) Save best models\n",
    "    # -----------------------------\n",
    "    best_grid_model = grid_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    with open(\"best_grid_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(best_grid_model, f)\n",
    "\n",
    "    #best_tpe_model = tpe_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    #with open(\"best_tpe_model.pkl\", \"wb\") as f:\n",
    "        #dill.dump(best_tpe_model, f)\n",
    "\n",
    "    # best_random_model = random_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    # with open(\"best_random_model.pkl\", \"wb\") as f:\n",
    "    #     dill.dump(best_random_model, f)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 10. Save all trials to CSV\n",
    "    # -----------------------------\n",
    "    save_study_results_to_csv(grid_study, \"grid_study_trials.csv\")\n",
    "    # save_study_results_to_csv(tpe_study, \"tpe_study_trials.csv\")\n",
    "    # save_study_results_to_csv(random_study, \"random_study_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part a, full code for TPE method ###\n",
    "\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_tokenized_dataset, get_trainer\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler, TPESampler, RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define your checkpoints & dataset\n",
    "# -----------------------------\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# Load tokenized dataset and tokenizer\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define search space\n",
    "# -----------------------------\n",
    "grid_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "# Same discrete sets for TPE/Random:\n",
    "tpe_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Construct the model given a trial\n",
    "# -----------------------------\n",
    "def construct_model(trial):\n",
    "    \"\"\"\n",
    "    Creates a BERT model with the hyperparameters suggested by the trial.\n",
    "    We manually map each parameter to the correct config attribute or layer override.\n",
    "    \"\"\"\n",
    "    # Load the default config from a small BERT checkpoint\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Map parameters to config\n",
    "    config.num_hidden_layers = trial.suggest_categorical(\n",
    "        \"num_layers\", tpe_search_space[\"num_layers\"]\n",
    "    )\n",
    "    config.num_attention_heads = trial.suggest_categorical(\n",
    "        \"num_heads\", tpe_search_space[\"num_heads\"]\n",
    "    )\n",
    "    config.hidden_size = trial.suggest_categorical(\n",
    "        \"hidden_size\", tpe_search_space[\"hidden_size\"]\n",
    "    )\n",
    "    config.intermediate_size = trial.suggest_categorical(\n",
    "        \"intermediate_size\", tpe_search_space[\"intermediate_size\"]\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Handle linear layer choice\n",
    "    linear_choice = trial.suggest_categorical(\n",
    "        \"linear_layer_choices\", tpe_search_space[\"linear_layer_choices\"]\n",
    "    )\n",
    "    if linear_choice == \"identity\":\n",
    "        # For each linear layer that is square (in_features == out_features),\n",
    "        # replace it with Identity\n",
    "        for name, layer in model.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "                deepsetattr(model, name, Identity())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define the objective function\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    "    # Build the model given the trial\n",
    "    trial_model = construct_model(trial)\n",
    "\n",
    "    # Create a Trainer (from chop) that handles fine-tuning\n",
    "    trainer = get_trainer(\n",
    "        model=trial_model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=3,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Save the model to the trial user attributes for later retrieval if needed\n",
    "    trial.set_user_attr(\"model\", trial_model)\n",
    "\n",
    "    # Return the metric you want to optimize\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Helper to run a study and collect \"running best accuracies\"\n",
    "# -----------------------------\n",
    "def run_study_and_get_curve(sampler, n_trials=None, study_name=\"study\"):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study with the provided sampler and returns:\n",
    "      - the study object\n",
    "      - a list of best accuracies up to each trial (running max)\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=study_name,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        timeout=60 * 60 * 24,  # or specify a time limit\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    # Build a list of max accuracies up to each trial\n",
    "    running_max_accuracies = []\n",
    "    current_max = 0.0\n",
    "    for t in study.trials:\n",
    "        if t.value is not None and t.value > current_max:\n",
    "            current_max = t.value\n",
    "        running_max_accuracies.append(current_max)\n",
    "\n",
    "    return study, running_max_accuracies\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Function to save trials data to CSV\n",
    "# -----------------------------\n",
    "def save_study_results_to_csv(study, filename):\n",
    "    \"\"\"\n",
    "    Saves each trial's results into a CSV, including:\n",
    "      - trial number\n",
    "      - objective value (accuracy)\n",
    "      - parameters\n",
    "      - model config parameters\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        row = {\n",
    "            \"trial_number\": t.number,\n",
    "            \"accuracy\": t.value,\n",
    "        }\n",
    "        # Merge in parameter key-value pairs directly\n",
    "        row.update(t.params)\n",
    "\n",
    "        # Add model config if it exists in user attributes\n",
    "        if \"model\" in t.user_attrs:\n",
    "            model_config = t.user_attrs[\"model\"].config.to_dict()\n",
    "            for key, value in model_config.items():\n",
    "                row[f\"config_{key}\"] = value\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(rows)} trials with model configs to {filename}.\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Compare GridSampler vs TPESampler vs RandomSampler\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # For GridSampler, define discrete search space\n",
    "    # grid_sampler = GridSampler(\n",
    "    #    search_space={\n",
    "    #        \"num_layers\": grid_search_space[\"num_layers\"],\n",
    "    #        \"num_heads\": grid_search_space[\"num_heads\"],\n",
    "    #        \"hidden_size\": grid_search_space[\"hidden_size\"],\n",
    "    #        \"intermediate_size\": grid_search_space[\"intermediate_size\"],\n",
    "    #        \"linear_layer_choices\": grid_search_space[\"linear_layer_choices\"],\n",
    "    #    }\n",
    "    # )\n",
    "\n",
    "    tpe_sampler = TPESampler()\n",
    "    # random_sampler = RandomSampler()\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7a. Run with GridSampler\n",
    "    # ---------------------------------------\n",
    "    # grid_study, grid_max_curve = run_study_and_get_curve(\n",
    "    #    sampler=grid_sampler,\n",
    "    #    n_trials=10,  # product of all combos if you set this None or a bigger number\n",
    "    #    study_name=\"bert-grid-study\",\n",
    "    # )\n",
    "    # print(f\"[GridSampler] Number of trials: {len(grid_study.trials)}\")\n",
    "    # print(f\"[GridSampler] Best accuracy: {grid_study.best_value:.4f}\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7b. Run with TPESampler\n",
    "    # ---------------------------------------\n",
    "    tpe_study, tpe_max_curve = run_study_and_get_curve(\n",
    "       sampler=tpe_sampler,\n",
    "       n_trials=10,  # pick more if desired\n",
    "       study_name=\"bert-tpe-study\",\n",
    "    )\n",
    "    print(f\"[TPESampler] Number of trials: {len(tpe_study.trials)}\")\n",
    "    print(f\"[TPESampler] Best accuracy: {tpe_study.best_value:.4f}\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7c. Run with RandomSampler\n",
    "    # ---------------------------------------\n",
    "    # random_study, random_max_curve = run_study_and_get_curve(\n",
    "    #     sampler=random_sampler,\n",
    "    #     n_trials=10,  # pick more if desired\n",
    "    #     study_name=\"bert-random-study\",\n",
    "    # )\n",
    "    # print(f\"[RandomSampler] Number of trials: {len(random_study.trials)}\")\n",
    "    # print(f\"[RandomSampler] Best accuracy: {random_study.best_value:.4f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8. Plot the results\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # plt.plot(range(1, len(grid_max_curve)+1), grid_max_curve, label=\"GridSampler\")\n",
    "    plt.plot(range(1, len(tpe_max_curve)+1), tpe_max_curve, label=\"TPESampler\")\n",
    "    # plt.plot(range(1, len(random_max_curve)+1), random_max_curve, label=\"RandomSampler\")\n",
    "    plt.xlabel(\"Number of Trials\")\n",
    "    plt.ylabel(\"Max Accuracy So Far\")\n",
    "    plt.title(\"Comparison of GridSampler vs TPESampler vs RandomSampler\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 9. (Optional) Save best models\n",
    "    # -----------------------------\n",
    "    # best_grid_model = grid_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    # with open(\"best_grid_model.pkl\", \"wb\") as f:\n",
    "    #     dill.dump(best_grid_model, f)\n",
    "\n",
    "    best_tpe_model = tpe_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    with open(\"best_tpe_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(best_tpe_model, f)\n",
    "\n",
    "    # best_random_model = random_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    # with open(\"best_random_model.pkl\", \"wb\") as f:\n",
    "    #     dill.dump(best_random_model, f)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 10. Save all trials to CSV\n",
    "    # -----------------------------\n",
    "    # save_study_results_to_csv(grid_study, \"grid_study_trials.csv\")\n",
    "    save_study_results_to_csv(tpe_study, \"tpe_study_trials.csv\")\n",
    "    # save_study_results_to_csv(random_study, \"random_study_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 5b Combined ###\n",
    "\n",
    "from chop.tools import get_tokenized_dataset\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_trainer\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "from pathlib import Path\n",
    "import dill\n",
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Update the paramaters in the config\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "    ]:\n",
    "        chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n",
    "        setattr(config, param, search_space[param][chosen_idx])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == nn.Linear:\n",
    "                continue\n",
    "            elif new_layer_cls == Identity:\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Quantize and prune the model\n",
    "    mg = MaseGraph(model)\n",
    "    pipe = CompressionPipeline()\n",
    "\n",
    "    quantization_config = {\n",
    "        \"by\": \"type\",\n",
    "        \"default\": {\n",
    "            \"config\": {\n",
    "                \"name\": None,\n",
    "            }\n",
    "        },\n",
    "        \"linear\": {\n",
    "            \"config\": {\n",
    "                \"name\": \"integer\",\n",
    "                \"data_in_width\": 8,\n",
    "                \"data_in_frac_width\": 4,\n",
    "                \"weight_width\": 8,\n",
    "                \"weight_frac_width\": 4,\n",
    "                \"bias_width\": 8,\n",
    "                \"bias_frac_width\": 4,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    pruning_config = {\n",
    "        \"weight\": {\n",
    "            \"sparsity\": 0.5,\n",
    "            \"method\": \"l1-norm\",\n",
    "            \"scope\": \"local\",\n",
    "        },\n",
    "        \"activation\": {\n",
    "            \"sparsity\": 0.5,\n",
    "            \"method\": \"l1-norm\",\n",
    "            \"scope\": \"local\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    mg, _ = pipe(\n",
    "        mg,\n",
    "        pass_args={\n",
    "            \"quantize_transform_pass\": quantization_config,\n",
    "            \"prune_transform_pass\": pruning_config,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Replace the original model with the compressed model\n",
    "    model = mg.model\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,  # Post-compression training phase\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Set the model as an attribute so we can fetch it later\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "def main_run():\n",
    "    global checkpoint, tokenizer, dataset, search_space\n",
    "\n",
    "    checkpoint = \"prajjwal1/bert-tiny\"\n",
    "    tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "    dataset_name = \"imdb\"\n",
    "\n",
    "    dataset, tokenizer = get_tokenized_dataset(\n",
    "        dataset=dataset_name,\n",
    "        checkpoint=tokenizer_checkpoint,\n",
    "        return_tokenizer=True,\n",
    "    )\n",
    "\n",
    "    search_space = {\n",
    "        \"num_layers\": [2, 4, 8],\n",
    "        \"num_heads\": [2, 4, 8, 16],\n",
    "        \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "        \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "        \"linear_layer_choices\": [\n",
    "            nn.Linear,\n",
    "            Identity,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    sampler = TPESampler()\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"bert-tiny-compression-aware-study\",\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=3,\n",
    "        timeout=60 * 60 * 24,\n",
    "    )\n",
    "\n",
    "    # Save the best model\n",
    "    model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "\n",
    "    with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(model, f)\n",
    "\n",
    "main_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download all csvs ###\n",
    "\n",
    "import glob\n",
    "# from google.colab import files\n",
    "\n",
    "def download_all_csvs():\n",
    "    \"\"\"\n",
    "    Find all CSV files in the directory structure and download them to the local machine.\n",
    "    Handles errors gracefully and provides feedback for each file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all CSV files (searches recursively)\n",
    "        csv_files = glob.glob(\"**/*.csv\", recursive=True)\n",
    "\n",
    "        if not csv_files:\n",
    "            print(\"No CSV files found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "        for file in csv_files:\n",
    "            print(f\"- {file}\")\n",
    "\n",
    "        # Download each CSV file\n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                print(f\"Downloading {file}...\")\n",
    "                files.download(file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {file}: {e}\")\n",
    "\n",
    "        print(\"All CSV files processed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function\n",
    "download_all_csvs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print Forward Info ###\n",
    "\n",
    "import inspect\n",
    "\n",
    "def print_model_forward_info(model, label=\"Model\"):\n",
    "    print(f\"[INFO] === {label} ===\")\n",
    "    print(f\"[INFO] Class: {model.__class__}\")\n",
    "    # Attempt to show the forward signature\n",
    "    try:\n",
    "        sig = inspect.signature(model.forward)\n",
    "        print(f\"[INFO] forward signature: {sig}\")\n",
    "    except ValueError:\n",
    "        print(\"[INFO] Could not retrieve a signature (might be a built-in / GraphModule).\")\n",
    "\n",
    "    # Try printing the docstring if available\n",
    "    doc = inspect.getdoc(model.forward)\n",
    "    print(f\"[INFO] forward docstring:\\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete pass of compression-aware training ###\n",
    "\n",
    "### Part 5b Combined ###\n",
    "import torch\n",
    "from chop.tools import get_tokenized_dataset\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_trainer\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "from pathlib import Path\n",
    "import dill\n",
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "import chop.passes as passes\n",
    "import inspect\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "    \"\"\"\n",
    "    Creates a BERT model with the hyperparameters suggested by the trial.\n",
    "    We manually map each parameter to the correct config attribute or layer override.\n",
    "    \"\"\"\n",
    "    # Load the default config from a small BERT checkpoint\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Map parameters to config\n",
    "    config.num_hidden_layers = trial.suggest_categorical(\n",
    "        \"num_layers\", search_space[\"num_layers\"]\n",
    "    )\n",
    "    config.num_attention_heads = trial.suggest_categorical(\n",
    "        \"num_heads\", search_space[\"num_heads\"]\n",
    "    )\n",
    "    config.hidden_size = trial.suggest_categorical(\n",
    "        \"hidden_size\", search_space[\"hidden_size\"]\n",
    "    )\n",
    "    config.intermediate_size = trial.suggest_categorical(\n",
    "        \"intermediate_size\", search_space[\"intermediate_size\"]\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Handle linear layer choice\n",
    "    linear_choice = trial.suggest_categorical(\n",
    "        \"linear_layer_choices\", search_space[\"linear_layer_choices\"]\n",
    "    )\n",
    "    if linear_choice == \"identity\":\n",
    "        # For each linear layer that is square (in_features == out_features),\n",
    "        # replace it with Identity\n",
    "        for name, layer in model.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "                deepsetattr(model, name, Identity())\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the model and ensure it is on the correct device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Model is being initialized on: {device}\")\n",
    "\n",
    "    model = construct_model(trial).to(device)\n",
    "    print(f\"[INFO] Model moved to: {next(model.parameters()).device}\")\n",
    "\n",
    "    original_forward = model.forward\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # Set up the trainer\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Starting initial training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Move model to CPU after training\n",
    "    model.cpu()\n",
    "    print(f\"[INFO] Model moved to: {next(model.parameters()).device} after training\")\n",
    "\n",
    "    # print_model_forward_info(model, label=\"Model BEFORE compression\")\n",
    "\n",
    "    # MaseGraph processing\n",
    "    # mg = MaseGraph(model)  # Create MaseGraph on the same device\n",
    "    mg = MaseGraph(\n",
    "        model,\n",
    "        hf_input_names=[\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"labels\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    mg, _ = passes.init_metadata_analysis_pass(mg)\n",
    "    mg, _ = passes.add_common_metadata_analysis_pass(mg)\n",
    "    pipe = CompressionPipeline()\n",
    "\n",
    "    quantization_config = {\n",
    "        \"by\": \"type\",\n",
    "        \"default\": {\"config\": {\"name\": None}},\n",
    "        \"linear\": {\n",
    "            \"config\": {\n",
    "                \"name\": \"integer\",\n",
    "                \"data_in_width\": 8,\n",
    "                \"data_in_frac_width\": 4,\n",
    "                \"weight_width\": 8,\n",
    "                \"weight_frac_width\": 4,\n",
    "                \"bias_width\": 8,\n",
    "                \"bias_frac_width\": 4,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    pruning_config = {\n",
    "        \"weight\": {\"sparsity\": 0.5, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "        \"activation\": {\"sparsity\": 0.5, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "    }\n",
    "\n",
    "    print(\"[INFO] Applying compression pipeline (quantization & pruning)...\")\n",
    "    mg, _ = pipe(\n",
    "        mg,\n",
    "        pass_args={\n",
    "            \"quantize_transform_pass\": quantization_config,\n",
    "            \"prune_transform_pass\": pruning_config,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    compressed_model = mg.model.to(device)\n",
    "    print(f\"[INFO] Compressed model class: {compressed_model.__class__}\")\n",
    "    print(f\"[INFO] Compressed model moved to: {next(compressed_model.parameters()).device}\")\n",
    "\n",
    "    print(\"[INFO] Reassigning original forward to the compressed model...\")\n",
    "    # compressed_model.forward = original_forward\n",
    "\n",
    "    print_model_forward_info(compressed_model, label=\"Model AFTER compression\")\n",
    "\n",
    "    # Continue training the compressed model\n",
    "    trainer = get_trainer(\n",
    "        model=compressed_model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,  # Post-compression training phase\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Starting post-compression training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the final model\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"[INFO] Final evaluation accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "    # Set the model as an attribute for later use\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "def main_run():\n",
    "    global checkpoint, tokenizer, dataset, search_space\n",
    "\n",
    "    checkpoint = \"prajjwal1/bert-tiny\"\n",
    "    tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "    dataset_name = \"imdb\"\n",
    "\n",
    "    dataset, tokenizer = get_tokenized_dataset(\n",
    "        dataset=dataset_name,\n",
    "        checkpoint=tokenizer_checkpoint,\n",
    "        return_tokenizer=True,\n",
    "    )\n",
    "\n",
    "    sampler = TPESampler()\n",
    "\n",
    "    # search_space = {\n",
    "    #     \"num_layers\": [2, 4, 8],\n",
    "    #     \"num_heads\": [2, 4, 8, 16],\n",
    "    #     \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    #     \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    #     \"linear_layer_choices\": [\n",
    "    #         nn.Linear,\n",
    "    #         Identity,\n",
    "    #     ],\n",
    "    # }\n",
    "\n",
    "    search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "    }\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"bert-tiny-compression-aware-study\",\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    print('Reached!')\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=3,\n",
    "        timeout=60 * 60 * 24,\n",
    "    )\n",
    "\n",
    "    # Save the best model\n",
    "    try:\n",
    "        model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    except AttributeError:  # If `.cpu()` fails, fetch without it\n",
    "        model = study.best_trial.user_attrs[\"model\"]\n",
    "\n",
    "\n",
    "    with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(model, f)\n",
    "\n",
    "main_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 5b Combined ###\n",
    "\n",
    "import torch\n",
    "from chop.tools import get_tokenized_dataset\n",
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_trainer\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from pathlib import Path\n",
    "import dill\n",
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "import chop.passes as passes\n",
    "import inspect\n",
    "import csv\n",
    "\n",
    "def construct_model(trial):\n",
    "    \"\"\"\n",
    "    Creates a BERT model with the hyperparameters suggested by the trial.\n",
    "    We manually map each parameter to the correct config attribute or layer override.\n",
    "    \"\"\"\n",
    "    # Load the default config from a small BERT checkpoint\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Map parameters to config\n",
    "    config.num_hidden_layers = trial.suggest_categorical(\n",
    "        \"num_layers\", search_space[\"num_layers\"]\n",
    "    )\n",
    "    config.num_attention_heads = trial.suggest_categorical(\n",
    "        \"num_heads\", search_space[\"num_heads\"]\n",
    "    )\n",
    "    config.hidden_size = trial.suggest_categorical(\n",
    "        \"hidden_size\", search_space[\"hidden_size\"]\n",
    "    )\n",
    "    config.intermediate_size = trial.suggest_categorical(\n",
    "        \"intermediate_size\", search_space[\"intermediate_size\"]\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Handle linear layer choice\n",
    "    linear_choice = trial.suggest_categorical(\n",
    "        \"linear_layer_choices\", search_space[\"linear_layer_choices\"]\n",
    "    )\n",
    "    if linear_choice == \"identity\":\n",
    "        # For each linear layer that is square (in_features == out_features),\n",
    "        # replace it with Identity\n",
    "        for name, layer in model.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "                deepsetattr(model, name, Identity())\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the model and ensure it is on the correct device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Model is being initialized on: {device}\")\n",
    "\n",
    "    model = construct_model(trial).to(device)\n",
    "    print(f\"[INFO] Model moved to: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Keep a copy of the original forward if needed later\n",
    "    original_forward = model.forward\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    # === 1) Training Only (No Compression) ===\n",
    "    print(\"[INFO] Starting initial training (no compression)...\")\n",
    "    trainer_no_comp = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=3,\n",
    "    )\n",
    "    trainer_no_comp.train()\n",
    "    eval_results_no_comp = trainer_no_comp.evaluate()\n",
    "    no_comp_acc = eval_results_no_comp[\"eval_accuracy\"]\n",
    "    print(f\"[INFO] Accuracy (no compression): {no_comp_acc:.4f}\")\n",
    "\n",
    "    # Move model to CPU after initial training if you prefer\n",
    "    model.cpu()\n",
    "    print(f\"[INFO] Model moved to: {next(model.parameters()).device} after initial training\")\n",
    "\n",
    "    # === 2) Apply Compression, Evaluate with No Additional Training ===\n",
    "    print(\"[INFO] Building MaseGraph and applying compression pipeline...\")\n",
    "    mg = MaseGraph(\n",
    "        model,\n",
    "        hf_input_names=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    )\n",
    "    mg, _ = passes.init_metadata_analysis_pass(mg)\n",
    "    mg, _ = passes.add_common_metadata_analysis_pass(mg)\n",
    "\n",
    "    pipe = CompressionPipeline()\n",
    "    quantization_config = {\n",
    "        \"by\": \"type\",\n",
    "        \"default\": {\"config\": {\"name\": None}},\n",
    "        \"linear\": {\n",
    "            \"config\": {\n",
    "                \"name\": \"integer\",\n",
    "                \"data_in_width\": 8,\n",
    "                \"data_in_frac_width\": 4,\n",
    "                \"weight_width\": 8,\n",
    "                \"weight_frac_width\": 4,\n",
    "                \"bias_width\": 8,\n",
    "                \"bias_frac_width\": 4,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    pruning_config = {\n",
    "        \"weight\": {\"sparsity\": 0.5, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "        \"activation\": {\"sparsity\": 0.5, \"method\": \"l1-norm\", \"scope\": \"local\"},\n",
    "    }\n",
    "\n",
    "    mg, _ = pipe(\n",
    "        mg,\n",
    "        pass_args={\n",
    "            \"quantize_transform_pass\": quantization_config,\n",
    "            \"prune_transform_pass\": pruning_config,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    compressed_model = mg.model.to(device)\n",
    "    print(f\"[INFO] Compressed model moved to: {next(compressed_model.parameters()).device}\")\n",
    "\n",
    "    print(\"[INFO] Evaluating compression result (no post-training)...\")\n",
    "    trainer_comp_no_post = get_trainer(\n",
    "        model=compressed_model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,  # We'll just create a trainer; not calling train() means no extra training\n",
    "    )\n",
    "    # Just evaluate directly (no further training call):\n",
    "    eval_results_no_post = trainer_comp_no_post.evaluate()\n",
    "    compression_no_post_acc = eval_results_no_post[\"eval_accuracy\"]\n",
    "    print(f\"[INFO] Accuracy (compression, no post-training): {compression_no_post_acc:.4f}\")\n",
    "\n",
    "    # === 3) Continue Training the Compressed Model (Post-Compression Training) ===\n",
    "    print(\"[INFO] Starting post-compression training...\")\n",
    "    trainer_comp_post = get_trainer(\n",
    "        model=compressed_model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=3,  # Post-compression training phase\n",
    "    )\n",
    "    trainer_comp_post.train()\n",
    "    eval_results_post_train = trainer_comp_post.evaluate()\n",
    "    compression_post_acc = eval_results_post_train[\"eval_accuracy\"]\n",
    "    print(f\"[INFO] Accuracy (compression + post-training): {compression_post_acc:.4f}\")\n",
    "\n",
    "    # Set the model as an attribute for later use\n",
    "    # (the final compressed & post-trained model)\n",
    "    trial.set_user_attr(\"model\", compressed_model)\n",
    "\n",
    "    # Store scenario accuracies for CSV logging\n",
    "    trial.set_user_attr(\"no_compression_acc\", no_comp_acc)\n",
    "    trial.set_user_attr(\"compression_no_post_acc\", compression_no_post_acc)\n",
    "    trial.set_user_attr(\"compression_post_acc\", compression_post_acc)\n",
    "\n",
    "    # Return the final accuracy after compression + post-training\n",
    "    return compression_post_acc\n",
    "\n",
    "def main_run():\n",
    "    global checkpoint, tokenizer, dataset, search_space\n",
    "\n",
    "    checkpoint = \"prajjwal1/bert-tiny\"\n",
    "    tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "    dataset_name = \"imdb\"\n",
    "\n",
    "    dataset, tokenizer = get_tokenized_dataset(\n",
    "        dataset=dataset_name,\n",
    "        checkpoint=tokenizer_checkpoint,\n",
    "        return_tokenizer=True,\n",
    "    )\n",
    "\n",
    "    sampler = TPESampler()\n",
    "\n",
    "    search_space = {\n",
    "        \"num_layers\": [2, 4, 8],\n",
    "        \"num_heads\": [2, 4, 8, 16],\n",
    "        \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "        \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "        \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "    }\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"bert-tiny-compression-aware-study\",\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Optimisation started...\")\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=10,       # Adjust as needed\n",
    "        timeout=60 * 60 * 24,  # One hour; adjust as needed\n",
    "    )\n",
    "\n",
    "    # Save the best model\n",
    "    try:\n",
    "        best_model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    except AttributeError:\n",
    "        best_model = study.best_trial.user_attrs[\"model\"]\n",
    "\n",
    "    with open(f\"{Path.home()}/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(best_model, f)\n",
    "\n",
    "    # === Write the results of all trials into a CSV file ===\n",
    "    results_path = \"results.csv\"  # Will be saved in your Colab working directory\n",
    "    with open(results_path, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"trial_number\", \"no_compression_acc\", \"compression_no_post_acc\", \"compression_post_acc\"])\n",
    "        for t in study.trials:\n",
    "            writer.writerow([\n",
    "                t.number,\n",
    "                t.user_attrs.get(\"no_compression_acc\", None),\n",
    "                t.user_attrs.get(\"compression_no_post_acc\", None),\n",
    "                t.user_attrs.get(\"compression_post_acc\", None),\n",
    "            ])\n",
    "\n",
    "    print(f\"[INFO] Results written to {results_path}\")\n",
    "\n",
    "main_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_tokenized_dataset, get_trainer\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler, TPESampler, RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define your checkpoints & dataset\n",
    "# -----------------------------\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# Load tokenized dataset and tokenizer\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define search space\n",
    "# -----------------------------\n",
    "grid_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "# Same discrete sets for TPE/Random:\n",
    "tpe_search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\"linear\", \"identity\"],\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Construct the model given a trial\n",
    "# -----------------------------\n",
    "def construct_model(trial):\n",
    "    \"\"\"\n",
    "    Creates a BERT model with the hyperparameters suggested by the trial.\n",
    "    We manually map each parameter to the correct config attribute or layer override.\n",
    "    \"\"\"\n",
    "    # Load the default config from a small BERT checkpoint\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Map parameters to config\n",
    "    config.num_hidden_layers = trial.suggest_categorical(\n",
    "        \"num_layers\", tpe_search_space[\"num_layers\"]\n",
    "    )\n",
    "    config.num_attention_heads = trial.suggest_categorical(\n",
    "        \"num_heads\", tpe_search_space[\"num_heads\"]\n",
    "    )\n",
    "    config.hidden_size = trial.suggest_categorical(\n",
    "        \"hidden_size\", tpe_search_space[\"hidden_size\"]\n",
    "    )\n",
    "    config.intermediate_size = trial.suggest_categorical(\n",
    "        \"intermediate_size\", tpe_search_space[\"intermediate_size\"]\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Handle linear layer choice\n",
    "    linear_choice = trial.suggest_categorical(\n",
    "        \"linear_layer_choices\", tpe_search_space[\"linear_layer_choices\"]\n",
    "    )\n",
    "    if linear_choice == \"identity\":\n",
    "        # For each linear layer that is square (in_features == out_features),\n",
    "        # replace it with Identity\n",
    "        for name, layer in model.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "                deepsetattr(model, name, Identity())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define the objective function\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    "    # Build the model given the trial\n",
    "    trial_model = construct_model(trial)\n",
    "\n",
    "    # Create a Trainer (from chop) that handles fine-tuning\n",
    "    trainer = get_trainer(\n",
    "        model=trial_model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,  # For demonstration only\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Save the model to the trial user attributes for later retrieval if needed\n",
    "    trial.set_user_attr(\"model\", trial_model)\n",
    "\n",
    "    # Return the metric you want to optimize\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Helper to run a study and collect \"running best accuracies\"\n",
    "# -----------------------------\n",
    "def run_study_and_get_curve(sampler, n_trials=None, study_name=\"study\"):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study with the provided sampler and returns:\n",
    "      - the study object\n",
    "      - a list of best accuracies up to each trial (running max)\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=study_name,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        timeout=60 * 60 * 24,  # or specify a time limit\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    # Build a list of max accuracies up to each trial\n",
    "    running_max_accuracies = []\n",
    "    current_max = 0.0\n",
    "    for t in study.trials:\n",
    "        if t.value is not None and t.value > current_max:\n",
    "            current_max = t.value\n",
    "        running_max_accuracies.append(current_max)\n",
    "\n",
    "    return study, running_max_accuracies\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Function to save trials data to CSV\n",
    "# -----------------------------\n",
    "def save_study_results_to_csv(study, filename):\n",
    "    \"\"\"\n",
    "    Saves each trial's results into a CSV, including:\n",
    "      - trial number\n",
    "      - objective value (accuracy)\n",
    "      - parameters\n",
    "      - model config parameters\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        row = {\n",
    "            \"trial_number\": t.number,\n",
    "            \"accuracy\": t.value,\n",
    "        }\n",
    "        # Merge in parameter key-value pairs directly\n",
    "        row.update(t.params)\n",
    "\n",
    "        # Add model config if it exists in user attributes\n",
    "        if \"model\" in t.user_attrs:\n",
    "            model_config = t.user_attrs[\"model\"].config.to_dict()\n",
    "            for key, value in model_config.items():\n",
    "                row[f\"config_{key}\"] = value\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(rows)} trials with model configs to {filename}.\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Compare GridSampler vs TPESampler vs RandomSampler\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # For GridSampler, define discrete search space\n",
    "    #grid_sampler = GridSampler(\n",
    "    #    search_space={\n",
    "    #        \"num_layers\": grid_search_space[\"num_layers\"],\n",
    "    #        \"num_heads\": grid_search_space[\"num_heads\"],\n",
    "    #        \"hidden_size\": grid_search_space[\"hidden_size\"],\n",
    "    #        \"intermediate_size\": grid_search_space[\"intermediate_size\"],\n",
    "    #        \"linear_layer_choices\": grid_search_space[\"linear_layer_choices\"],\n",
    "    #    }\n",
    "    #)\n",
    "\n",
    "    #tpe_sampler = TPESampler()\n",
    "    random_sampler = RandomSampler()\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7a. Run with GridSampler\n",
    "    # ---------------------------------------\n",
    "    #grid_study, grid_max_curve = run_study_and_get_curve(\n",
    "    #    sampler=grid_sampler,\n",
    "    #    n_trials=10,  # product of all combos if you set this None or a bigger number\n",
    "    #    study_name=\"bert-grid-study\",\n",
    "    #)\n",
    "    #print(f\"[GridSampler] Number of trials: {len(grid_study.trials)}\")\n",
    "    #print(f\"[GridSampler] Best accuracy: {grid_study.best_value:.4f}\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7b. Run with TPESampler\n",
    "    # ---------------------------------------\n",
    "    #tpe_study, tpe_max_curve = run_study_and_get_curve(\n",
    "    #    sampler=tpe_sampler,\n",
    "    #    n_trials=10,  # pick more if desired\n",
    "    #    study_name=\"bert-tpe-study\",\n",
    "    #)\n",
    "    #print(f\"[TPESampler] Number of trials: {len(tpe_study.trials)}\")\n",
    "    #print(f\"[TPESampler] Best accuracy: {tpe_study.best_value:.4f}\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 7c. Run with RandomSampler\n",
    "    # ---------------------------------------\n",
    "    random_study, random_max_curve = run_study_and_get_curve(\n",
    "        sampler=random_sampler,\n",
    "        n_trials=10,  # pick more if desired\n",
    "        study_name=\"bert-random-study\",\n",
    "    )\n",
    "    print(f\"[RandomSampler] Number of trials: {len(random_study.trials)}\")\n",
    "    print(f\"[RandomSampler] Best accuracy: {random_study.best_value:.4f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 8. Plot the results\n",
    "    # -----------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    #plt.plot(range(1, len(grid_max_curve)+1), grid_max_curve, label=\"GridSampler\")\n",
    "    #plt.plot(range(1, len(tpe_max_curve)+1), tpe_max_curve, label=\"TPESampler\")\n",
    "    plt.plot(range(1, len(random_max_curve)+1), random_max_curve, label=\"RandomSampler\")\n",
    "    plt.xlabel(\"Number of Trials\")\n",
    "    plt.ylabel(\"Max Accuracy So Far\")\n",
    "    plt.title(\"Comparison of GridSampler vs TPESampler vs RandomSampler\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 9. (Optional) Save best models\n",
    "    # -----------------------------\n",
    "    #best_grid_model = grid_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    #with open(\"best_grid_model.pkl\", \"wb\") as f:\n",
    "        #dill.dump(best_grid_model, f)\n",
    "\n",
    "    #best_tpe_model = tpe_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    #with open(\"best_tpe_model.pkl\", \"wb\") as f:\n",
    "        #dill.dump(best_tpe_model, f)\n",
    "\n",
    "    best_random_model = random_study.best_trial.user_attrs[\"model\"].cpu()\n",
    "    with open(\"best_random_model.pkl\", \"wb\") as f:\n",
    "        dill.dump(best_random_model, f)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 10. Save all trials to CSV\n",
    "    # -----------------------------\n",
    "    #save_study_results_to_csv(grid_study, \"grid_study_trials.csv\")\n",
    "    #save_study_results_to_csv(tpe_study, \"tpe_study_trials.csv\")\n",
    "    save_study_results_to_csv(random_study, \"random_study_trials.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
