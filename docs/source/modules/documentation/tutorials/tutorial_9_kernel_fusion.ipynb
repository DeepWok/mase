{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9: Running Kernel Fusion for Inference Acceleration on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will look at how to perform kernel fusion using Triton.\n",
    "\n",
    "## What is a Custom Kernel?\n",
    "The idea of a kernel in the GPU world is a function that is parallelized and running on the GPU fabric. This is slightly different from the kernel in the context of operating systems. A custom GPU kernel is a kernel that is not provided by the GPU vendor, such as Nvidia, but it is written by developers. We develop custom kernels to optimize the performance of our application.\n",
    "\n",
    "## What is Kernel Fusion?\n",
    "Kernel fusion refers to the idea of combining instructions from multiple kernels into a single kernel. This has multiple benefits:\n",
    "1. Reducing the number of kernel launches, which can be expensive.\n",
    "2. If designed correctly, it can reduce the number of memory accesses.\n",
    "3. If designed correctly, it can reduce the number of intermediate results stored in memory, which can be expensive.\n",
    "\n",
    "## What is Triton?\n",
    "When prioritizing performance optimization on GPUs, programmability becomes a critical challenge, especially for managing HBM (High-bandwidth Memory) load and store operations and choosing the right level of parallelism. High-level languages or frameworks like PyTorch lack the constructs necessary for such control. \n",
    "\n",
    "Typically, the workaround involves delving into Nvidia's lower-level programming languages like CUDA or utilizing libraries like CUTLASS for tensor operations. Triton is then standing on a unique middle ground, bridging the gap between Python's accessibility and ease of programmability and CUDA's fine-grained control.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# naive softmax\n",
    "def torch_naive_softmax(x):\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z)\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None]\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtl\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m driver\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "        output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "        num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 8\n",
    "\n",
    "    # Number of software pipelining stages.\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # pre-compile kernel to get register usage and compute thread occupancy.\n",
    "    kernel = softmax_kernel.warmup(\n",
    "        y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "        num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    size_smem = kernel.metadata.shared\n",
    "    if is_hip():\n",
    "        # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "        # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "        # ISA SECTION (3.6.4 for CDNA3)\n",
    "        # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "        # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "        # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "        # not required to be equal numbers of both types.\n",
    "        if is_cdna():\n",
    "            NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "        # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "        # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "        # execute on a CU (multi-processor)  in parallel.\n",
    "        MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "        max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "        occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "    else:\n",
    "        occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    num_programs = NUM_SM * occupancy\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    # Create a number of persistent programs.\n",
    "    kernel[(num_programs, 1, 1)](\n",
    "        y,\n",
    "        x,\n",
    "        x.stride(0),\n",
    "        y.stride(0),\n",
    "        n_rows,\n",
    "        n_cols,\n",
    "    )\n",
    "    return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
