{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome To the TensorRT Quantization Tutorial!\n",
    "\n",
    "This notebook is designed to show the features of the TensorRT passes integrated into MASE.\n",
    "\n",
    "## Section 1. INT8 Quantization\n",
    "Firstly, we will show you how to do a INT8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model using the `Machop API`. The quantization process is split into the following stages, each using their own individual pass, and are explained in depth at each subsection:\n",
    "\n",
    "1. [Fake quantization](#section-11-fake-quantization): `tensorrt_fake_quantize_transform_pass`\n",
    "2. [Calibration](#sect): `tensorrt_calibrate_transform_pass`\n",
    "3. [Quantized Aware Training](#quantized-aware-training): `tensorrt_fine_tune_transform_pass`\n",
    "4. [Quantization](#quantization): `tensorrt_engine_interface_pass`\n",
    "5. [Analysis](#analysis): `tensorrt_analysis_pass`\n",
    "\n",
    "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-17 13:47:19,104] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0317 13:47:21.085121 140617952859968 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import toml\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "# Figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "# Add directory to the PATH so that chop can be called\n",
    "new_path = \"../../../machop\"\n",
    "full_path = os.path.abspath(new_path)\n",
    "os.environ['PATH'] += os.pathsep + full_path\n",
    "\n",
    "from chop.tools.utils import to_numpy_if_tensor\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "from chop.models import get_model_info, get_model\n",
    "from chop.dataset import MaseDataModule\n",
    "from chop.passes.graph.transforms import metadata_value_type_cast_transform_pass\n",
    "from chop.passes.graph import (\n",
    "    summarize_quantization_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    tensorrt_analysis_pass,\n",
    "    )\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml), or read the documentation on Mase [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your TOML file\n",
    "toml_file_path = '../../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(toml_file_path, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt_quantize' section and its children\n",
    "tensorrt_quantize_config = pass_args.get('passes', {}).get('tensorrt_quantize', {})\n",
    "# Extract the 'passes.tensorrt_fine_tune' section and its children\n",
    "tensorrt_train_config = pass_args.get('passes', {}).get('tensorrt_fine_tune', {})\n",
    "# Extract the 'passes.tensorrt_analysis' section and its children\n",
    "tensorrt_analysis_config = pass_args.get('passes', {}).get('tensorrt_analysis', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MaseGraph` by loading in a pre-trained model using the checkpoint provided and using the toml configuration model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\u001b[0m\n",
      "I0317 13:47:26.846760 140617952859968 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "# Load the basics in to \n",
    "model_name = pass_args['model']\n",
    "dataset_name = pass_args['dataset']\n",
    "max_epochs = pass_args['max_epochs']\n",
    "batch_size = pass_args['batch_size']\n",
    "learning_rate = pass_args['learning_rate']\n",
    "accelerator = pass_args['accelerator']\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "mg, _ = metadata_value_type_cast_transform_pass(mg, pass_args={\"fn\": to_numpy_if_tensor})\n",
    "\n",
    "# Before we begin, we will copy the original MaseGraph model to use for comparison during quantization analysis\n",
    "mg_original = deepcopy_mase_graph(mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1 Fake Quantization\n",
    "\n",
    "Firstly, we fake quantize the module in order to perform calibration and fine tuning before actually quantizing - this is only used if we have INT8 calibration as other precisions are not currently supported within [pytorch-quantization](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#) library.\n",
    "\n",
    "This is acheived through the `tensorrt_fake_quantize_transform_pass` which goes through the model, either by type or by name, replaces each layer appropriately to a fake quantized form if the `quantize` parameter is set in the default config (`passes.tensorrt_quantize.default.config`) or on a per name or type basis. I.e. if we would like to only quantize the linear layers but not the convolutional layers, we could set the `quantize` parameter to true for `passes.tensorrt_quantize.linear.config` and the default to false.\n",
    "\n",
    "Currently the quantizable layers are:\n",
    "- Linear\n",
    "- Conv1d, Conv2d, ConvNd \n",
    "- ConvTranspose1d, ConvTranspose2d, ConvTransposeNd \n",
    "- MaxPool1d, MaxPool2d, MaxPool3d\n",
    "- AvgPool1d, AvgPool2d, AvgPool3d\n",
    "- Clip (Tensor)\n",
    "- LSTM, LSTMCell\n",
    "\n",
    "To create a custom quantized module, click [here](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#document-tutorials/creating_custom_quantized_modules).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0317 13:47:27.066725 140617952859968 utils.py:166] Applying fake quantization to PyTorch model...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0317 13:47:27.359983 140617952859968 utils.py:182] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "I0317 13:47:27.378832 140617952859968 summary.py:84] Quantized graph histogram:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| ReLU            | relu         |       4 |         0 |           4 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
      "I0317 13:47:27.382598 140617952859968 summary.py:85] \n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| ReLU            | relu         |       4 |         0 |           4 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\n"
     ]
    }
   ],
   "source": [
    "configs = [tensorrt_quantize_config, tensorrt_train_config, tensorrt_analysis_config]\n",
    "for config in configs:\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "summarize_quantization_analysis_pass(mg_original, mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have succesfully quantized all linear layers inside `jsc-toy`. See Section X for how to apply quantization layerwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2 Calibration\n",
    "\n",
    "Next, we perform calibration using the `tensorrt_calibrate_transform_pass`. Calibration is achieved by passing data samples to the quantizer and deciding the best amax for activations. \n",
    "\n",
    "Calibrators can be added as a search space parameter to examine the best performing calibrator. The calibrators have been included in the toml as follows.\n",
    "For example: `calibrators = [\"percentile\", \"mse\", \"entropy\"]`\n",
    "\n",
    "Note: \n",
    "- To use `percentile` calibration, a list of percentiles must be given\n",
    "- To use `max` calibration, the `histogram` weight and input calibrators must be removed and replaced with `max`. This will use global maximum absolute value to calibrate the model.\n",
    "- If `post_calibration_analysis` is set true the `tensorrt_analysis_pass` will be run for each calibrator tested to evaluate the most suitable calibrator for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0317 13:47:27.396504 140617952859968 calibrate.py:84] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 13:47:27.401233 140617952859968 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 13:47:27.402549 140617952859968 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 13:47:27.403945 140617952859968 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 13:47:27.405315 140617952859968 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 13:47:27.406633 140617952859968 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 13:47:27.407929 140617952859968 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 13:47:28.763406 140617952859968 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 13:47:28.765608 140617952859968 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 13:47:28.766356 140617952859968 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 13:47:28.769678 140617952859968 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 13:47:28.770352 140617952859968 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 13:47:28.772021 140617952859968 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 13:47:28.773056 140617952859968 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 13:47:28.774536 140617952859968 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 13:47:28.775284 140617952859968 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 13:47:28.776809 140617952859968 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 13:47:28.777737 140617952859968 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 13:47:28.779728 140617952859968 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0317 13:47:28.785115 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0317 13:47:28.786056 140617952859968 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0392 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:28.787508 140617952859968 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0392 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:28.789955 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7247 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:28.790810 140617952859968 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7247 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:28.793244 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4372 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:28.794053 140617952859968 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4372 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:28.796450 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:28.797263 140617952859968 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:28.799667 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7383 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:28.800466 140617952859968 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7383 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:28.802114 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:28.802638 140617952859968 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "I0317 13:47:28.804714 140617952859968 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.0...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 13:47:28.805786 140617952859968 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |    0.7226    |\n",
      "|      Average Precision       |   0.73513    |\n",
      "|        Average Recall        |   0.72076    |\n",
      "|       Average F1 Score       |   0.72351    |\n",
      "|         Average Loss         |   0.79905    |\n",
      "|       Average Latency        |  3.4763 ms   |\n",
      "|   Average GPU Power Usage    |   64.749 W   |\n",
      "| Inference Energy Consumption | 0.062524 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 13:47:29.768193 140617952859968 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |    0.7226    |\n",
      "|      Average Precision       |   0.73513    |\n",
      "|        Average Recall        |   0.72076    |\n",
      "|       Average F1 Score       |   0.72351    |\n",
      "|         Average Loss         |   0.79905    |\n",
      "|       Average Latency        |  3.4763 ms   |\n",
      "|   Average GPU Power Usage    |   64.749 W   |\n",
      "| Inference Energy Consumption | 0.062524 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 13:47:29.770939 140617952859968 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 13:47:29.773039 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.2036 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:29.774172 140617952859968 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.2036 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:29.776292 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:29.777585 140617952859968 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:29.780272 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.8232 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:29.781401 140617952859968 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.8232 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:29.783609 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:29.784756 140617952859968 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:29.787020 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.3199 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:29.788119 140617952859968 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.3199 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:29.790309 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:29.791360 140617952859968 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "I0317 13:47:29.793754 140617952859968 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.9...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 13:47:29.795233 140617952859968 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+------------+\n",
      "|            Metric            |   Value    |\n",
      "+------------------------------+------------+\n",
      "| Average Validation Accuracy  |  0.73373   |\n",
      "|      Average Precision       |  0.74673   |\n",
      "|        Average Recall        |  0.73191   |\n",
      "|       Average F1 Score       |  0.73521   |\n",
      "|         Average Loss         |   0.7644   |\n",
      "|       Average Latency        | 3.4635 ms  |\n",
      "|   Average GPU Power Usage    |  64.755 W  |\n",
      "| Inference Energy Consumption | 0.0623 mWh |\n",
      "+------------------------------+------------+\u001b[0m\n",
      "I0317 13:47:30.727840 140617952859968 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+------------+\n",
      "|            Metric            |   Value    |\n",
      "+------------------------------+------------+\n",
      "| Average Validation Accuracy  |  0.73373   |\n",
      "|      Average Precision       |  0.74673   |\n",
      "|        Average Recall        |  0.73191   |\n",
      "|       Average F1 Score       |  0.73521   |\n",
      "|         Average Loss         |   0.7644   |\n",
      "|       Average Latency        | 3.4635 ms  |\n",
      "|   Average GPU Power Usage    |  64.755 W  |\n",
      "| Inference Energy Consumption | 0.0623 mWh |\n",
      "+------------------------------+------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 13:47:30.732584 140617952859968 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 13:47:30.735238 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.9675 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:30.736667 140617952859968 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.9675 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:30.739023 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:30.739757 140617952859968 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:30.741493 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.2396 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:30.742228 140617952859968 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.2396 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:30.743767 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:30.744584 140617952859968 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:30.746204 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.7691 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:30.746989 140617952859968 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.7691 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:30.748558 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:30.749436 140617952859968 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "I0317 13:47:30.751144 140617952859968 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.99...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 13:47:30.752208 140617952859968 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |    0.7346    |\n",
      "|      Average Precision       |   0.74806    |\n",
      "|        Average Recall        |   0.73285    |\n",
      "|       Average F1 Score       |   0.73626    |\n",
      "|         Average Loss         |    0.7595    |\n",
      "|       Average Latency        |  3.5015 ms   |\n",
      "|   Average GPU Power Usage    |   64.74 W    |\n",
      "| Inference Energy Consumption | 0.062969 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 13:47:32.032355 140617952859968 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |    0.7346    |\n",
      "|      Average Precision       |   0.74806    |\n",
      "|        Average Recall        |   0.73285    |\n",
      "|       Average F1 Score       |   0.73626    |\n",
      "|         Average Loss         |    0.7595    |\n",
      "|       Average Latency        |  3.5015 ms   |\n",
      "|   Average GPU Power Usage    |   64.74 W    |\n",
      "| Inference Energy Consumption | 0.062969 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 13:47:32.035518 140617952859968 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 13:47:33.599452 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.9014 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:33.601526 140617952859968 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.9014 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:35.026550 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:35.028527 140617952859968 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:36.677643 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.0598 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:36.679901 140617952859968 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.0598 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:38.109362 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:38.111028 140617952859968 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:39.647496 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.7481 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:39.648947 140617952859968 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.7481 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:41.121448 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:41.122817 140617952859968 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "I0317 13:47:41.125805 140617952859968 calibrate.py:53] Performing post calibration analysis for calibrator mse...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 13:47:41.127109 140617952859968 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73481    |\n",
      "|      Average Precision       |   0.74776    |\n",
      "|        Average Recall        |   0.73298    |\n",
      "|       Average F1 Score       |   0.73624    |\n",
      "|         Average Loss         |   0.76088    |\n",
      "|       Average Latency        |  3.7446 ms   |\n",
      "|   Average GPU Power Usage    |   65.835 W   |\n",
      "| Inference Energy Consumption | 0.068479 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 13:47:42.146633 140617952859968 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73481    |\n",
      "|      Average Precision       |   0.74776    |\n",
      "|        Average Recall        |   0.73298    |\n",
      "|       Average F1 Score       |   0.73624    |\n",
      "|         Average Loss         |   0.76088    |\n",
      "|       Average Latency        |  3.7446 ms   |\n",
      "|   Average GPU Power Usage    |   65.835 W   |\n",
      "| Inference Energy Consumption | 0.068479 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 13:47:42.149724 140617952859968 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 13:47:48.956993 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.2190 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:48.959841 140617952859968 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.2190 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:47:53.209888 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:47:53.211752 140617952859968 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:48:00.605459 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.6656 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:48:00.608213 140617952859968 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.6656 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:48:04.603729 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:48:04.605121 140617952859968 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:48:11.357201 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.8448 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:48:11.359984 140617952859968 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.8448 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 13:48:15.292741 140617952859968 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 13:48:15.294555 140617952859968 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "I0317 13:48:15.297659 140617952859968 calibrate.py:53] Performing post calibration analysis for calibrator entropy...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 13:48:15.299291 140617952859968 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73451    |\n",
      "|      Average Precision       |   0.74767    |\n",
      "|        Average Recall        |   0.73269    |\n",
      "|       Average F1 Score       |   0.73608    |\n",
      "|         Average Loss         |   0.76169    |\n",
      "|       Average Latency        |  3.9805 ms   |\n",
      "|   Average GPU Power Usage    |   66.442 W   |\n",
      "| Inference Energy Consumption | 0.073464 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 13:48:16.424930 140617952859968 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73451    |\n",
      "|      Average Precision       |   0.74767    |\n",
      "|        Average Recall        |   0.73269    |\n",
      "|       Average F1 Score       |   0.73608    |\n",
      "|         Average Loss         |   0.76169    |\n",
      "|       Average Latency        |  3.9805 ms   |\n",
      "|   Average GPU Power Usage    |   66.442 W   |\n",
      "| Inference Energy Consumption | 0.073464 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 13:48:16.427903 140617952859968 calibrate.py:66] Post calibration analysis complete.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0317 13:48:16.429003 140617952859968 calibrate.py:150] Succeeded in calibrating the model in PyTorch!\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, the 99% `percentile` clips too many values during the amax calibration, comprimising the loss. However 99.99% demonstrates higher validation accuracy alongside `mse` and `entropy` for `jsc-toy`. For such a small model, the methods are not highly distinguished, however for larger models this calibration process will be important for ensuring the quantized model still performs well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3 Quantized Aware Training (QAT)\n",
    "\n",
    "The `tensorrt_fine_tune_transform_pass` is used to fine tune the quantized model. \n",
    "\n",
    "For QAT it is typical to employ 10% of the original training epochs, starting at 1% of the initial training learning rate, and a cosine annealing learning rate schedule that follows the decreasing half of a cosine period, down to 1% of the initial fine tuning learning rate (0.01% of the initial training learning rate). However this default can be overidden by setting the `epochs`, `initial_learning_rate` and `final_learning_rate` in `passes.tensorrt_quantize.fine_tune`.\n",
    "\n",
    "The fine tuned checkpoints are stored in the ckpts/fine_tuning folder:\n",
    "\n",
    "```\n",
    "mase_output\n",
    "└── tensorrt\n",
    "    └── quantization\n",
    "        ├── cache\n",
    "        ├── ckpts\n",
    "        │   └── fine_tuning\n",
    "        ├── json\n",
    "        ├── onnx\n",
    "        └── trt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4 TensorRT Quantization\n",
    "\n",
    "After QAT, we are now ready to convert the model to a tensorRT engine so that it can be run with the superior inference speeds. To do so, we use the `tensorrt_engine_interface_pass` which converts the `MaseGraph`'s model from a Pytorch one to an ONNX format as an intermediate stage of the conversion.\n",
    "\n",
    "During the conversion process, the `.onnx` and `.trt` files are stored to their respective folders shown in [Section 1.3](#section-13-quantized-aware-training-qat).\n",
    "\n",
    "This interface pass returns a dictionary containing the `onnx_path` and `trt_engine_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg, meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.5 Performance Analysis\n",
    "\n",
    "To showcase the improved inference speeds and to evaluate accuracy and other performance metrics, the `tensorrt_analysis_pass` can be used.\n",
    "\n",
    "The tensorRT engine path obtained the previous interface pass is now inputted into the the analysis pass. The same pass can take a MaseGraph as an input, as well as an ONNX graph. For this comparison, we will first run the anaylsis pass on the original unquantized model and then on the INT8 quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = tensorrt_analysis_pass(mg_original, pass_args=tensorrt_analysis_config)\n",
    "_, _ = tensorrt_analysis_pass(meta['trt_engine_path'], pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the latency has decreased almost 4x with the `jsc-toy` model without significantly compromising accuracy.\n",
    "\n",
    "## Section 2. FP16 Quantization\n",
    "\n",
    "We will now load in a new toml configuration that uses FP16 instead of INT8, whilst keeping the other settings the exact same for a fair comparison. This time however, we will use chop from the terminal which runs all the passes showcased in [Section 1](#section-1---int8-quantization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ch transform --config ../../../machop/configs/tensorrt/jsc_toy_FP16_quantization_by_type.toml --load checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy INT8 only quantization \n",
    "meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_0/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy FP16 only quantization \n",
    "meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_1/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(meta['graph_path'], pass_args=tensorrt_analysis_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
