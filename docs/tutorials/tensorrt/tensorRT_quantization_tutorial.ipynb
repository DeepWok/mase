{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Welcome To the TensorRT Quantization Tutorial!\n",
                "\n",
                "This notebook is designed to show the features of the TensorRT passes integrated into MASE as part of the MASERT framework.\n",
                "\n",
                "## Section 1. int8 Quantization\n",
                "Firstly, we will show you how to do a int8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model using the `Machop API`. The quantization process is split into the following stages, each using their own individual pass, and are explained in depth at each subsection:\n",
                "\n",
                "1. [Fake quantization](#section-11-fake-quantization): `tensorrt_fake_quantize_transform_pass`\n",
                "2. [Calibration](#section-12-calibration): `tensorrt_calibrate_transform_pass`\n",
                "3. [Quantized Aware Training](#section-13-quantized-aware-training-qat): `tensorrt_fine_tune_transform_pass`\n",
                "4. [Quantization](#section-14-tensorrt-quantization): `tensorrt_engine_interface_pass`\n",
                "5. [Analysis](#section-15-performance-analysis): `tensorrt_analysis_pass`\n",
                "\n",
                "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-03-28 02:14:29,451] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
                        "WARNING: Logging before flag parsing goes to stderr.\n",
                        "I0328 02:14:31.882540 140139267819328 logger.py:44] Set logging level to info\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import toml\n",
                "\n",
                "# Figure out the correct path\n",
                "machop_path = Path(\".\").resolve().parent.parent.parent /\"machop\"\n",
                "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
                "sys.path.append(str(machop_path))\n",
                "\n",
                "# Add directory to the PATH so that chop can be called\n",
                "new_path = \"../../../machop\"\n",
                "full_path = os.path.abspath(new_path)\n",
                "os.environ['PATH'] += os.pathsep + full_path\n",
                "\n",
                "from chop.tools.utils import to_numpy_if_tensor\n",
                "from chop.tools.logger import set_logging_verbosity\n",
                "from chop.tools import get_cf_args, get_dummy_input\n",
                "from chop.passes.graph.utils import deepcopy_mase_graph\n",
                "from chop.tools.get_input import InputGenerator\n",
                "from chop.tools.checkpoint_load import load_model\n",
                "from chop.ir import MaseGraph\n",
                "from chop.models import get_model_info, get_model, get_tokenizer\n",
                "from chop.dataset import MaseDataModule, get_dataset_info\n",
                "from chop.passes.graph.transforms import metadata_value_type_cast_transform_pass\n",
                "from chop.passes.graph import (\n",
                "    summarize_quantization_analysis_pass,\n",
                "    add_common_metadata_analysis_pass,\n",
                "    init_metadata_analysis_pass,\n",
                "    add_software_metadata_analysis_pass,\n",
                "    tensorrt_calibrate_transform_pass,\n",
                "    tensorrt_fake_quantize_transform_pass,\n",
                "    tensorrt_fine_tune_transform_pass,\n",
                "    tensorrt_engine_interface_pass,\n",
                "    runtime_analysis_pass,\n",
                "    )\n",
                "\n",
                "set_logging_verbosity(\"info\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to your TOML file\n",
                "JSC_TOML_PATH = '../../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml'\n",
                "\n",
                "# Reading TOML file and converting it into a Python dictionary\n",
                "with open(JSC_TOML_PATH, 'r') as toml_file:\n",
                "    pass_args = toml.load(toml_file)\n",
                "\n",
                "# Extract the 'passes.tensorrt' section and its children\n",
                "tensorrt_config = pass_args.get('passes', {}).get('tensorrt', {})\n",
                "# Extract the 'passes.runtime_analysis' section and its children\n",
                "runtime_analysis_config = pass_args.get('passes', {}).get('runtime_analysis', {})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We then create a `MaseGraph` by loading in a model and training it using the toml configuration model arguments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the basics in\n",
                "model_name = pass_args['model']\n",
                "dataset_name = pass_args['dataset']\n",
                "max_epochs = pass_args['max_epochs']\n",
                "batch_size = pass_args['batch_size']\n",
                "learning_rate = pass_args['learning_rate']\n",
                "accelerator = pass_args['accelerator']\n",
                "\n",
                "data_module = MaseDataModule(\n",
                "    name=dataset_name,\n",
                "    batch_size=batch_size,\n",
                "    model_name=model_name,\n",
                "    num_workers=0,\n",
                ")\n",
                "data_module.prepare_data()\n",
                "data_module.setup()\n",
                "\n",
                "# Add the data_module and other necessary information to the configs\n",
                "configs = [tensorrt_config, runtime_analysis_config]\n",
                "for config in configs:\n",
                "    config['task'] = pass_args['task']\n",
                "    config['dataset'] = pass_args['dataset']\n",
                "    config['batch_size'] = pass_args['batch_size']\n",
                "    config['model'] = pass_args['model']\n",
                "    config['data_module'] = data_module\n",
                "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
                "    if config['accelerator'] == 'gpu':\n",
                "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
                "\n",
                "model_info = get_model_info(model_name)\n",
                "# quant_modules.initialize()\n",
                "model = get_model(\n",
                "    model_name,\n",
                "    task=\"cls\",\n",
                "    dataset_info=data_module.dataset_info,\n",
                "    pretrained=False)\n",
                "\n",
                "\n",
                "input_generator = InputGenerator(\n",
                "    data_module=data_module,\n",
                "    model_info=model_info,\n",
                "    task=\"cls\",\n",
                "    which_dataloader=\"train\",\n",
                ")\n",
                "\n",
                "# generate the mase graph and initialize node metadata\n",
                "mg = MaseGraph(model=model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we train the `jsc-toy` model using the machop `train` action with the config from the toml file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !ch train --config {JSC_TOML_PATH}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then we load in the checkpoint. You will have to adjust this according to where it has been stored in the mase_output directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from ../../../mase_output/jsc-toy_cls_jsc-pre-trained/best.ckpt\u001b[0m\n",
                        "I0328 02:14:34.762252 140139267819328 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from ../../../mase_output/jsc-toy_cls_jsc-pre-trained/best.ckpt\n"
                    ]
                }
            ],
            "source": [
                "# Load in the trained checkpoint - change this accordingly\n",
                "JSC_CHECKPOINT_PATH = \"../../../mase_output/jsc-toy_cls_jsc-pre-trained/best.ckpt\"\n",
                "\n",
                "model = load_model(load_name=JSC_CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
                "\n",
                "# Initiate metadata\n",
                "dummy_in = next(iter(input_generator))\n",
                "_ = model(**dummy_in)\n",
                "mg, _ = init_metadata_analysis_pass(mg, None)\n",
                "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
                "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
                "mg, _ = metadata_value_type_cast_transform_pass(mg, pass_args={\"fn\": to_numpy_if_tensor})\n",
                "\n",
                "# Before we begin, we will copy the original MaseGraph model to use for comparison during quantization analysis\n",
                "mg_original = deepcopy_mase_graph(mg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Section 1.1 Fake Quantization\n",
                "\n",
                "Firstly, we fake quantize the module in order to perform calibration and fine tuning before actually quantizing - this is only used if we have int8 calibration as other precisions are not currently supported within [pytorch-quantization](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#) library.\n",
                "\n",
                "This is acheived through the `tensorrt_fake_quantize_transform_pass` which goes through the model, either by type or by name, replaces each layer appropriately to a fake quantized form if the `quantize` parameter is set in the default config (`passes.tensorrt_quantize.default.config`) or on a per name or type basis. \n",
                "\n",
                "Currently the quantizable layers are:\n",
                "- Linear\n",
                "- Conv1d, Conv2d, Conv3d \n",
                "- ConvTranspose1d, ConvTranspose2d, ConvTranspose3d \n",
                "- MaxPool1d, MaxPool2d, MaxPool3d\n",
                "- AvgPool1d, AvgPool2d, AvgPool3d\n",
                "- LSTM, LSTMCell\n",
                "\n",
                "To create a custom quantized module, click [here](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#document-tutorials/creating_custom_quantized_modules).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
                        "I0328 02:14:34.895051 140139267819328 utils.py:240] Applying fake quantization to PyTorch model...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
                        "I0328 02:14:35.187025 140139267819328 utils.py:265] Fake quantization applied to PyTorch model.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
                        "I0328 02:14:35.199829 140139267819328 summary.py:84] Quantized graph histogram:\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         3 |           0 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
                        "I0328 02:14:35.201924 140139267819328 summary.py:85] \n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         3 |           0 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\n"
                    ]
                }
            ],
            "source": [
                "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_config)\n",
                "summarize_quantization_analysis_pass(mg_original, mg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see we have succesfully fake quantized all linear layers inside `jsc-toy`. This means that we will be able to simulate a quantized model in order to calibrate and fine tune it. This fake quantization was done on typewise i.e. for linear layers only. See [Section 4](#section-4-layer-wise-mixed-precision) for how to apply quantization layerwise - i.e. only first and second layers for example."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Section 1.2 Calibration\n",
                "\n",
                "Next, we perform calibration using the `tensorrt_calibrate_transform_pass`. Calibration is achieved by passing data samples to the quantizer and deciding the best amax for activations. \n",
                "\n",
                "Calibrators can be added as a search space parameter to examine the best performing calibrator. The calibrators have been included in the toml as follows.\n",
                "For example: `calibrators = [\"percentile\", \"mse\", \"entropy\"]`\n",
                "\n",
                "Note: \n",
                "- To use `percentile` calibration, a list of percentiles must be given\n",
                "- To use `max` calibration, the `histogram` weight and input calibrators must be removed and replaced with `max`. This will use global maximum absolute value to calibrate the model.\n",
                "- If `post_calibration_analysis` is set true the `tensorrt_analysis_pass` will be run for each calibrator tested to evaluate the most suitable calibrator for the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
                        "I0328 02:14:35.212774 140139267819328 calibrate.py:142] Starting calibration of the model in PyTorch...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.216601 140139267819328 calibrate.py:151] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.217824 140139267819328 calibrate.py:151] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.218976 140139267819328 calibrate.py:151] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.220192 140139267819328 calibrate.py:151] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.221400 140139267819328 calibrate.py:151] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.222592 140139267819328 calibrate.py:151] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.401098 140139267819328 calibrate.py:171] Enabling Quantization and Disabling Calibration\n",
                        "W0328 02:14:35.402625 140139267819328 tensor_quantizer.py:174] Disable MaxCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.403221 140139267819328 calibrate.py:171] Enabling Quantization and Disabling Calibration\n",
                        "W0328 02:14:35.404430 140139267819328 tensor_quantizer.py:174] Disable MaxCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.405012 140139267819328 calibrate.py:171] Enabling Quantization and Disabling Calibration\n",
                        "W0328 02:14:35.406134 140139267819328 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.406682 140139267819328 calibrate.py:171] Enabling Quantization and Disabling Calibration\n",
                        "W0328 02:14:35.407754 140139267819328 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.408290 140139267819328 calibrate.py:171] Enabling Quantization and Disabling Calibration\n",
                        "W0328 02:14:35.409381 140139267819328 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0328 02:14:35.409991 140139267819328 calibrate.py:171] Enabling Quantization and Disabling Calibration\n",
                        "W0328 02:14:35.411114 140139267819328 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "W0328 02:14:35.411704 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "W0328 02:14:35.412285 140139267819328 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:35.412958 140139267819328 calibrate.py:130] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:35.414108 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([8, 1]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:35.415244 140139267819328 calibrate.py:130] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:35.424989 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:35.425938 140139267819328 calibrate.py:130] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4583 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:35.427620 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5621 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:35.428250 140139267819328 calibrate.py:130] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5621 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:35.430244 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7310 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:35.430798 140139267819328 calibrate.py:130] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7310 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:35.432455 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5606 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:35.433058 140139267819328 calibrate.py:130] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5606 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
                        "I0328 02:14:35.435403 140139267819328 calibrate.py:104] Performing post calibration analysis for calibrator percentile_99.0...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on jsc-toy\u001b[0m\n",
                        "I0328 02:14:35.436510 140139267819328 runtime_analysis.py:309] Starting transformation analysis on jsc-toy\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "|    Average Test Accuracy     |   0.71232    |\n",
                        "|      Average Precision       |   0.71349    |\n",
                        "|        Average Recall        |   0.70401    |\n",
                        "|       Average F1 Score       |   0.70544    |\n",
                        "|         Average Loss         |   0.84152    |\n",
                        "|       Average Latency        |  2.8861 ms   |\n",
                        "|   Average GPU Power Usage    |   23.236 W   |\n",
                        "| Inference Energy Consumption | 0.018628 mWh |\n",
                        "+------------------------------+--------------+\u001b[0m\n",
                        "I0328 02:14:38.888058 140139267819328 runtime_analysis.py:437] \n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "|    Average Test Accuracy     |   0.71232    |\n",
                        "|      Average Precision       |   0.71349    |\n",
                        "|        Average Recall        |   0.70401    |\n",
                        "|       Average F1 Score       |   0.70544    |\n",
                        "|         Average Loss         |   0.84152    |\n",
                        "|       Average Latency        |  2.8861 ms   |\n",
                        "|   Average GPU Power Usage    |   23.236 W   |\n",
                        "| Inference Energy Consumption | 0.018628 mWh |\n",
                        "+------------------------------+--------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_20/model.json\u001b[0m\n",
                        "I0328 02:14:38.890786 140139267819328 runtime_analysis.py:123] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_20/model.json\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
                        "I0328 02:14:38.892017 140139267819328 calibrate.py:117] Post calibration analysis complete.\n",
                        "W0328 02:14:38.893283 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:38.893980 140139267819328 calibrate.py:130] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:38.895207 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([8, 1]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:38.896208 140139267819328 calibrate.py:130] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:38.897923 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.0614 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:38.898739 140139267819328 calibrate.py:130] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.0614 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:38.900309 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5621 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:38.900975 140139267819328 calibrate.py:130] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5621 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:38.902891 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.6858 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:38.903722 140139267819328 calibrate.py:130] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.6858 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:38.905442 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5606 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:38.906109 140139267819328 calibrate.py:130] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5606 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
                        "I0328 02:14:38.908041 140139267819328 calibrate.py:104] Performing post calibration analysis for calibrator percentile_99.9...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on jsc-toy\u001b[0m\n",
                        "I0328 02:14:38.909076 140139267819328 runtime_analysis.py:309] Starting transformation analysis on jsc-toy\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "| Average Validation Accuracy  |   0.71678    |\n",
                        "|      Average Precision       |   0.71959    |\n",
                        "|        Average Recall        |   0.71039    |\n",
                        "|       Average F1 Score       |   0.71209    |\n",
                        "|         Average Loss         |   0.81512    |\n",
                        "|       Average Latency        |  2.8058 ms   |\n",
                        "|   Average GPU Power Usage    |   23.118 W   |\n",
                        "| Inference Energy Consumption | 0.018017 mWh |\n",
                        "+------------------------------+--------------+\u001b[0m\n",
                        "I0328 02:14:42.023770 140139267819328 runtime_analysis.py:437] \n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "| Average Validation Accuracy  |   0.71678    |\n",
                        "|      Average Precision       |   0.71959    |\n",
                        "|        Average Recall        |   0.71039    |\n",
                        "|       Average F1 Score       |   0.71209    |\n",
                        "|         Average Loss         |   0.81512    |\n",
                        "|       Average Latency        |  2.8058 ms   |\n",
                        "|   Average GPU Power Usage    |   23.118 W   |\n",
                        "| Inference Energy Consumption | 0.018017 mWh |\n",
                        "+------------------------------+--------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_21/model.json\u001b[0m\n",
                        "I0328 02:14:42.026330 140139267819328 runtime_analysis.py:123] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_21/model.json\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
                        "I0328 02:14:42.027460 140139267819328 calibrate.py:117] Post calibration analysis complete.\n",
                        "W0328 02:14:42.028636 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:42.030091 140139267819328 calibrate.py:130] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:42.031203 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([8, 1]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:42.032423 140139267819328 calibrate.py:130] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:42.034133 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.9840 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:42.034699 140139267819328 calibrate.py:130] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.9840 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:42.035923 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5621 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:42.036485 140139267819328 calibrate.py:130] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5621 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:42.037689 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0583 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:42.038240 140139267819328 calibrate.py:130] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0583 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:42.039328 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5606 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:42.039882 140139267819328 calibrate.py:130] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5606 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
                        "I0328 02:14:42.040998 140139267819328 calibrate.py:104] Performing post calibration analysis for calibrator percentile_99.99...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on jsc-toy\u001b[0m\n",
                        "I0328 02:14:42.041776 140139267819328 runtime_analysis.py:309] Starting transformation analysis on jsc-toy\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results jsc-toy:\n",
                        "+------------------------------+-------------+\n",
                        "|      Metric (Per Batch)      |    Value    |\n",
                        "+------------------------------+-------------+\n",
                        "| Average Validation Accuracy  |   0.71766   |\n",
                        "|      Average Precision       |   0.72036   |\n",
                        "|        Average Recall        |   0.71136   |\n",
                        "|       Average F1 Score       |   0.71308   |\n",
                        "|         Average Loss         |   0.81424   |\n",
                        "|       Average Latency        |  2.8509 ms  |\n",
                        "|   Average GPU Power Usage    |  23.171 W   |\n",
                        "| Inference Energy Consumption | 0.01835 mWh |\n",
                        "+------------------------------+-------------+\u001b[0m\n",
                        "I0328 02:14:45.358841 140139267819328 runtime_analysis.py:437] \n",
                        "Results jsc-toy:\n",
                        "+------------------------------+-------------+\n",
                        "|      Metric (Per Batch)      |    Value    |\n",
                        "+------------------------------+-------------+\n",
                        "| Average Validation Accuracy  |   0.71766   |\n",
                        "|      Average Precision       |   0.72036   |\n",
                        "|        Average Recall        |   0.71136   |\n",
                        "|       Average F1 Score       |   0.71308   |\n",
                        "|         Average Loss         |   0.81424   |\n",
                        "|       Average Latency        |  2.8509 ms  |\n",
                        "|   Average GPU Power Usage    |  23.171 W   |\n",
                        "| Inference Energy Consumption | 0.01835 mWh |\n",
                        "+------------------------------+-------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_22/model.json\u001b[0m\n",
                        "I0328 02:14:45.361584 140139267819328 runtime_analysis.py:123] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_22/model.json\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
                        "I0328 02:14:45.362433 140139267819328 calibrate.py:117] Post calibration analysis complete.\n",
                        "W0328 02:14:45.363259 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:45.364219 140139267819328 calibrate.py:130] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:45.365046 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([8, 1]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:45.365911 140139267819328 calibrate.py:130] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:46.805171 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.9235 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:46.806403 140139267819328 calibrate.py:130] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.9235 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:47.913218 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5601 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:47.914515 140139267819328 calibrate.py:130] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5601 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:49.518495 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0265 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:49.519893 140139267819328 calibrate.py:130] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0265 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:50.615883 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5588 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:50.617150 140139267819328 calibrate.py:130] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5588 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
                        "I0328 02:14:50.619230 140139267819328 calibrate.py:104] Performing post calibration analysis for calibrator mse...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on jsc-toy\u001b[0m\n",
                        "I0328 02:14:50.620320 140139267819328 runtime_analysis.py:309] Starting transformation analysis on jsc-toy\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "| Average Validation Accuracy  |   0.71739    |\n",
                        "|      Average Precision       |   0.72013    |\n",
                        "|        Average Recall        |   0.71102    |\n",
                        "|       Average F1 Score       |   0.71269    |\n",
                        "|         Average Loss         |   0.81419    |\n",
                        "|       Average Latency        |  2.7599 ms   |\n",
                        "|   Average GPU Power Usage    |   23.328 W   |\n",
                        "| Inference Energy Consumption | 0.017884 mWh |\n",
                        "+------------------------------+--------------+\u001b[0m\n",
                        "I0328 02:14:53.710566 140139267819328 runtime_analysis.py:437] \n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "| Average Validation Accuracy  |   0.71739    |\n",
                        "|      Average Precision       |   0.72013    |\n",
                        "|        Average Recall        |   0.71102    |\n",
                        "|       Average F1 Score       |   0.71269    |\n",
                        "|         Average Loss         |   0.81419    |\n",
                        "|       Average Latency        |  2.7599 ms   |\n",
                        "|   Average GPU Power Usage    |   23.328 W   |\n",
                        "| Inference Energy Consumption | 0.017884 mWh |\n",
                        "+------------------------------+--------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_23/model.json\u001b[0m\n",
                        "I0328 02:14:53.713332 140139267819328 runtime_analysis.py:123] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_23/model.json\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
                        "I0328 02:14:53.714445 140139267819328 calibrate.py:117] Post calibration analysis complete.\n",
                        "W0328 02:14:53.715567 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:53.716240 140139267819328 calibrate.py:130] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3010 calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:53.717407 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([8, 1]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:53.718444 140139267819328 calibrate.py:130] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.4018, 0.7529](8) calibrator=MaxCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:57.680802 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.7816 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:57.682134 140139267819328 calibrate.py:130] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.7816 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:14:59.845564 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5624 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:14:59.846817 140139267819328 calibrate.py:130] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5624 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:15:05.167145 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0593 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:15:05.168424 140139267819328 calibrate.py:130] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0593 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0328 02:15:07.370557 140139267819328 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5609 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0328 02:15:07.371812 140139267819328 calibrate.py:130] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5609 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
                        "I0328 02:15:07.373893 140139267819328 calibrate.py:104] Performing post calibration analysis for calibrator entropy...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on jsc-toy\u001b[0m\n",
                        "I0328 02:15:07.374985 140139267819328 runtime_analysis.py:309] Starting transformation analysis on jsc-toy\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "| Average Validation Accuracy  |   0.71737    |\n",
                        "|      Average Precision       |   0.72008    |\n",
                        "|        Average Recall        |   0.71095    |\n",
                        "|       Average F1 Score       |   0.71263    |\n",
                        "|         Average Loss         |   0.81421    |\n",
                        "|       Average Latency        |  2.9435 ms   |\n",
                        "|   Average GPU Power Usage    |   23.426 W   |\n",
                        "| Inference Energy Consumption | 0.019154 mWh |\n",
                        "+------------------------------+--------------+\u001b[0m\n",
                        "I0328 02:15:10.811730 140139267819328 runtime_analysis.py:437] \n",
                        "Results jsc-toy:\n",
                        "+------------------------------+--------------+\n",
                        "|      Metric (Per Batch)      |    Value     |\n",
                        "+------------------------------+--------------+\n",
                        "| Average Validation Accuracy  |   0.71737    |\n",
                        "|      Average Precision       |   0.72008    |\n",
                        "|        Average Recall        |   0.71095    |\n",
                        "|       Average F1 Score       |   0.71263    |\n",
                        "|         Average Loss         |   0.81421    |\n",
                        "|       Average Latency        |  2.9435 ms   |\n",
                        "|   Average GPU Power Usage    |   23.426 W   |\n",
                        "| Inference Energy Consumption | 0.019154 mWh |\n",
                        "+------------------------------+--------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_24/model.json\u001b[0m\n",
                        "I0328 02:15:10.814496 140139267819328 runtime_analysis.py:123] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_24/model.json\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
                        "I0328 02:15:10.815635 140139267819328 calibrate.py:117] Post calibration analysis complete.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
                        "I0328 02:15:10.816925 140139267819328 calibrate.py:209] Succeeded in calibrating the model in PyTorch!\n"
                    ]
                }
            ],
            "source": [
                "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the results, the 99% `percentile` clips too many values during the amax calibration, compromising the loss. However 99.99% demonstrates higher validation accuracy alongside `mse` and `entropy` for `jsc-toy`. For such a small model, the methods are not highly distinguished, however for larger models this calibration process will be important for ensuring the quantized model still performs well. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Section 1.3 Quantized Aware Training (QAT)\n",
                "\n",
                "The `tensorrt_fine_tune_transform_pass` is used to fine tune the quantized model. \n",
                "\n",
                "For QAT it is typical to employ 10% of the original training epochs, starting at 1% of the initial training learning rate, and a cosine annealing learning rate schedule that follows the decreasing half of a cosine period, down to 1% of the initial fine tuning learning rate (0.01% of the initial training learning rate). However this default can be overidden by setting the `epochs`, `initial_learning_rate` and `final_learning_rate` in `passes.tensorrt_quantize.fine_tune`.\n",
                "\n",
                "The fine tuned checkpoints are stored in the ckpts/fine_tuning folder:\n",
                "\n",
                "```\n",
                "mase_output\n",
                " tensorrt\n",
                "     model_task_dataset_date\n",
                "        quantization\n",
                "             cache\n",
                "             ckpts\n",
                "                fine_tuning\n",
                "             json\n",
                "             onnx\n",
                "             trt\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
                        "I0328 02:16:13.824619 140139267819328 fine_tune.py:136] Starting Fine Tuning for 2 epochs...\n",
                        "I0328 02:16:13.984824 140139267819328 rank_zero.py:64] GPU available: True (cuda), used: True\n",
                        "I0328 02:16:14.013438 140139267819328 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
                        "I0328 02:16:14.014248 140139267819328 rank_zero.py:64] IPU available: False, using: 0 IPUs\n",
                        "I0328 02:16:14.015104 140139267819328 rank_zero.py:64] HPU available: False, using: 0 HPUs\n",
                        "I0328 02:16:14.024717 140139267819328 rank_zero.py:64] You are using a CUDA device ('NVIDIA RTX A2000 12GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
                        "I0328 02:16:16.733907 140139267819328 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "I0328 02:16:16.747727 140139267819328 model_summary.py:94] \n",
                        "  | Name      | Type               | Params\n",
                        "-------------------------------------------------\n",
                        "0 | model     | GraphModule        | 327   \n",
                        "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
                        "2 | acc_train | MulticlassAccuracy | 0     \n",
                        "3 | loss_val  | MeanMetric         | 0     \n",
                        "4 | loss_test | MeanMetric         | 0     \n",
                        "-------------------------------------------------\n",
                        "327       Trainable params\n",
                        "0         Non-trainable params\n",
                        "327       Total params\n",
                        "0.001     Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "16dacd9641fb4c62890e9152eef8b118",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n",
                        "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d7b0e2204f774811a87354b730e8d049",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Training: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c64f5196dc5844588c65775cad7cc822",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d2c5c54d8e5a4553beacdb5a1e3e8ad8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "I0328 02:22:52.141722 140139267819328 rank_zero.py:64] `Trainer.fit` stopped: `max_epochs=2` reached.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
                        "I0328 02:22:52.149791 140139267819328 fine_tune.py:155] Fine Tuning Complete\n"
                    ]
                }
            ],
            "source": [
                "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Section 1.4 TensorRT Quantization\n",
                "\n",
                "After QAT, we are now ready to convert the model to a tensorRT engine so that it can be run with the superior inference speeds. To do so, we use the `tensorrt_engine_interface_pass` which converts the `MaseGraph`'s model from a Pytorch one to an ONNX format as an intermediate stage of the conversion.\n",
                "\n",
                "During the conversion process, the `.onnx` and `.trt` files are stored to their respective folders shown in [Section 1.3](#section-13-quantized-aware-training-qat).\n",
                "\n",
                "This interface pass returns a dictionary containing the `onnx_path` and `trt_engine_path`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
                        "I0328 02:23:01.618408 140139267819328 quantize.py:159] Converting PyTorch model to ONNX...\n",
                        "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
                        "  if min_amax < 0:\n",
                        "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
                        "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
                        "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
                        "/root/anaconda3/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
                        "  if min_amax <= epsilon:\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024-03-28/version_1/model.onnx\u001b[0m\n",
                        "I0328 02:23:01.890016 140139267819328 quantize.py:182] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024-03-28/version_1/model.onnx\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
                        "I0328 02:23:01.891598 140139267819328 quantize.py:85] Converting PyTorch model to TensorRT...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024-03-28/version_0/model.trt\u001b[0m\n",
                        "I0328 02:23:54.227923 140139267819328 quantize.py:154] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024-03-28/version_0/model.trt\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024-03-28/version_0/model.json\u001b[0m\n",
                        "I0328 02:23:54.467350 140139267819328 quantize.py:198] TensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024-03-28/version_0/model.json\n"
                    ]
                }
            ],
            "source": [
                "mg, meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Section 1.5 Performance Analysis\n",
                "\n",
                "To showcase the improved inference speeds and to evaluate accuracy and other performance metrics, the `tensorrt_analysis_pass` can be used.\n",
                "\n",
                "The tensorRT engine path obtained the previous interface pass is now inputted into the the analysis pass. The same pass can take a MaseGraph as an input, as well as an ONNX graph. For this comparison, we will first run the anaylsis pass on the original unquantized model and then on the int8 quantized model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on jsc-toy\u001b[0m\n",
                        "I0328 02:24:11.350569 140139267819328 runtime_analysis.py:309] Starting transformation analysis on jsc-toy\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results jsc-toy:\n",
                        "+------------------------------+---------------+\n",
                        "|      Metric (Per Batch)      |     Value     |\n",
                        "+------------------------------+---------------+\n",
                        "|    Average Test Accuracy     |    0.71971    |\n",
                        "|      Average Precision       |    0.71884    |\n",
                        "|        Average Recall        |    0.71127    |\n",
                        "|       Average F1 Score       |    0.71274    |\n",
                        "|         Average Loss         |    0.8116     |\n",
                        "|       Average Latency        |  0.87184 ms   |\n",
                        "|   Average GPU Power Usage    |   23.327 W    |\n",
                        "| Inference Energy Consumption | 0.0056493 mWh |\n",
                        "+------------------------------+---------------+\u001b[0m\n",
                        "I0328 02:24:13.870435 140139267819328 runtime_analysis.py:437] \n",
                        "Results jsc-toy:\n",
                        "+------------------------------+---------------+\n",
                        "|      Metric (Per Batch)      |     Value     |\n",
                        "+------------------------------+---------------+\n",
                        "|    Average Test Accuracy     |    0.71971    |\n",
                        "|      Average Precision       |    0.71884    |\n",
                        "|        Average Recall        |    0.71127    |\n",
                        "|       Average F1 Score       |    0.71274    |\n",
                        "|         Average Loss         |    0.8116     |\n",
                        "|       Average Latency        |  0.87184 ms   |\n",
                        "|   Average GPU Power Usage    |   23.327 W    |\n",
                        "| Inference Energy Consumption | 0.0056493 mWh |\n",
                        "+------------------------------+---------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mRuntime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_25/model.json\u001b[0m\n",
                        "I0328 02:24:13.873045 140139267819328 runtime_analysis.py:123] Runtime analysis results saved to /root/mase_output/tensorrt/quantization/jsc-toy_cls_jsc_2024-03-28/mase_graph/version_25/model.json\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "TensorRT Engine Input/Output Information:\n",
                        "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
                        "------|---------|----------|----------------------|----------------------|-----------------------\n",
                        "0     | Input   | FLOAT    | (64, 16)               | (64, 16)               | input\n",
                        "1     | Output  | FLOAT    | (64, 5)                | (64, 5)                | 109\u001b[0m\n",
                        "I0328 02:24:13.886346 140139267819328 runtime_analysis.py:167] \n",
                        "TensorRT Engine Input/Output Information:\n",
                        "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
                        "------|---------|----------|----------------------|----------------------|-----------------------\n",
                        "0     | Input   | FLOAT    | (64, 16)               | (64, 16)               | input\n",
                        "1     | Output  | FLOAT    | (64, 5)                | (64, 5)                | 109\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis on jsc-toy-trt_quantized\u001b[0m\n",
                        "I0328 02:24:13.887727 140139267819328 runtime_analysis.py:309] Starting transformation analysis on jsc-toy-trt_quantized\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m runtime_analysis_pass(mg_original, pass_args\u001b[38;5;241m=\u001b[39mruntime_analysis_config)\n\u001b[0;32m----> 2\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mruntime_analysis_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrt_engine_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruntime_analysis_config\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/mase/machop/chop/passes/graph/analysis/runtime/runtime_analysis.py:53\u001b[0m, in \u001b[0;36mruntime_analysis_pass\u001b[0;34m(model, pass_args)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mEvaluates the performance of a model by analyzing its inference speed, accuracy, and other relevant metrics.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mThese metrics provide valuable insights into the model's efficiency, effectiveness, and operational cost, crucial for informed decision-making regarding model deployment in production environments.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m analysis \u001b[38;5;241m=\u001b[39m RuntimeAnalysis(model, pass_args)\n\u001b[0;32m---> 53\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m analysis\u001b[38;5;241m.\u001b[39mstore(results)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, results\n",
                        "File \u001b[0;32m~/mase/machop/chop/passes/graph/analysis/runtime/runtime_analysis.py:353\u001b[0m, in \u001b[0;36mRuntimeAnalysis.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorRT inference is only supported on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 353\u001b[0m     preds, latency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_trt_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# ONNX Inference\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ort\u001b[38;5;241m.\u001b[39mInferenceSession):\n",
                        "File \u001b[0;32m~/mase/machop/chop/passes/graph/analysis/runtime/runtime_analysis.py:214\u001b[0m, in \u001b[0;36mRuntimeAnalysis.infer_trt_cuda\u001b[0;34m(self, trt_context, input_data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer_trt_cuda\u001b[39m(\u001b[38;5;28mself\u001b[39m, trt_context, input_data):\n\u001b[1;32m    213\u001b[0m     bufferH \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 214\u001b[0m     bufferH\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_Input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_io):\n\u001b[1;32m    216\u001b[0m         bufferH\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mget_tensor_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlTensorName[i]), dtype\u001b[38;5;241m=\u001b[39mtrt\u001b[38;5;241m.\u001b[39mnptype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mget_tensor_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlTensorName[i]))))\n",
                        "File \u001b[0;32m~/anaconda3/envs/mase/lib/python3.11/site-packages/torch/_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
                        "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
                    ]
                }
            ],
            "source": [
                "_, _ = runtime_analysis_pass(mg_original, pass_args=runtime_analysis_config)\n",
                "_, _ = runtime_analysis_pass(meta['trt_engine_path'], pass_args=runtime_analysis_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As shown above, the latency has decreased around 4x with the `jsc-toy` model without compromising accuracy due to the well calibrated amax and quantization-aware fine tuning. The inference energy consumption has thus also dropped tremendously and this is an excellent demonstration for the need to quantize in industry especially for LLMs in order to reduce energy usage. \n",
                "\n",
                "## Section 2. fp16 Quantization\n",
                "\n",
                "We will now load in a new toml configuration that uses fp16 instead of int8, whilst keeping the other settings the exact same for a fair comparison. This time however, we will use chop from the terminal which runs all the passes showcased in [Section 1](#section-1---int8-quantization).\n",
                "\n",
                "Since float quantization does not require calibration, nor is it supported by `pytorch-quantization`, the model will not undergo fake quantization; for the time being this unfortunately means QAT is unavailable and only undergoes Post Training Quantization (PTQ). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "567.49s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
                        "[2024-03-18 13:29:23,372] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
                        "INFO: Seed set to 0\n",
                        "WARNING: Logging before flag parsing goes to stderr.\n",
                        "I0318 13:29:24.963454 139971211573056 seed.py:54] Seed set to 0\n",
                        "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
                        "| Name                    |        Default         | Config. File |     Manual Override      |        Effective         |\n",
                        "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
                        "| task                    |     \u001b[38;5;8mclassification\u001b[0m     |     cls      |                          |           cls            |\n",
                        "| load_name               |          \u001b[38;5;8mNone\u001b[0m          |              | /root/mase/mase_output/j | /root/mase/mase_output/j |\n",
                        "|                         |                        |              | sc-toy-cls_jsc/best.ckpt | sc-toy-cls_jsc/best.ckpt |\n",
                        "| load_type               |           \u001b[38;5;8mmz\u001b[0m           |              |            pl            |            pl            |\n",
                        "| batch_size              |          \u001b[38;5;8m128\u001b[0m           |     256      |                          |           256            |\n",
                        "| to_debug                |         False          |              |                          |          False           |\n",
                        "| log_level               |          info          |              |                          |           info           |\n",
                        "| report_to               |      tensorboard       |              |                          |       tensorboard        |\n",
                        "| seed                    |           0            |              |                          |            0             |\n",
                        "| quant_config            |          None          |              |                          |           None           |\n",
                        "| training_optimizer      |          adam          |              |                          |           adam           |\n",
                        "| trainer_precision       |        16-mixed        |              |                          |         16-mixed         |\n",
                        "| learning_rate           |         \u001b[38;5;8m1e-05\u001b[0m          |    0.001     |                          |          0.001           |\n",
                        "| weight_decay            |           0            |              |                          |            0             |\n",
                        "| max_epochs              |           \u001b[38;5;8m20\u001b[0m           |      10      |                          |            10            |\n",
                        "| max_steps               |           -1           |              |                          |            -1            |\n",
                        "| accumulate_grad_batches |           1            |              |                          |            1             |\n",
                        "| log_every_n_steps       |           50           |              |                          |            50            |\n",
                        "| num_workers             |           32           |              |                          |            32            |\n",
                        "| num_devices             |           1            |              |                          |            1             |\n",
                        "| num_nodes               |           1            |              |                          |            1             |\n",
                        "| accelerator             |          \u001b[38;5;8mauto\u001b[0m          |     gpu      |                          |           gpu            |\n",
                        "| strategy                |          auto          |              |                          |           auto           |\n",
                        "| is_to_auto_requeue      |         False          |              |                          |          False           |\n",
                        "| github_ci               |         False          |              |                          |          False           |\n",
                        "| disable_dataset_cache   |         False          |              |                          |          False           |\n",
                        "| target                  |  xcu250-figd2104-2L-e  |              |                          |   xcu250-figd2104-2L-e   |\n",
                        "| num_targets             |          100           |              |                          |           100            |\n",
                        "| is_pretrained           |         False          |              |                          |          False           |\n",
                        "| max_token_len           |          512           |              |                          |           512            |\n",
                        "| project_dir             | /root/mase/mase_output |              |                          |  /root/mase/mase_output  |\n",
                        "| project                 |          None          |              |                          |           None           |\n",
                        "| model                   |          \u001b[38;5;8mNone\u001b[0m          |   jsc-toy    |                          |         jsc-toy          |\n",
                        "| dataset                 |          \u001b[38;5;8mNone\u001b[0m          |     jsc      |                          |           jsc            |\n",
                        "| t_max                   |           20           |              |                          |            20            |\n",
                        "| eta_min                 |         1e-06          |              |                          |          1e-06           |\n",
                        "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'jsc-toy'...\u001b[0m\n",
                        "I0318 13:29:24.973379 139971211573056 cli.py:841] Initialising model 'jsc-toy'...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'jsc'...\u001b[0m\n",
                        "I0318 13:29:24.996573 139971211573056 cli.py:869] Initialising dataset 'jsc'...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/mase/mase_output/jsc-toy_cls_jsc_2024-03-18\u001b[0m\n",
                        "I0318 13:29:24.997021 139971211573056 cli.py:905] Project will be created at /root/mase/mase_output/jsc-toy_cls_jsc_2024-03-18\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'jsc-toy'...\u001b[0m\n",
                        "I0318 13:29:25.030123 139971211573056 cli.py:365] Transforming model 'jsc-toy'...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/jsc-toy-cls_jsc/best.ckpt\u001b[0m\n",
                        "I0318 13:29:27.502959 139971211573056 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/jsc-toy-cls_jsc/best.ckpt\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
                        "I0318 13:29:30.147488 139971211573056 utils.py:167] Applying fake quantization to PyTorch model...\n",
                        "\u001b[33mWARNING \u001b[0m \u001b[34mint8 precision not found in config. Skipping fake quantization.\u001b[0m\n",
                        "W0318 13:29:30.147828 139971211573056 utils.py:170] int8 precision not found in config. Skipping fake quantization.\n",
                        "\u001b[33mWARNING \u001b[0m \u001b[34mint8 precision not found in config. Skipping calibration.\u001b[0m\n",
                        "W0318 13:29:30.147962 139971211573056 calibrate.py:86] int8 precision not found in config. Skipping calibration.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
                        "I0318 13:29:30.171943 139971211573056 summary.py:84] Quantized graph histogram:\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         0 |           3 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
                        "I0318 13:29:30.173280 139971211573056 summary.py:85] \n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         0 |           3 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\n",
                        "\u001b[33mWARNING \u001b[0m \u001b[34mint8 precision not found in config. Skipping QAT fine tuning.\u001b[0m\n",
                        "W0318 13:29:30.174435 139971211573056 fine_tune.py:57] int8 precision not found in config. Skipping QAT fine tuning.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
                        "I0318 13:29:30.183183 139971211573056 summary.py:84] Quantized graph histogram:\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         0 |           3 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
                        "I0318 13:29:30.183739 139971211573056 summary.py:85] \n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         0 |           3 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
                        "I0318 13:29:30.185087 139971211573056 quantize.py:129] Converting PyTorch model to ONNX...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024_03_18/version_11/model.onnx\u001b[0m\n",
                        "I0318 13:29:32.601623 139971211573056 quantize.py:152] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024_03_18/version_11/model.onnx\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
                        "I0318 13:29:32.601973 139971211573056 quantize.py:55] Converting PyTorch model to TensorRT...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024_03_18/version_7/model.trt\u001b[0m\n",
                        "I0318 13:29:47.617324 139971211573056 quantize.py:124] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024_03_18/version_7/model.trt\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024_03_18/version_7/model.json\u001b[0m\n",
                        "I0318 13:29:47.872362 139971211573056 quantize.py:168] TensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024_03_18/version_7/model.json\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
                        "I0318 13:29:47.894693 139971211573056 summary.py:84] Quantized graph histogram:\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         0 |           3 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
                        "I0318 13:29:47.895272 139971211573056 summary.py:85] \n",
                        "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
                        "|-----------------+--------------+---------+-----------+-------------|\n",
                        "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
                        "| Linear          | linear       |       3 |         0 |           3 |\n",
                        "| ReLU            | relu         |       4 |         0 |           4 |\n",
                        "| output          | output       |       1 |         0 |           1 |\n",
                        "| x               | placeholder  |       1 |         0 |           1 |\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "TensorRT Engine Input/Output Information:\n",
                        "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
                        "------|---------|----------|----------------------|----------------------|-----------------------\n",
                        "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
                        "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 37\u001b[0m\n",
                        "I0318 13:29:48.377556 139971211573056 analysis.py:117] \n",
                        "TensorRT Engine Input/Output Information:\n",
                        "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
                        "------|---------|----------|----------------------|----------------------|-----------------------\n",
                        "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
                        "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 37\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
                        "I0318 13:29:48.377788 139971211573056 analysis.py:214] Starting transformation analysis\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results jsc-toy-quantized:\n",
                        "+------------------------------+---------------+\n",
                        "|            Metric            |     Value     |\n",
                        "+------------------------------+---------------+\n",
                        "|    Average Test Accuracy     |    0.73526    |\n",
                        "|      Average Precision       |    0.75069    |\n",
                        "|        Average Recall        |    0.73547    |\n",
                        "|       Average F1 Score       |    0.73897    |\n",
                        "|         Average Loss         |    0.74842    |\n",
                        "|       Average Latency        |  0.389921 ms  |\n",
                        "|   Average GPU Power Usage    |    60.18 W    |\n",
                        "| Inference Energy Consumption | 0.0055032 mWh |\n",
                        "+------------------------------+---------------+\u001b[0m\n",
                        "I0318 13:29:53.848961 139971211573056 analysis.py:330] \n",
                        "Results jsc-toy-quantized:\n",
                        "+------------------------------+---------------+\n",
                        "|            Metric            |     Value     |\n",
                        "+------------------------------+---------------+\n",
                        "|    Average Test Accuracy     |    0.73526    |\n",
                        "|      Average Precision       |    0.75069    |\n",
                        "|        Average Recall        |    0.73547    |\n",
                        "|       Average F1 Score       |    0.73897    |\n",
                        "|         Average Loss         |    0.74842    |\n",
                        "|       Average Latency        |  0.389921 ms  |\n",
                        "|   Average GPU Power Usage    |    60.18 W    |\n",
                        "| Inference Energy Consumption | 0.0055032 mWh |\n",
                        "+------------------------------+---------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mSaved mase graph to /root/mase/mase_output/jsc-toy_cls_jsc_2024-03-18/software/transform/transformed_ckpt\u001b[0m\n",
                        "I0318 13:29:53.973135 139971211573056 save_and_load.py:147] Saved mase graph to /root/mase/mase_output/jsc-toy_cls_jsc_2024-03-18/software/transform/transformed_ckpt\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTransformation is completed\u001b[0m\n",
                        "I0318 13:29:53.973508 139971211573056 cli.py:383] Transformation is completed\n"
                    ]
                }
            ],
            "source": [
                "!ch transform --config ../../../machop/configs/tensorrt/jsc_toy_fp16_quantization_by_type.toml --load {JSC_CHECKPOINT_PATH} --load-type pl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see, `fp16` acheives a slighty higher test accuracy but a slightly lower latency (~30%) from that of int8 quantization; it is still ~2.5x faster than the unquantized model. Now lets apply quantization to a more complicated model.\n",
                "\n",
                "## Section 3. Type-wise Mixed Precision on Larger Model\n",
                "We will now quantize `vgg7` which includes both convolutional and linear layers, however for this demonstration we want to quantize all layer types except the linear layers.\n",
                "\n",
                "In this case, we set:\n",
                "\n",
                "- The `by` parameter to `type`\n",
                "- The `quantize` parameter to true for `passes.tensorrt_quantize.conv2d.config` and `precision` parameter to 'int8'.\n",
                "- The `input` and `weight` quantize axis for the conv2d layers.\n",
                "- The default `passes.tensorrt_quantize.default.config` precision to true. \n",
                "\n",
                "During the TensorRT quantization, the model's conv2d layers will be converted to an int8 fake quantized form, whilst the linear layers are kept to their default 'fp16'. Calibration of the conv2d layers will be undergone and fine tuning.  \n",
                "\n",
                "You may either download a pretrained model [here](https://imperiallondon-my.sharepoint.com/:f:/g/personal/zz7522_ic_ac_uk/Emh3VT7Q_qRFmnp8kDrcgDoBwGUuzLwwKNtX8ZAt368jJQ?e=gsKONa), otherwise train it yourself as shown below. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ch train --config ../../../machop/configs/tensorrt/vgg7_layerwise_mixed_precision.toml.toml"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will now load the checkpoint in, quantize the model and compare it to the unquantized version as we did in [Section 1.5](#section-15-performance-analysis)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "VGG_CHECKPOINT_PATH = \"../../../mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-03-18 15:52:43,166] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
                        "INFO: Seed set to 0\n",
                        "WARNING: Logging before flag parsing goes to stderr.\n",
                        "I0318 15:52:44.743802 139761495775040 seed.py:54] Seed set to 0\n",
                        "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
                        "| Name                    |        Default         | Config. File |     Manual Override      |        Effective         |\n",
                        "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
                        "| task                    |     \u001b[38;5;8mclassification\u001b[0m     |     cls      |                          |           cls            |\n",
                        "| load_name               |          \u001b[38;5;8mNone\u001b[0m          |              | /root/mase/mase_output/v | /root/mase/mase_output/v |\n",
                        "|                         |                        |              |  gg7-pre-trained/test-   |  gg7-pre-trained/test-   |\n",
                        "|                         |                        |              |     accu-0.9332.ckpt     |     accu-0.9332.ckpt     |\n",
                        "| load_type               |           \u001b[38;5;8mmz\u001b[0m           |              |            pl            |            pl            |\n",
                        "| batch_size              |          \u001b[38;5;8m128\u001b[0m           |     256      |                          |           256            |\n",
                        "| to_debug                |         False          |              |                          |          False           |\n",
                        "| log_level               |          info          |              |                          |           info           |\n",
                        "| report_to               |      tensorboard       |              |                          |       tensorboard        |\n",
                        "| seed                    |           0            |              |                          |            0             |\n",
                        "| quant_config            |          None          |              |                          |           None           |\n",
                        "| training_optimizer      |          adam          |              |                          |           adam           |\n",
                        "| trainer_precision       |        16-mixed        |              |                          |         16-mixed         |\n",
                        "| learning_rate           |         \u001b[38;5;8m1e-05\u001b[0m          |    0.001     |                          |          0.001           |\n",
                        "| weight_decay            |           0            |              |                          |            0             |\n",
                        "| max_epochs              |           \u001b[38;5;8m20\u001b[0m           |      10      |                          |            10            |\n",
                        "| max_steps               |           -1           |              |                          |            -1            |\n",
                        "| accumulate_grad_batches |           1            |              |                          |            1             |\n",
                        "| log_every_n_steps       |           50           |              |                          |            50            |\n",
                        "| num_workers             |           32           |              |                          |            32            |\n",
                        "| num_devices             |           1            |              |                          |            1             |\n",
                        "| num_nodes               |           1            |              |                          |            1             |\n",
                        "| accelerator             |          \u001b[38;5;8mauto\u001b[0m          |     gpu      |                          |           gpu            |\n",
                        "| strategy                |          auto          |              |                          |           auto           |\n",
                        "| is_to_auto_requeue      |         False          |              |                          |          False           |\n",
                        "| github_ci               |         False          |              |                          |          False           |\n",
                        "| disable_dataset_cache   |         False          |              |                          |          False           |\n",
                        "| target                  |  xcu250-figd2104-2L-e  |              |                          |   xcu250-figd2104-2L-e   |\n",
                        "| num_targets             |          100           |              |                          |           100            |\n",
                        "| is_pretrained           |         False          |              |                          |          False           |\n",
                        "| max_token_len           |          512           |              |                          |           512            |\n",
                        "| project_dir             | /root/mase/mase_output |              |                          |  /root/mase/mase_output  |\n",
                        "| project                 |          None          |              |                          |           None           |\n",
                        "| model                   |          \u001b[38;5;8mNone\u001b[0m          |     vgg7     |                          |           vgg7           |\n",
                        "| dataset                 |          \u001b[38;5;8mNone\u001b[0m          |   cifar10    |                          |         cifar10          |\n",
                        "| t_max                   |           20           |              |                          |            20            |\n",
                        "| eta_min                 |         1e-06          |              |                          |          1e-06           |\n",
                        "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'vgg7'...\u001b[0m\n",
                        "I0318 15:52:44.753937 139761495775040 cli.py:841] Initialising model 'vgg7'...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'cifar10'...\u001b[0m\n",
                        "I0318 15:52:44.878532 139761495775040 cli.py:869] Initialising dataset 'cifar10'...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-18\u001b[0m\n",
                        "I0318 15:52:44.878995 139761495775040 cli.py:905] Project will be created at /root/mase/mase_output/vgg7_cls_cifar10_2024-03-18\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'vgg7'...\u001b[0m\n",
                        "I0318 15:52:44.909238 139761495775040 cli.py:365] Transforming model 'vgg7'...\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\u001b[0m\n",
                        "I0318 15:52:50.380097 139761495775040 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\n"
                    ]
                }
            ],
            "source": [
                "!ch transform --config ../../../machop/configs/tensorrt/vgg7_typewise_mixed_precision.toml --load {VGG_CHECKPOINT_PATH} --load-type pl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 4. Layer-wise Mixed Precision\n",
                "\n",
                "So far we have strictly quantized either in int8 or fp16. Now, we will show how to conduct layerwise mixed precision using the same `vgg7` model. In this case we will show how for instance, layer 0 and 1 can be set to fp16, while layer 2 and 3 can be int8 quantized. \n",
                "\n",
                "For this, we set:\n",
                "- The `by` parameter to `name`\n",
                "- The `precision` to 'fp16' for `passes.tensorrt_quantize.feature_layers_0.config and passes.tensorrt_quantize.feature_layers_1.config`\n",
                "- The `precision` to 'int8' for `passes.tensorrt_quantize.feature_layers_0.config and passes.tensorrt_quantize.feature_layers_1.config`\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/root/anaconda3/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-03-19 16:33:49,791] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
                        "WARNING: Logging before flag parsing goes to stderr.\n",
                        "I0319 16:33:51.317272 140297220093760 logger.py:44] Set logging level to info\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import toml\n",
                "from copy import copy, deepcopy\n",
                "\n",
                "# Figure out the correct path\n",
                "machop_path = Path(\".\").resolve().parent.parent.parent /\"machop\"\n",
                "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
                "sys.path.append(str(machop_path))\n",
                "\n",
                "# Add directory to the PATH so that chop can be called\n",
                "new_path = \"../../../machop\"\n",
                "full_path = os.path.abspath(new_path)\n",
                "os.environ['PATH'] += os.pathsep + full_path\n",
                "\n",
                "from chop.tools.utils import to_numpy_if_tensor\n",
                "from chop.tools.logger import set_logging_verbosity\n",
                "from chop.tools import get_cf_args, get_dummy_input\n",
                "from chop.passes.graph.utils import deepcopy_mase_graph\n",
                "from chop.tools.get_input import InputGenerator\n",
                "from chop.tools.checkpoint_load import load_model\n",
                "from chop.ir import MaseGraph\n",
                "from chop.models import get_model_info, get_model, get_tokenizer\n",
                "from chop.dataset import MaseDataModule, get_dataset_info\n",
                "from chop.passes.graph.transforms import metadata_value_type_cast_transform_pass\n",
                "from chop.passes.graph import (\n",
                "    summarize_quantization_analysis_pass,\n",
                "    add_common_metadata_analysis_pass,\n",
                "    init_metadata_analysis_pass,\n",
                "    add_software_metadata_analysis_pass,\n",
                "    tensorrt_calibrate_transform_pass,\n",
                "    tensorrt_fake_quantize_transform_pass,\n",
                "    tensorrt_fine_tune_transform_pass,\n",
                "    tensorrt_engine_interface_pass,\n",
                "    runtime_analysis_pass,\n",
                "    )\n",
                "\n",
                "set_logging_verbosity(\"info\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n"
                    ]
                }
            ],
            "source": [
                "# Path to your TOML file\n",
                "# toml_file_path = '../../../machop/configs/tensorrt/vgg7_layerwise_mixed_precision.toml'\n",
                "toml_file_path = '../../../machop/configs/tensorrt/vgg7_typewise_mixed_precision.toml'\n",
                "\n",
                "# Reading TOML file and converting it into a Python dictionary\n",
                "with open(toml_file_path, 'r') as toml_file:\n",
                "    pass_args = toml.load(toml_file)\n",
                "\n",
                "# Extract the 'passes.tensorrt' section and its children\n",
                "tensorrt_config = pass_args.get('passes', {}).get('tensorrt', {})\n",
                "# Extract the 'passes.runtime_analysis' section and its children\n",
                "runtime_analysis_config = pass_args.get('passes', {}).get('runtime_analysis', {})\n",
                "\n",
                "# Load the basics in\n",
                "model_name = pass_args['model']\n",
                "dataset_name = pass_args['dataset']\n",
                "max_epochs = pass_args['max_epochs']\n",
                "batch_size = pass_args['batch_size']\n",
                "learning_rate = pass_args['learning_rate']\n",
                "accelerator = pass_args['accelerator']\n",
                "\n",
                "data_module = MaseDataModule(\n",
                "    name=dataset_name,\n",
                "    batch_size=batch_size,\n",
                "    model_name=model_name,\n",
                "    num_workers=0,\n",
                ")\n",
                "data_module.prepare_data()\n",
                "data_module.setup()\n",
                "\n",
                "# Add the data_module and other necessary information to the configs\n",
                "configs = [tensorrt_config, runtime_analysis_config]\n",
                "for config in configs:\n",
                "    config['task'] = pass_args['task']\n",
                "    config['batch_size'] = pass_args['batch_size']\n",
                "    config['model'] = pass_args['model']\n",
                "    config['data_module'] = data_module\n",
                "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
                "    if config['accelerator'] == 'gpu':\n",
                "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
                "\n",
                "model_info = get_model_info(model_name)\n",
                "model = get_model(\n",
                "    model_name,\n",
                "    task=\"cls\",\n",
                "    dataset_info=data_module.dataset_info,\n",
                "    pretrained=False)\n",
                "\n",
                "input_generator = InputGenerator(\n",
                "    data_module=data_module,\n",
                "    model_info=model_info,\n",
                "    task=\"cls\",\n",
                "    which_dataloader=\"train\",\n",
                ")\n",
                "\n",
                "# generate the mase graph and initialize node metadata\n",
                "mg = MaseGraph(model=model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from ../../../mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\u001b[0m\n",
                        "I0318 15:33:56.843245 140155325249344 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from ../../../mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\n"
                    ]
                }
            ],
            "source": [
                "# Load in the trained checkpoint - change this accordingly\n",
                "VGG_CHECKPOINT_PATH = \"../../../mase_output/vgg7-pre-trained/test-accu-0.9332.ckpt\"\n",
                "\n",
                "model = load_model(load_name=VGG_CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
                "\n",
                "# Initiate metadata\n",
                "dummy_in = next(iter(input_generator))\n",
                "_ = model(**dummy_in)\n",
                "mg, _ = init_metadata_analysis_pass(mg, None)\n",
                "\n",
                "mg_original = deepcopy_mase_graph(mg)\n",
                "\n",
                "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
                "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
                "mg, _ = metadata_value_type_cast_transform_pass(mg, pass_args={\"fn\": to_numpy_if_tensor})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
                        "I0318 09:55:37.107798 140195009255232 analysis.py:214] Starting transformation analysis\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results vgg7:\n",
                        "+------------------------------+-------------+\n",
                        "|            Metric            |    Value    |\n",
                        "+------------------------------+-------------+\n",
                        "|    Average Test Accuracy     |   0.91967   |\n",
                        "|      Average Precision       |   0.92315   |\n",
                        "|        Average Recall        |   0.92362   |\n",
                        "|       Average F1 Score       |   0.92326   |\n",
                        "|         Average Loss         |   0.23674   |\n",
                        "|       Average Latency        |  28.123 ms  |\n",
                        "|   Average GPU Power Usage    |  100.17 W   |\n",
                        "| Inference Energy Consumption | 0.78256 mWh |\n",
                        "+------------------------------+-------------+\u001b[0m\n",
                        "I0318 09:55:40.969246 140195009255232 analysis.py:330] \n",
                        "Results vgg7:\n",
                        "+------------------------------+-------------+\n",
                        "|            Metric            |    Value    |\n",
                        "+------------------------------+-------------+\n",
                        "|    Average Test Accuracy     |   0.91967   |\n",
                        "|      Average Precision       |   0.92315   |\n",
                        "|        Average Recall        |   0.92362   |\n",
                        "|       Average F1 Score       |   0.92326   |\n",
                        "|         Average Loss         |   0.23674   |\n",
                        "|       Average Latency        |  28.123 ms  |\n",
                        "|   Average GPU Power Usage    |  100.17 W   |\n",
                        "| Inference Energy Consumption | 0.78256 mWh |\n",
                        "+------------------------------+-------------+\n"
                    ]
                }
            ],
            "source": [
                "_, _ = runtime_analysis_pass(mg, pass_args=runtime_analysis_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
                        "I0318 15:34:00.409916 140155325249344 utils.py:168] Applying fake quantization to PyTorch model...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
                        "I0318 15:34:00.682099 140155325249344 utils.py:193] Fake quantization applied to PyTorch model.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
                        "I0318 15:34:00.683382 140155325249344 calibrate.py:91] Starting calibration of the model in PyTorch...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.694193 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.695298 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.695932 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.696544 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.697141 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.697722 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.698303 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.698865 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.699461 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.700013 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.700582 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
                        "I0318 15:34:00.701155 140155325249344 calibrate.py:100] Disabling Quantization and Enabling Calibration\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.481201 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.482733 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.483446 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.484515 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.485068 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.486183 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.486567 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.487422 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.487805 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.488553 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.488957 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.489720 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.490093 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.490822 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.491214 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.491982 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.492378 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.493196 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.493597 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.494379 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.494779 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.495552 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
                        "I0318 15:34:02.495936 140155325249344 calibrate.py:121] Enabling Quantization and Disabling Calibration\n",
                        "W0318 15:34:02.496721 140155325249344 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
                        "W0318 15:34:02.503043 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "W0318 15:34:02.503463 140155325249344 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=2.6392 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.504164 140155325249344 calibrate.py:79] feature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=2.6392 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.505316 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2797 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.505767 140155325249344 calibrate.py:79] feature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2797 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.507040 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.3020 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.507491 140155325249344 calibrate.py:79] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.3020 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.508672 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2366 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.509136 140155325249344 calibrate.py:79] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2366 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.510375 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=1.8184 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.510823 140155325249344 calibrate.py:79] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=1.8184 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.512043 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2296 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.512472 140155325249344 calibrate.py:79] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.2296 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.513764 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.4591 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.514211 140155325249344 calibrate.py:79] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.4591 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.515444 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2080 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.515901 140155325249344 calibrate.py:79] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2080 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.517146 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.9262 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.517589 140155325249344 calibrate.py:79] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.9262 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.518806 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2013 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.519248 140155325249344 calibrate.py:79] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2013 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.520494 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.6157 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.520926 140155325249344 calibrate.py:79] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=1.6157 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:02.522181 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.1879 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:02.522614 140155325249344 calibrate.py:79] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.1879 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
                        "I0318 15:34:02.524394 140155325249344 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.0...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
                        "I0318 15:34:02.525266 140155325249344 analysis.py:214] Starting transformation analysis\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results vgg7:\n",
                        "+------------------------------+------------+\n",
                        "|            Metric            |   Value    |\n",
                        "+------------------------------+------------+\n",
                        "|    Average Test Accuracy     |  0.91682   |\n",
                        "|      Average Precision       |  0.91546   |\n",
                        "|        Average Recall        |  0.91579   |\n",
                        "|       Average F1 Score       |  0.91553   |\n",
                        "|         Average Loss         |  0.25923   |\n",
                        "|       Average Latency        | 36.294 ms  |\n",
                        "|   Average GPU Power Usage    |  108.73 W  |\n",
                        "| Inference Energy Consumption | 1.0962 mWh |\n",
                        "+------------------------------+------------+\u001b[0m\n",
                        "I0318 15:34:06.029327 140155325249344 analysis.py:323] \n",
                        "Results vgg7:\n",
                        "+------------------------------+------------+\n",
                        "|            Metric            |   Value    |\n",
                        "+------------------------------+------------+\n",
                        "|    Average Test Accuracy     |  0.91682   |\n",
                        "|      Average Precision       |  0.91546   |\n",
                        "|        Average Recall        |  0.91579   |\n",
                        "|       Average F1 Score       |  0.91553   |\n",
                        "|         Average Loss         |  0.25923   |\n",
                        "|       Average Latency        | 36.294 ms  |\n",
                        "|   Average GPU Power Usage    |  108.73 W  |\n",
                        "| Inference Energy Consumption | 1.0962 mWh |\n",
                        "+------------------------------+------------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
                        "I0318 15:34:06.030981 140155325249344 calibrate.py:66] Post calibration analysis complete.\n",
                        "W0318 15:34:06.032042 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=2.6392 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.032518 140155325249344 calibrate.py:79] feature_layers.0._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=2.6392 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.033420 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3434 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.033899 140155325249344 calibrate.py:79] feature_layers.0._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3434 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.034892 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=6.0151 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.035389 140155325249344 calibrate.py:79] feature_layers.3._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=6.0151 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.036347 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3704 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.036843 140155325249344 calibrate.py:79] feature_layers.3._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3704 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.037833 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2875 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.038331 140155325249344 calibrate.py:79] feature_layers.7._input_quantizer       : TensorQuantizer(8bit fake per-tensor amax=3.2875 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.039333 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3621 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.039835 140155325249344 calibrate.py:79] feature_layers.7._weight_quantizer      : TensorQuantizer(8bit fake per-tensor amax=0.3621 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.040834 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.4120 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.041361 140155325249344 calibrate.py:79] feature_layers.10._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.4120 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.042325 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2821 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.042822 140155325249344 calibrate.py:79] feature_layers.10._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2821 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.043805 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.9829 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.044299 140155325249344 calibrate.py:79] feature_layers.14._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.9829 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.045269 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2734 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.045764 140155325249344 calibrate.py:79] feature_layers.14._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2734 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.046752 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.7024 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.047246 140155325249344 calibrate.py:79] feature_layers.17._input_quantizer      : TensorQuantizer(8bit fake per-tensor amax=2.7024 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "W0318 15:34:06.048205 140155325249344 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mfeature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2519 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
                        "I0318 15:34:06.048707 140155325249344 calibrate.py:79] feature_layers.17._weight_quantizer     : TensorQuantizer(8bit fake per-tensor amax=0.2519 calibrator=HistogramCalibrator scale=1.0 quant)\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
                        "I0318 15:34:06.050029 140155325249344 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.9...\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
                        "I0318 15:34:06.050704 140155325249344 analysis.py:214] Starting transformation analysis\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results vgg7:\n",
                        "+------------------------------+-----------+\n",
                        "|            Metric            |   Value   |\n",
                        "+------------------------------+-----------+\n",
                        "| Average Validation Accuracy  |  0.92441  |\n",
                        "|      Average Precision       |  0.92283  |\n",
                        "|        Average Recall        |  0.92325  |\n",
                        "|       Average F1 Score       |  0.92293  |\n",
                        "|         Average Loss         |  0.23622  |\n",
                        "|       Average Latency        | 36.334 ms |\n",
                        "|   Average GPU Power Usage    | 111.46 W  |\n",
                        "| Inference Energy Consumption | 1.125 mWh |\n",
                        "+------------------------------+-----------+\u001b[0m\n",
                        "I0318 15:34:09.521739 140155325249344 analysis.py:323] \n",
                        "Results vgg7:\n",
                        "+------------------------------+-----------+\n",
                        "|            Metric            |   Value   |\n",
                        "+------------------------------+-----------+\n",
                        "| Average Validation Accuracy  |  0.92441  |\n",
                        "|      Average Precision       |  0.92283  |\n",
                        "|        Average Recall        |  0.92325  |\n",
                        "|       Average F1 Score       |  0.92293  |\n",
                        "|         Average Loss         |  0.23622  |\n",
                        "|       Average Latency        | 36.334 ms |\n",
                        "|   Average GPU Power Usage    | 111.46 W  |\n",
                        "| Inference Energy Consumption | 1.125 mWh |\n",
                        "+------------------------------+-----------+\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
                        "I0318 15:34:09.524197 140155325249344 calibrate.py:66] Post calibration analysis complete.\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
                        "I0318 15:34:09.525355 140155325249344 calibrate.py:159] Succeeded in calibrating the model in PyTorch!\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting Fine Tuning for 2 epochs...\u001b[0m\n",
                        "I0318 15:34:09.568219 140155325249344 fine_tune.py:101] Starting Fine Tuning for 2 epochs...\n",
                        "I0318 15:34:09.647709 140155325249344 rank_zero.py:64] GPU available: True (cuda), used: True\n",
                        "I0318 15:34:09.662323 140155325249344 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
                        "I0318 15:34:09.662955 140155325249344 rank_zero.py:64] IPU available: False, using: 0 IPUs\n",
                        "I0318 15:34:09.663497 140155325249344 rank_zero.py:64] HPU available: False, using: 0 HPUs\n",
                        "I0318 15:34:09.669328 140155325249344 rank_zero.py:64] You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n",
                        "Files already downloaded and verified\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "I0318 15:34:13.796578 140155325249344 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "I0318 15:34:13.808069 140155325249344 model_summary.py:94] \n",
                        "  | Name      | Type               | Params\n",
                        "-------------------------------------------------\n",
                        "0 | model     | GraphModule        | 14.0 M\n",
                        "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
                        "2 | acc_train | MulticlassAccuracy | 0     \n",
                        "3 | loss_val  | MeanMetric         | 0     \n",
                        "4 | loss_test | MeanMetric         | 0     \n",
                        "-------------------------------------------------\n",
                        "14.0 M    Trainable params\n",
                        "0         Non-trainable params\n",
                        "14.0 M    Total params\n",
                        "56.118    Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sanity Checking DataLoader 0:  50%|     | 1/2 [00:00<00:00, 13.05it/s]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                                                                           "
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1: 100%|| 196/196 [00:43<00:00,  4.55it/s, v_num=8, train_acc_step=0.863, val_acc_epoch=0.893, val_loss_epoch=0.223]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "I0318 15:35:42.103955 140155325249344 rank_zero.py:64] `Trainer.fit` stopped: `max_epochs=2` reached.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1: 100%|| 196/196 [00:43<00:00,  4.52it/s, v_num=8, train_acc_step=0.863, val_acc_epoch=0.893, val_loss_epoch=0.223]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mFine Tuning Complete\u001b[0m\n",
                        "I0318 15:35:42.213792 140155325249344 fine_tune.py:120] Fine Tuning Complete\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
                        "I0318 15:35:42.219084 140155325249344 quantize.py:129] Converting PyTorch model to ONNX...\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
                        "  if min_amax < 0:\n",
                        "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
                        "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
                        "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
                        "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
                        "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
                        "  if min_amax <= epsilon:\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024_03_18/version_15/model.onnx\u001b[0m\n",
                        "I0318 15:35:43.000104 140155325249344 quantize.py:152] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024_03_18/version_15/model.onnx\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
                        "I0318 15:35:43.003496 140155325249344 quantize.py:55] Converting PyTorch model to TensorRT...\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[03/18/2024-15:35:48] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024_03_18/version_10/model.trt\u001b[0m\n",
                        "I0318 15:38:52.261729 140155325249344 quantize.py:124] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024_03_18/version_10/model.trt\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024_03_18/version_10/model.json\u001b[0m\n",
                        "I0318 15:38:52.553308 140155325249344 quantize.py:168] TensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024_03_18/version_10/model.json\n"
                    ]
                }
            ],
            "source": [
                "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_config)\n",
                "# summarize_quantization_analysis_pass(mg_original, mg)\n",
                "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_config)\n",
                "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_config)\n",
                "mg, meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "TensorRT Engine Input/Output Information:\n",
                        "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
                        "------|---------|----------|----------------------|----------------------|-----------------------\n",
                        "0     | Input   | FLOAT    | (256, 3, 32, 32)       | (256, 3, 32, 32)       | input\n",
                        "1     | Output  | FLOAT    | (256, 10)              | (256, 10)              | 220\u001b[0m\n",
                        "I0318 15:38:52.609910 140155325249344 analysis.py:117] \n",
                        "TensorRT Engine Input/Output Information:\n",
                        "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
                        "------|---------|----------|----------------------|----------------------|-----------------------\n",
                        "0     | Input   | FLOAT    | (256, 3, 32, 32)       | (256, 3, 32, 32)       | input\n",
                        "1     | Output  | FLOAT    | (256, 10)              | (256, 10)              | 220\n",
                        "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
                        "I0318 15:38:52.611444 140155325249344 analysis.py:214] Starting transformation analysis\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
                        "Results vgg7-quantized:\n",
                        "+------------------------------+-----------+\n",
                        "|            Metric            |   Value   |\n",
                        "+------------------------------+-----------+\n",
                        "|    Average Test Accuracy     |  0.93222  |\n",
                        "|      Average Precision       |  0.93206  |\n",
                        "|        Average Recall        |  0.93222  |\n",
                        "|       Average F1 Score       |  0.93204  |\n",
                        "|         Average Loss         |  0.22506  |\n",
                        "|       Average Latency        | 19.479 ms |\n",
                        "|   Average GPU Power Usage    | 110.15 W  |\n",
                        "| Inference Energy Consumption | 0.596 mWh |\n",
                        "+------------------------------+-----------+\u001b[0m\n",
                        "I0318 15:38:55.434816 140155325249344 analysis.py:323] \n",
                        "Results vgg7-quantized:\n",
                        "+------------------------------+-----------+\n",
                        "|            Metric            |   Value   |\n",
                        "+------------------------------+-----------+\n",
                        "|    Average Test Accuracy     |  0.93222  |\n",
                        "|      Average Precision       |  0.93206  |\n",
                        "|        Average Recall        |  0.93222  |\n",
                        "|       Average F1 Score       |  0.93204  |\n",
                        "|         Average Loss         |  0.22506  |\n",
                        "|       Average Latency        | 19.479 ms |\n",
                        "|   Average GPU Power Usage    | 110.15 W  |\n",
                        "| Inference Energy Consumption | 0.596 mWh |\n",
                        "+------------------------------+-----------+\n"
                    ]
                }
            ],
            "source": [
                "_, _ = runtime_analysis_pass(meta['trt_engine_path'], pass_args=runtime_analysis_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Section 5. Language Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to your TOML file\n",
                "toml_file_path = '../../../machop/configs/tensorrt/opt-125M_layerwise_mixed_precision_by_name.toml'\n",
                "\n",
                "# Reading TOML file and converting it into a Python dictionary\n",
                "with open(toml_file_path, 'r') as toml_file:\n",
                "    pass_args = toml.load(toml_file)\n",
                "\n",
                "# Extract the 'passes.tensorrt' section and its children\n",
                "tensorrt_config = pass_args.get('passes', {}).get('tensorrt', {})\n",
                "# Extract the 'passes.runtime_analysis' section and its children\n",
                "runtime_analysis_config = pass_args.get('passes', {}).get('runtime_analysis', {})\n",
                "\n",
                "# Load the basics in\n",
                "model_name = pass_args['model']\n",
                "dataset_name = pass_args['dataset']\n",
                "max_epochs = pass_args['max_epochs']\n",
                "batch_size = pass_args['batch_size']\n",
                "learning_rate = pass_args['learning_rate']\n",
                "accelerator = pass_args['accelerator']\n",
                "\n",
                "opt_tokenizer = get_tokenizer(\"facebook/opt-125m\")\n",
                "\n",
                "data_module = MaseDataModule(\n",
                "    name=dataset_name,\n",
                "    batch_size=batch_size,\n",
                "    model_name=model_name,\n",
                "    num_workers=0, # os.cpu_count()\n",
                "    max_token_len=128,\n",
                "    tokenizer=opt_tokenizer,\n",
                "    load_from_cache_file=True,\n",
                ")\n",
                "data_module.prepare_data()\n",
                "data_module.setup()\n",
                "\n",
                "# Add the data_module and other necessary information to the configs\n",
                "configs = [tensorrt_config, runtime_analysis_config]\n",
                "for config in configs:\n",
                "    config['task'] = pass_args['task']\n",
                "    config['batch_size'] = pass_args['batch_size']\n",
                "    config['model'] = pass_args['model']\n",
                "    config['data_module'] = data_module\n",
                "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
                "    if config['accelerator'] == 'gpu':\n",
                "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_info = get_model_info(model_name)\n",
                "model = get_model(\n",
                "    \"facebook/opt-125m:patched\",\n",
                "    task=\"lm\",\n",
                "    dataset_info=get_dataset_info(\"wikitext2\"),\n",
                "    pretrained=True,\n",
                ")\n",
                "\n",
                "# Load in the trained checkpoint - change this accordingly\n",
                "# OPT125M_CHECKPOINT_PATH = \"../../../mase_output/jsc-toy_classification_jsc_2024-03-17/software/training_ckpts/best.ckpt\"\n",
                "# model = load_model(load_name=OPT125M_CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
                "\n",
                "model_info = get_model_info(\"facebook/opt-125m:patched\")\n",
                "cf_args = get_cf_args(model_info=model_info, task=\"lm\", model=model)\n",
                "\n",
                "mg = MaseGraph(model=model, cf_args=cf_args)\n",
                "\n",
                "# dummy_in = get_dummy_input(model_info, data_module=data_module, task=\"lm\")\n",
                "# if len(mg.model.additional_inputs) > 0:\n",
                "#     dummy_in = dummy_in | mg.model.additional_inputs\n",
                "\n",
                "# Initiate metadata\n",
                "mg, _ = init_metadata_analysis_pass(mg, pass_args=None)\n",
                "\n",
                "# # Before we begin, we will copy the original MaseGraph model to use for comparison during quantization analysis\n",
                "# mg_original = deepcopy_mase_graph(mg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# _, _ = runtime_analysis_pass(mg, pass_args=runtime_analysis_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
                        "I0318 08:54:47.010779 139760749061952 quantize.py:129] Converting PyTorch model to ONNX...\n",
                        "ERROR:tornado.general:SEND Error: Host unreachable\n",
                        "Traceback (most recent call last):\n",
                        "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 577, in _pydevd_bundle.pydevd_cython.PyDBFrame._handle_exception\n",
                        "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 312, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
                        "  File \"/opt/conda/envs/mase/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2070, in do_wait_suspend\n",
                        "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
                        "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "  File \"/opt/conda/envs/mase/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2106, in _do_wait_suspend\n",
                        "    time.sleep(0.01)\n",
                        "KeyboardInterrupt\n"
                    ]
                }
            ],
            "source": [
                "# mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_config)\n",
                "# summarize_quantization_analysis_pass(mg_original, mg)\n",
                "\n",
                "# mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_config)\n",
                "\n",
                "# mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_config)\n",
                "\n",
                "mg, meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_config)\n",
                "\n",
                "_, _ = runtime_analysis_pass(mg_original, pass_args=runtime_analysis_config)\n",
                "_, _ = runtime_analysis_pass(meta['trt_engine_path'], pass_args=runtime_analysis_config)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "mase",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
