{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome To the TensorRT Quantization Tutorial!\n",
    "\n",
    "This notebook is designed to show the features of the TensorRT passes integrated into MASE.\n",
    "\n",
    "## Section 1. INT8 Quantization\n",
    "Firstly, we will show you how to do a INT8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model using the `Machop API`. The quantization process is split into the following stages, each using their own individual pass, and are explained in depth at each subsection:\n",
    "\n",
    "1. [Fake quantization](#section-11-fake-quantization): `tensorrt_fake_quantize_transform_pass`\n",
    "2. [Calibration](#sect): `tensorrt_calibrate_transform_pass`\n",
    "3. [Quantized Aware Training](#quantized-aware-training): `tensorrt_fine_tune_transform_pass`\n",
    "4. [Quantization](#quantization): `tensorrt_engine_interface_pass`\n",
    "5. [Analysis](#analysis): `tensorrt_analysis_pass`\n",
    "\n",
    "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:758: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-17 18:02:17,446] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0317 18:02:19.461184 140043280435008 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import toml\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "# Figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "# Add directory to the PATH so that chop can be called\n",
    "new_path = \"../../../machop\"\n",
    "full_path = os.path.abspath(new_path)\n",
    "os.environ['PATH'] += os.pathsep + full_path\n",
    "\n",
    "from chop.tools.utils import to_numpy_if_tensor\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "from chop.models import get_model_info, get_model\n",
    "from chop.dataset import MaseDataModule\n",
    "from chop.passes.graph.transforms import metadata_value_type_cast_transform_pass\n",
    "from chop.passes.graph import (\n",
    "    summarize_quantization_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    tensorrt_analysis_pass,\n",
    "    )\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml), or read the documentation on Mase [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your TOML file\n",
    "toml_file_path = '../../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(toml_file_path, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt_quantize' section and its children\n",
    "tensorrt_quantize_config = pass_args.get('passes', {}).get('tensorrt_quantize', {})\n",
    "# Extract the 'passes.tensorrt_analysis' section and its children\n",
    "tensorrt_analysis_config = pass_args.get('passes', {}).get('tensorrt_analysis', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MaseGraph` by loading in a pre-trained model using the checkpoint provided and using the toml configuration model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# quant_modules.initialize()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(\n\u001b[1;32m     33\u001b[0m     model_name,\n\u001b[1;32m     34\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     dataset_info\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdataset_info,\n\u001b[1;32m     36\u001b[0m     pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m input_generator \u001b[38;5;241m=\u001b[39m InputGenerator(\n\u001b[1;32m     41\u001b[0m     data_module\u001b[38;5;241m=\u001b[39mdata_module,\n\u001b[1;32m     42\u001b[0m     model_info\u001b[38;5;241m=\u001b[39mmodel_info,\n\u001b[1;32m     43\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     which_dataloader\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m dummy_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(input_generator))\n",
      "File \u001b[0;32m~/mase/machop/chop/tools/checkpoint_load.py:82\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(load_name, load_type, model)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m load_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     79\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightning checkpoint should end with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m         )\n\u001b[0;32m---> 82\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_lightning_ckpt_to_unwrapped_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded pytorch lightning checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mase/machop/chop/tools/checkpoint_load.py:13\u001b[0m, in \u001b[0;36mload_lightning_ckpt_to_unwrapped_model\u001b[0;34m(checkpoint, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lightning_ckpt_to_unwrapped_model\u001b[39m(checkpoint: \u001b[38;5;28mstr\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Load a PyTorch Lightning checkpoint to a PyTorch model.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     src_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m     tgt_state_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     15\u001b[0m     new_tgt_state_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/opt/conda/envs/mase/lib/python3.11/site-packages/torch/serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "# Load the basics in to \n",
    "model_name = pass_args['model']\n",
    "dataset_name = pass_args['dataset']\n",
    "max_epochs = pass_args['max_epochs']\n",
    "batch_size = pass_args['batch_size']\n",
    "learning_rate = pass_args['learning_rate']\n",
    "accelerator = pass_args['accelerator']\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# Add the data_module and other necessary information to the configs\n",
    "configs = [tensorrt_quantize_config, tensorrt_analysis_config]\n",
    "for config in configs:\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "mg, _ = metadata_value_type_cast_transform_pass(mg, pass_args={\"fn\": to_numpy_if_tensor})\n",
    "\n",
    "# Before we begin, we will copy the original MaseGraph model to use for comparison during quantization analysis\n",
    "mg_original = deepcopy_mase_graph(mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1 Fake Quantization\n",
    "\n",
    "Firstly, we fake quantize the module in order to perform calibration and fine tuning before actually quantizing - this is only used if we have INT8 calibration as other precisions are not currently supported within [pytorch-quantization](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#) library.\n",
    "\n",
    "This is acheived through the `tensorrt_fake_quantize_transform_pass` which goes through the model, either by type or by name, replaces each layer appropriately to a fake quantized form if the `quantize` parameter is set in the default config (`passes.tensorrt_quantize.default.config`) or on a per name or type basis. I.e. if we would like to only quantize the linear layers but not the convolutional layers, we could set the `quantize` parameter to true for `passes.tensorrt_quantize.linear.config` and the default to false.\n",
    "\n",
    "Currently the quantizable layers are:\n",
    "- Linear\n",
    "- Conv1d, Conv2d, ConvNd \n",
    "- ConvTranspose1d, ConvTranspose2d, ConvTransposeNd \n",
    "- MaxPool1d, MaxPool2d, MaxPool3d\n",
    "- AvgPool1d, AvgPool2d, AvgPool3d\n",
    "- Clip (Tensor)\n",
    "- LSTM, LSTMCell\n",
    "\n",
    "To create a custom quantized module, click [here](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#document-tutorials/creating_custom_quantized_modules).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0317 15:03:21.413895 140341721720640 utils.py:166] Applying fake quantization to PyTorch model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0317 15:03:22.608005 140341721720640 utils.py:182] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "I0317 15:03:22.673146 140341721720640 summary.py:84] Quantized graph histogram:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| ReLU            | relu         |       4 |         0 |           4 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
      "I0317 15:03:22.679919 140341721720640 summary.py:85] \n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| ReLU            | relu         |       4 |         0 |           4 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "summarize_quantization_analysis_pass(mg_original, mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have succesfully quantized all linear layers inside `jsc-toy`. See Section X for how to apply quantization layerwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2 Calibration\n",
    "\n",
    "Next, we perform calibration using the `tensorrt_calibrate_transform_pass`. Calibration is achieved by passing data samples to the quantizer and deciding the best amax for activations. \n",
    "\n",
    "Calibrators can be added as a search space parameter to examine the best performing calibrator. The calibrators have been included in the toml as follows.\n",
    "For example: `calibrators = [\"percentile\", \"mse\", \"entropy\"]`\n",
    "\n",
    "Note: \n",
    "- To use `percentile` calibration, a list of percentiles must be given\n",
    "- To use `max` calibration, the `histogram` weight and input calibrators must be removed and replaced with `max`. This will use global maximum absolute value to calibrate the model.\n",
    "- If `post_calibration_analysis` is set true the `tensorrt_analysis_pass` will be run for each calibrator tested to evaluate the most suitable calibrator for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0317 15:03:22.733889 140341721720640 calibrate.py:84] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 15:03:22.754286 140341721720640 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 15:03:22.757311 140341721720640 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 15:03:22.759768 140341721720640 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 15:03:22.762053 140341721720640 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 15:03:22.764362 140341721720640 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 15:03:22.766232 140341721720640 calibrate.py:93] Disabling Quantization and Enabling Calibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 15:03:24.101662 140341721720640 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 15:03:24.103986 140341721720640 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 15:03:24.104778 140341721720640 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 15:03:24.106118 140341721720640 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 15:03:24.106809 140341721720640 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 15:03:24.108095 140341721720640 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 15:03:24.108847 140341721720640 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 15:03:24.110136 140341721720640 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 15:03:24.110907 140341721720640 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 15:03:24.112316 140341721720640 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 15:03:24.113040 140341721720640 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 15:03:24.114289 140341721720640 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0317 15:03:24.117855 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0317 15:03:24.118523 140341721720640 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.9942 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:24.119815 140341721720640 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.9942 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:24.121768 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7247 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:24.122550 140341721720640 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7247 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:24.124664 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4028 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:24.125764 140341721720640 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4028 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:24.127837 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:24.128691 140341721720640 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:24.130878 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7253 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:24.131731 140341721720640 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7253 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:24.133972 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:24.134821 140341721720640 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "I0317 15:03:24.137998 140341721720640 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.0...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 15:03:24.140183 140341721720640 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72109   |\n",
      "|      Average Precision       |   0.73373   |\n",
      "|        Average Recall        |   0.7192    |\n",
      "|       Average F1 Score       |   0.72196   |\n",
      "|         Average Loss         |   0.8013    |\n",
      "|       Average Latency        |  5.1335 ms  |\n",
      "|   Average GPU Power Usage    |  65.156 W   |\n",
      "| Inference Energy Consumption | 0.09291 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0317 15:03:26.356073 140341721720640 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.72109   |\n",
      "|      Average Precision       |   0.73373   |\n",
      "|        Average Recall        |   0.7192    |\n",
      "|       Average F1 Score       |   0.72196   |\n",
      "|         Average Loss         |   0.8013    |\n",
      "|       Average Latency        |  5.1335 ms  |\n",
      "|   Average GPU Power Usage    |  65.156 W   |\n",
      "| Inference Energy Consumption | 0.09291 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 15:03:26.360910 140341721720640 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 15:03:26.364049 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.2559 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:26.365334 140341721720640 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.2559 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:26.367811 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:26.368740 140341721720640 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:26.370589 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.9114 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:26.371526 140341721720640 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.9114 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:26.373734 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:26.375226 140341721720640 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:26.378216 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.3819 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:26.379697 140341721720640 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.3819 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:26.382607 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:26.384107 140341721720640 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "I0317 15:03:26.387978 140341721720640 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.9...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 15:03:26.392198 140341721720640 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73406    |\n",
      "|      Average Precision       |   0.74732    |\n",
      "|        Average Recall        |   0.73232    |\n",
      "|       Average F1 Score       |   0.73568    |\n",
      "|         Average Loss         |   0.76396    |\n",
      "|       Average Latency        |  4.8967 ms   |\n",
      "|   Average GPU Power Usage    |   64.809 W   |\n",
      "| Inference Energy Consumption | 0.088154 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 15:03:29.161168 140341721720640 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73406    |\n",
      "|      Average Precision       |   0.74732    |\n",
      "|        Average Recall        |   0.73232    |\n",
      "|       Average F1 Score       |   0.73568    |\n",
      "|         Average Loss         |   0.76396    |\n",
      "|       Average Latency        |  4.8967 ms   |\n",
      "|   Average GPU Power Usage    |   64.809 W   |\n",
      "| Inference Energy Consumption | 0.088154 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 15:03:29.164948 140341721720640 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 15:03:29.168289 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.0118 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:29.169535 140341721720640 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.0118 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:29.172566 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:29.173959 140341721720640 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:29.176721 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3052 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:29.178182 140341721720640 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.3052 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:29.181079 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:29.182591 140341721720640 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:29.185827 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.8963 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:29.187695 140341721720640 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.8963 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:29.190899 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:29.192572 140341721720640 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "I0317 15:03:29.196258 140341721720640 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.99...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 15:03:29.198628 140341721720640 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73569   |\n",
      "|      Average Precision       |   0.74852   |\n",
      "|        Average Recall        |   0.73388   |\n",
      "|       Average F1 Score       |   0.73712   |\n",
      "|         Average Loss         |   0.75966   |\n",
      "|       Average Latency        |  5.5779 ms  |\n",
      "|   Average GPU Power Usage    |  64.976 W   |\n",
      "| Inference Energy Consumption | 0.10067 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0317 15:03:31.555993 140341721720640 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73569   |\n",
      "|      Average Precision       |   0.74852   |\n",
      "|        Average Recall        |   0.73388   |\n",
      "|       Average F1 Score       |   0.73712   |\n",
      "|         Average Loss         |   0.75966   |\n",
      "|       Average Latency        |  5.5779 ms  |\n",
      "|   Average GPU Power Usage    |  64.976 W   |\n",
      "| Inference Energy Consumption | 0.10067 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 15:03:31.560204 140341721720640 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 15:03:34.141136 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.2680 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:34.145796 140341721720640 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.2680 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:35.931363 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:35.935918 140341721720640 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:37.994828 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.5862 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:37.997375 140341721720640 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.5862 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:39.739986 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:39.743911 140341721720640 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:41.960041 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.9174 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:41.962184 140341721720640 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.9174 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:03:43.717983 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:03:43.720129 140341721720640 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "I0317 15:03:43.723909 140341721720640 calibrate.py:53] Performing post calibration analysis for calibrator mse...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 15:03:43.726157 140341721720640 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73461    |\n",
      "|      Average Precision       |   0.74761    |\n",
      "|        Average Recall        |   0.73285    |\n",
      "|       Average F1 Score       |   0.73617    |\n",
      "|         Average Loss         |   0.76059    |\n",
      "|       Average Latency        |  4.9834 ms   |\n",
      "|   Average GPU Power Usage    |   65.438 W   |\n",
      "| Inference Energy Consumption | 0.090583 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 15:03:45.951988 140341721720640 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73461    |\n",
      "|      Average Precision       |   0.74761    |\n",
      "|        Average Recall        |   0.73285    |\n",
      "|       Average F1 Score       |   0.73617    |\n",
      "|         Average Loss         |   0.76059    |\n",
      "|       Average Latency        |  4.9834 ms   |\n",
      "|   Average GPU Power Usage    |   65.438 W   |\n",
      "| Inference Energy Consumption | 0.090583 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 15:03:45.957024 140341721720640 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 15:04:01.281428 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.5722 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:04:01.290075 140341721720640 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.5722 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:04:08.441881 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:04:08.446881 140341721720640 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:04:20.034908 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.5908 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:04:20.038228 140341721720640 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.5908 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:04:26.721535 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:04:26.724914 140341721720640 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:04:40.997195 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.8975 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:04:40.999308 140341721720640 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.8975 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 15:04:47.576806 140341721720640 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 15:04:47.580615 140341721720640 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "I0317 15:04:47.586142 140341721720640 calibrate.py:53] Performing post calibration analysis for calibrator entropy...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 15:04:47.588738 140341721720640 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73461   |\n",
      "|      Average Precision       |   0.74757   |\n",
      "|        Average Recall        |   0.73289   |\n",
      "|       Average F1 Score       |   0.73613   |\n",
      "|         Average Loss         |   0.76041   |\n",
      "|       Average Latency        |  5.3648 ms  |\n",
      "|   Average GPU Power Usage    |  67.294 W   |\n",
      "| Inference Energy Consumption | 0.10028 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0317 15:04:49.808050 140341721720640 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73461   |\n",
      "|      Average Precision       |   0.74757   |\n",
      "|        Average Recall        |   0.73289   |\n",
      "|       Average F1 Score       |   0.73613   |\n",
      "|         Average Loss         |   0.76041   |\n",
      "|       Average Latency        |  5.3648 ms  |\n",
      "|   Average GPU Power Usage    |  67.294 W   |\n",
      "| Inference Energy Consumption | 0.10028 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 15:04:49.812267 140341721720640 calibrate.py:66] Post calibration analysis complete.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0317 15:04:49.813901 140341721720640 calibrate.py:150] Succeeded in calibrating the model in PyTorch!\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, the 99% `percentile` clips too many values during the amax calibration, comprimising the loss. However 99.99% demonstrates higher validation accuracy alongside `mse` and `entropy` for `jsc-toy`. For such a small model, the methods are not highly distinguished, however for larger models this calibration process will be important for ensuring the quantized model still performs well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3 Quantized Aware Training (QAT)\n",
    "\n",
    "The `tensorrt_fine_tune_transform_pass` is used to fine tune the quantized model. \n",
    "\n",
    "For QAT it is typical to employ 10% of the original training epochs, starting at 1% of the initial training learning rate, and a cosine annealing learning rate schedule that follows the decreasing half of a cosine period, down to 1% of the initial fine tuning learning rate (0.01% of the initial training learning rate). However this default can be overidden by setting the `epochs`, `initial_learning_rate` and `final_learning_rate` in `passes.tensorrt_quantize.fine_tune`.\n",
    "\n",
    "The fine tuned checkpoints are stored in the ckpts/fine_tuning folder:\n",
    "\n",
    "```\n",
    "mase_output\n",
    " tensorrt\n",
    "     quantization\n",
    "         cache\n",
    "         ckpts\n",
    "            fine_tuning\n",
    "         json\n",
    "         onnx\n",
    "         trt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 15:06:14.956480 140341721720640 rank_zero.py:64] GPU available: True (cuda), used: True\n",
      "I0317 15:06:15.025855 140341721720640 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
      "I0317 15:06:15.026797 140341721720640 rank_zero.py:64] IPU available: False, using: 0 IPUs\n",
      "I0317 15:06:15.027601 140341721720640 rank_zero.py:64] HPU available: False, using: 0 HPUs\n",
      "I0317 15:06:19.062803 140341721720640 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0317 15:06:19.104042 140341721720640 model_summary.py:94] \n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 327   \n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | loss_val  | MeanMetric         | 0     \n",
      "4 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "327       Trainable params\n",
      "0         Non-trainable params\n",
      "327       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:00<00:00, 83.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 3084/3084 [02:16<00:00, 22.64it/s, v_num=8, train_acc_step=0.714, val_acc_epoch=0.733, val_loss_epoch=0.751]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 15:10:52.470276 140341721720640 rank_zero.py:64] `Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 3084/3084 [02:16<00:00, 22.64it/s, v_num=8, train_acc_step=0.714, val_acc_epoch=0.733, val_loss_epoch=0.751]\n"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4 TensorRT Quantization\n",
    "\n",
    "After QAT, we are now ready to convert the model to a tensorRT engine so that it can be run with the superior inference speeds. To do so, we use the `tensorrt_engine_interface_pass` which converts the `MaseGraph`'s model from a Pytorch one to an ONNX format as an intermediate stage of the conversion.\n",
    "\n",
    "During the conversion process, the `.onnx` and `.trt` files are stored to their respective folders shown in [Section 1.3](#section-13-quantized-aware-training-qat).\n",
    "\n",
    "This interface pass returns a dictionary containing the `onnx_path` and `trt_engine_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "I0317 16:02:32.975717 140341721720640 quantize.py:129] Converting PyTorch model to ONNX...\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024_03_17/version_0/model.onnx\u001b[0m\n",
      "I0317 16:02:33.312848 140341721720640 quantize.py:152] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/tensorrt/quantization/onnx/2024_03_17/version_0/model.onnx\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "I0317 16:02:33.314947 140341721720640 quantize.py:55] Converting PyTorch model to TensorRT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/17/2024-16:02:43] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024_03_17/version_0/model.trt\u001b[0m\n",
      "I0317 16:03:44.612838 140341721720640 quantize.py:124] TensorRT Conversion Complete. Stored trt model to /root/mase/mase_output/tensorrt/quantization/trt/2024_03_17/version_0/model.trt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024_03_17/version_0/model.json\u001b[0m\n",
      "I0317 16:03:44.884434 140341721720640 quantize.py:168] TensorRT Model Summary Exported to /root/mase/mase_output/tensorrt/quantization/json/2024_03_17/version_0/model.json\n"
     ]
    }
   ],
   "source": [
    "mg, meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.5 Performance Analysis\n",
    "\n",
    "To showcase the improved inference speeds and to evaluate accuracy and other performance metrics, the `tensorrt_analysis_pass` can be used.\n",
    "\n",
    "The tensorRT engine path obtained the previous interface pass is now inputted into the the analysis pass. The same pass can take a MaseGraph as an input, as well as an ONNX graph. For this comparison, we will first run the anaylsis pass on the original unquantized model and then on the INT8 quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 16:06:19.663290 140341721720640 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73168    |\n",
      "|      Average Precision       |   0.74472    |\n",
      "|        Average Recall        |   0.73037    |\n",
      "|       Average F1 Score       |   0.73369    |\n",
      "|         Average Loss         |   0.76315    |\n",
      "|       Average Latency        |  1.3956 ms   |\n",
      "|   Average GPU Power Usage    |   63.409 W   |\n",
      "| Inference Energy Consumption | 0.024581 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 16:06:26.247036 140341721720640 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73168    |\n",
      "|      Average Precision       |   0.74472    |\n",
      "|        Average Recall        |   0.73037    |\n",
      "|       Average F1 Score       |   0.73369    |\n",
      "|         Average Loss         |   0.76315    |\n",
      "|       Average Latency        |  1.3956 ms   |\n",
      "|   Average GPU Power Usage    |   63.409 W   |\n",
      "| Inference Energy Consumption | 0.024581 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\u001b[0m\n",
      "I0317 16:06:26.279335 140341721720640 analysis.py:117] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 16:06:26.282451 140341721720640 analysis.py:214] Starting transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/17/2024-16:06:26] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73438    |\n",
      "|      Average Precision       |    0.74741    |\n",
      "|        Average Recall        |    0.73465    |\n",
      "|       Average F1 Score       |    0.73784    |\n",
      "|         Average Loss         |    0.75311    |\n",
      "|       Average Latency        |  0.23044 ms   |\n",
      "|   Average GPU Power Usage    |   64.415 W    |\n",
      "| Inference Energy Consumption | 0.0041234 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "I0317 16:06:32.705540 140341721720640 analysis.py:317] \n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73438    |\n",
      "|      Average Precision       |    0.74741    |\n",
      "|        Average Recall        |    0.73465    |\n",
      "|       Average F1 Score       |    0.73784    |\n",
      "|         Average Loss         |    0.75311    |\n",
      "|       Average Latency        |  0.23044 ms   |\n",
      "|   Average GPU Power Usage    |   64.415 W    |\n",
      "| Inference Energy Consumption | 0.0041234 mWh |\n",
      "+------------------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "_, _ = tensorrt_analysis_pass(mg_original, pass_args=tensorrt_analysis_config)\n",
    "_, _ = tensorrt_analysis_pass(meta['trt_engine_path'], pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the latency has decreased around 4x with the `jsc-toy` model without compromising accuracy due to the well calibrated amax and QAT.\n",
    "\n",
    "## Section 2. FP16 Quantization\n",
    "\n",
    "We will now load in a new toml configuration that uses FP16 instead of INT8, whilst keeping the other settings the exact same for a fair comparison. This time however, we will use chop from the terminal which runs all the passes showcased in [Section 1](#section-1---int8-quantization).\n",
    "\n",
    "Since float quantization does not require calibration, nor is it supported by `pytorch-quantization`, the model will not undergo fake quantization, unfortunately, for the time being this means QAT is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4218.70s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "[2024-03-17 16:13:27,067] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0317 16:13:29.661992 140483290875712 seed.py:54] Seed set to 0\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |        Default         | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |     classification     |              |                          |      classification      |\n",
      "| load_name               |          \u001b[38;5;8mNone\u001b[0m          |              | /root/mase/docs/tutorial | /root/mase/docs/tutorial |\n",
      "|                         |                        |              | s/tensorrt/checkpoints/j | s/tensorrt/checkpoints/j |\n",
      "|                         |                        |              | sc-toy_classification_js | sc-toy_classification_js |\n",
      "|                         |                        |              | c/software/training_ckpt | c/software/training_ckpt |\n",
      "|                         |                        |              |       s/best.ckpt        |       s/best.ckpt        |\n",
      "| load_type               |           \u001b[38;5;8mmz\u001b[0m           |              |            pl            |            pl            |\n",
      "| batch_size              |          \u001b[38;5;8m128\u001b[0m           |     256      |                          |           256            |\n",
      "| to_debug                |         False          |              |                          |          False           |\n",
      "| log_level               |          info          |              |                          |           info           |\n",
      "| report_to               |      tensorboard       |              |                          |       tensorboard        |\n",
      "| seed                    |           0            |              |                          |            0             |\n",
      "| quant_config            |          None          |              |                          |           None           |\n",
      "| training_optimizer      |          adam          |              |                          |           adam           |\n",
      "| trainer_precision       |        16-mixed        |              |                          |         16-mixed         |\n",
      "| learning_rate           |         \u001b[38;5;8m1e-05\u001b[0m          |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |           0            |              |                          |            0             |\n",
      "| max_epochs              |           \u001b[38;5;8m20\u001b[0m           |      20      |                          |            20            |\n",
      "| max_steps               |           -1           |              |                          |            -1            |\n",
      "| accumulate_grad_batches |           1            |              |                          |            1             |\n",
      "| log_every_n_steps       |           50           |              |                          |            50            |\n",
      "| num_workers             |           48           |              |                          |            48            |\n",
      "| num_devices             |           1            |              |                          |            1             |\n",
      "| num_nodes               |           1            |              |                          |            1             |\n",
      "| accelerator             |          \u001b[38;5;8mauto\u001b[0m          |     gpu      |                          |           gpu            |\n",
      "| strategy                |          auto          |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |         False          |              |                          |          False           |\n",
      "| github_ci               |         False          |              |                          |          False           |\n",
      "| disable_dataset_cache   |         False          |              |                          |          False           |\n",
      "| target                  |  xcu250-figd2104-2L-e  |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |          100           |              |                          |           100            |\n",
      "| is_pretrained           |         False          |              |                          |          False           |\n",
      "| max_token_len           |          512           |              |                          |           512            |\n",
      "| project_dir             | /root/mase/mase_output |              |                          |  /root/mase/mase_output  |\n",
      "| project                 |          None          |              |                          |           None           |\n",
      "| model                   |          \u001b[38;5;8mNone\u001b[0m          |   jsc-toy    |                          |         jsc-toy          |\n",
      "| dataset                 |          \u001b[38;5;8mNone\u001b[0m          |     jsc      |                          |           jsc            |\n",
      "| t_max                   |           20           |              |                          |            20            |\n",
      "| eta_min                 |         1e-06          |              |                          |          1e-06           |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'jsc-toy'...\u001b[0m\n",
      "I0317 16:13:29.951667 140483290875712 cli.py:841] Initialising model 'jsc-toy'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'jsc'...\u001b[0m\n",
      "I0317 16:13:29.975686 140483290875712 cli.py:869] Initialising dataset 'jsc'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/mase/mase_output/jsc-toy_classification_jsc_2024-03-17\u001b[0m\n",
      "I0317 16:13:29.976247 140483290875712 cli.py:905] Project will be created at /root/mase/mase_output/jsc-toy_classification_jsc_2024-03-17\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'jsc-toy'...\u001b[0m\n",
      "I0317 16:13:30.028887 140483290875712 cli.py:365] Transforming model 'jsc-toy'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/docs/tutorials/tensorrt/checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\u001b[0m\n",
      "I0317 16:13:34.151346 140483290875712 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/docs/tutorials/tensorrt/checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0317 16:13:37.658099 140483290875712 utils.py:166] Applying fake quantization to PyTorch model...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0317 16:13:37.924054 140483290875712 utils.py:182] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0317 16:13:37.924349 140483290875712 calibrate.py:84] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 16:13:37.925075 140483290875712 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 16:13:37.925246 140483290875712 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 16:13:37.925415 140483290875712 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 16:13:37.925549 140483290875712 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 16:13:37.925684 140483290875712 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0317 16:13:37.925811 140483290875712 calibrate.py:93] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 16:13:41.223090 140483290875712 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 16:13:41.224065 140483290875712 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 16:13:41.224244 140483290875712 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 16:13:41.224520 140483290875712 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 16:13:41.224669 140483290875712 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 16:13:41.224906 140483290875712 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 16:13:41.225026 140483290875712 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 16:13:41.225247 140483290875712 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 16:13:41.225378 140483290875712 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 16:13:41.225600 140483290875712 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0317 16:13:41.225713 140483290875712 calibrate.py:114] Enabling Quantization and Disabling Calibration\n",
      "W0317 16:13:41.225928 140483290875712 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0317 16:13:41.236539 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0317 16:13:41.236638 140483290875712 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0452 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:41.236851 140483290875712 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0452 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:41.237217 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7247 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:41.237350 140483290875712 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7247 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:41.237787 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4818 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:41.237918 140483290875712 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.4818 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:41.238274 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:41.238404 140483290875712 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:41.238767 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7389 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:41.238897 140483290875712 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=1.7389 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:41.239248 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:41.239386 140483290875712 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.0...\u001b[0m\n",
      "I0317 16:13:41.240139 140483290875712 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.0...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 16:13:41.240427 140483290875712 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72279    |\n",
      "|      Average Precision       |   0.73548    |\n",
      "|        Average Recall        |   0.72097    |\n",
      "|       Average F1 Score       |   0.72381    |\n",
      "|         Average Loss         |   0.79749    |\n",
      "|       Average Latency        |  4.0776 ms   |\n",
      "|   Average GPU Power Usage    |   64.107 W   |\n",
      "| Inference Energy Consumption | 0.072613 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 16:13:45.707171 140483290875712 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.72279    |\n",
      "|      Average Precision       |   0.73548    |\n",
      "|        Average Recall        |   0.72097    |\n",
      "|       Average F1 Score       |   0.72381    |\n",
      "|         Average Loss         |   0.79749    |\n",
      "|       Average Latency        |  4.0776 ms   |\n",
      "|   Average GPU Power Usage    |   64.107 W   |\n",
      "| Inference Energy Consumption | 0.072613 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 16:13:45.708947 140483290875712 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 16:13:45.709855 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.0374 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:45.710148 140483290875712 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.0374 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:45.710637 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:45.710887 140483290875712 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:45.711425 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.7418 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:45.711645 140483290875712 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.7418 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:45.712118 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:45.712337 140483290875712 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:45.712812 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.3874 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:45.713028 140483290875712 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.3874 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:45.713477 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:45.713695 140483290875712 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.9...\u001b[0m\n",
      "I0317 16:13:45.714325 140483290875712 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.9...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 16:13:45.714582 140483290875712 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73391    |\n",
      "|      Average Precision       |    0.7469    |\n",
      "|        Average Recall        |   0.73215    |\n",
      "|       Average F1 Score       |   0.73547    |\n",
      "|         Average Loss         |   0.76579    |\n",
      "|       Average Latency        |  3.9923 ms   |\n",
      "|   Average GPU Power Usage    |   64.784 W   |\n",
      "| Inference Energy Consumption | 0.071843 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 16:13:50.100040 140483290875712 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73391    |\n",
      "|      Average Precision       |    0.7469    |\n",
      "|        Average Recall        |   0.73215    |\n",
      "|       Average F1 Score       |   0.73547    |\n",
      "|         Average Loss         |   0.76579    |\n",
      "|       Average Latency        |  3.9923 ms   |\n",
      "|   Average GPU Power Usage    |   64.784 W   |\n",
      "| Inference Energy Consumption | 0.071843 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 16:13:50.101582 140483290875712 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 16:13:50.102361 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.9534 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:50.102588 140483290875712 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.9534 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:50.102946 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:50.103120 140483290875712 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:50.103515 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.5005 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:50.103681 140483290875712 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.5005 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:50.104025 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:50.104194 140483290875712 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:50.104560 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.2350 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:50.104727 140483290875712 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.2350 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:50.105064 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:50.105229 140483290875712 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator percentile_99.99...\u001b[0m\n",
      "I0317 16:13:50.105721 140483290875712 calibrate.py:53] Performing post calibration analysis for calibrator percentile_99.99...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 16:13:50.105903 140483290875712 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73505    |\n",
      "|      Average Precision       |   0.74815    |\n",
      "|        Average Recall        |   0.73331    |\n",
      "|       Average F1 Score       |   0.73662    |\n",
      "|         Average Loss         |   0.75982    |\n",
      "|       Average Latency        |   3.739 ms   |\n",
      "|   Average GPU Power Usage    |   64.376 W   |\n",
      "| Inference Energy Consumption | 0.066863 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 16:13:54.310167 140483290875712 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73505    |\n",
      "|      Average Precision       |   0.74815    |\n",
      "|        Average Recall        |   0.73331    |\n",
      "|       Average F1 Score       |   0.73662    |\n",
      "|         Average Loss         |   0.75982    |\n",
      "|       Average Latency        |   3.739 ms   |\n",
      "|   Average GPU Power Usage    |   64.376 W   |\n",
      "| Inference Energy Consumption | 0.066863 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 16:13:54.311804 140483290875712 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 16:13:56.819308 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.6785 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:56.819873 140483290875712 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=6.6785 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:13:58.164679 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:13:58.165177 140483290875712 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7464 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:00.617918 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.7790 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:00.618522 140483290875712 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.7790 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:01.978494 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:01.978965 140483290875712 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5179 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:03.349750 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.6285 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:03.350297 140483290875712 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.6285 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:04.744019 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:04.744502 140483290875712 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5531 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator mse...\u001b[0m\n",
      "I0317 16:14:04.746009 140483290875712 calibrate.py:53] Performing post calibration analysis for calibrator mse...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 16:14:04.746453 140483290875712 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73576    |\n",
      "|      Average Precision       |    0.7489    |\n",
      "|        Average Recall        |    0.734     |\n",
      "|       Average F1 Score       |   0.73741    |\n",
      "|         Average Loss         |   0.76079    |\n",
      "|       Average Latency        |   4.364 ms   |\n",
      "|   Average GPU Power Usage    |   66.186 W   |\n",
      "| Inference Energy Consumption | 0.080232 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0317 16:14:09.522426 140483290875712 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "| Average Validation Accuracy  |   0.73576    |\n",
      "|      Average Precision       |    0.7489    |\n",
      "|        Average Recall        |    0.734     |\n",
      "|       Average F1 Score       |   0.73741    |\n",
      "|         Average Loss         |   0.76079    |\n",
      "|       Average Latency        |   4.364 ms   |\n",
      "|   Average GPU Power Usage    |   66.186 W   |\n",
      "| Inference Energy Consumption | 0.080232 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 16:14:09.524381 140483290875712 calibrate.py:66] Post calibration analysis complete.\n",
      "W0317 16:14:25.334856 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.7303 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:25.340094 140483290875712 calibrate.py:79] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.7303 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:29.467193 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:29.467676 140483290875712 calibrate.py:79] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7465 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:44.313213 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.5929 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:44.314083 140483290875712 calibrate.py:79] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.5929 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:48.162850 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:48.163284 140483290875712 calibrate.py:79] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5203 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:54.098813 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0568 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:54.099245 140483290875712 calibrate.py:79] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0568 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0317 16:14:57.848513 140483290875712 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0317 16:14:57.848929 140483290875712 calibrate.py:79] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5548 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for calibrator entropy...\u001b[0m\n",
      "I0317 16:14:57.850044 140483290875712 calibrate.py:53] Performing post calibration analysis for calibrator entropy...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting transformation analysis\u001b[0m\n",
      "I0317 16:14:57.850436 140483290875712 analysis.py:214] Starting transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73586   |\n",
      "|      Average Precision       |   0.74928   |\n",
      "|        Average Recall        |   0.73409   |\n",
      "|       Average F1 Score       |   0.73749   |\n",
      "|         Average Loss         |   0.75934   |\n",
      "|       Average Latency        |  4.112 ms   |\n",
      "|   Average GPU Power Usage    |   67.56 W   |\n",
      "| Inference Energy Consumption | 0.07717 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0317 16:15:02.408135 140483290875712 analysis.py:317] \n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "| Average Validation Accuracy  |   0.73586   |\n",
      "|      Average Precision       |   0.74928   |\n",
      "|        Average Recall        |   0.73409   |\n",
      "|       Average F1 Score       |   0.73749   |\n",
      "|         Average Loss         |   0.75934   |\n",
      "|       Average Latency        |  4.112 ms   |\n",
      "|   Average GPU Power Usage    |   67.56 W   |\n",
      "| Inference Energy Consumption | 0.07717 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0317 16:15:02.410362 140483290875712 calibrate.py:66] Post calibration analysis complete.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0317 16:15:02.410579 140483290875712 calibrate.py:150] Succeeded in calibrating the model in PyTorch!\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mQuantized graph histogram:\u001b[0m\n",
      "I0317 16:15:02.460503 140483290875712 summary.py:84] Quantized graph histogram:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| ReLU            | relu         |       4 |         0 |           4 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\u001b[0m\n",
      "I0317 16:15:02.461714 140483290875712 summary.py:85] \n",
      "| Original type   | OP           |   Total |   Changed |   Unchanged |\n",
      "|-----------------+--------------+---------+-----------+-------------|\n",
      "| BatchNorm1d     | batch_norm1d |       4 |         0 |           4 |\n",
      "| Linear          | linear       |       3 |         3 |           0 |\n",
      "| ReLU            | relu         |       4 |         0 |           4 |\n",
      "| output          | output       |       1 |         0 |           1 |\n",
      "| x               | placeholder  |       1 |         0 |           1 |\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "I0317 16:15:02.654273 140483290875712 rank_zero.py:64] GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "I0317 16:15:02.688218 140483290875712 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "I0317 16:15:02.688484 140483290875712 rank_zero.py:64] IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "I0317 16:15:02.688598 140483290875712 rank_zero.py:64] HPU available: False, using: 0 HPUs\n",
      "I0317 16:15:06.743206 140483290875712 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0317 16:15:06.751039 140483290875712 model_summary.py:94] \n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 327   \n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | loss_val  | MeanMetric         | 0     \n",
      "4 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "327       Trainable params\n",
      "0         Non-trainable params\n",
      "327       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "Epoch 0: 100%|| 3084/3084 [00:38<00:00, 79.49it/s, v_num=9, train_acc_step=0.71"
     ]
    }
   ],
   "source": [
    "!ch transform --config ../../../machop/configs/tensorrt/jsc_toy_FP16_quantization_by_type.toml --load checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorrt_fake_quantize_transform_pass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mg, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtensorrt_fake_quantize_transform_pass\u001b[49m(mg, pass_args\u001b[38;5;241m=\u001b[39mtensorrt_quantize_config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorrt_fake_quantize_transform_pass' is not defined"
     ]
    }
   ],
   "source": [
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy INT8 only quantization \n",
    "meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_0/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy FP16 only quantization \n",
    "meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_1/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(meta['graph_path'], pass_args=tensorrt_analysis_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
