{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to generate an FPGA accelerator for a quantized Bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we'll see how to load a Bert model from the Mase model library, optimize it by quantizing the weights, then emit the SystemVerilog code for a custom dataflow accelerator, ready to be deployed on an Intel or Xilinx FPGA. This involves using generating a computation graph for the model, then invoking several Mase compiler passes. First, we go through this in detail, discussing the steps required. Then, we show how to use the `chop.pipelines` pass managers to encapsulate all this functionality within a single function call. Finally, we'll run the generated [Cocotb](https://www.cocotb.org/) testbench to evaluate the throughput and latency of the emitted accelerator.\n",
    "\n",
    "This tutorial assumes you have a working Mase environment. Follow the instructions [here](https://deepwok.github.io/mase/modules/documentation/getting_started.html) to get started using Conda or Docker. You will also need a working Questa installation to run the testbench of the accelerator. If you don't have Questa available, you can also use Verilator, however the runtime may be very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a logger\n",
    "from chop.tools import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and quantize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the Bert model from Mase's [patched model library](https://github.com/DeepWok/mase/tree/main/src/chop/models). We'll define a small configuration with 3 layers and a hidden size of 96. We'll also define a quantization configuration which specifies the fixed-point precision we want to run the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.models.patched.bert import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "config.num_hidden_layers = 3\n",
    "config.hidden_size = 96\n",
    "config.intermediate_size = 384\n",
    "\n",
    "q_config = {\n",
    "    \"data_in_width\": 8,\n",
    "    \"data_in_frac_width\": 3,\n",
    "    \"weight_width\": 8,\n",
    "    \"weight_frac_width\": 3,\n",
    "    \"bias_width\": 8,\n",
    "    \"bias_frac_width\": 3,\n",
    "    \"data_out_width\": 8,\n",
    "    \"data_out_frac_width\": 3,\n",
    "}\n",
    "\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is defined, we are ready to quantize it by writing a module-level pass. This simply iterates through the modules in the Pytorch model and replaces the relevant ones with their quantized equivalents. In the Bert model, the relevant modules that need to be quantized are:\n",
    "1. The self attention layer\n",
    "2. Linear layers\n",
    "3. Layer normalization layer\n",
    "4. GELU activation layer\n",
    "\n",
    "You can see that Mase has a library of quantized neural network layers under the `chop.nn.quantized` API. See [here](https://github.com/DeepWok/mase/tree/main/src/chop/nn) for a full reference of the available modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.activations import GELUActivation\n",
    "from chop.models.patched.bert.modeling_bert import BertSelfAttention\n",
    "from chop.nn.quantized import (\n",
    "    BertSelfAttentionInteger,\n",
    "    LinearInteger,\n",
    "    LayerNormInteger,\n",
    "    GELUInteger,\n",
    ")\n",
    "from chop.passes.graph.utils import deepsetattr\n",
    "\n",
    "\n",
    "def bert_module_level_quantize(model, model_config, q_config):\n",
    "    for module in model.named_modules():\n",
    "        if isinstance(module[1], BertSelfAttention):\n",
    "            new_module = BertSelfAttentionInteger(\n",
    "                model_config, q_config, output_tensor_only=True\n",
    "            )\n",
    "        elif isinstance(module[1], nn.Linear):\n",
    "            new_module = LinearInteger(\n",
    "                in_features=module[1].in_features,\n",
    "                out_features=module[1].out_features,\n",
    "                bias=module[1].bias is not None,\n",
    "                config=q_config,\n",
    "            )\n",
    "        elif isinstance(module[1], nn.LayerNorm):\n",
    "            new_module = LayerNormInteger(\n",
    "                normalized_shape=module[1].normalized_shape,\n",
    "                eps=module[1].eps,\n",
    "                config=q_config,\n",
    "            )\n",
    "        elif isinstance(module[1], GELUActivation):\n",
    "            new_module = GELUInteger(config=q_config)\n",
    "        else:\n",
    "            continue\n",
    "        logger.info(f\"Replacing module: {module[0]}\")\n",
    "        deepsetattr(model, module[0], new_module)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = bert_module_level_quantize(model, config, q_config)\n",
    "logger.info(f\"Quantized BERT model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emit SystemVerilog code for the accelerator: step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is quantized, we are ready to run Mase's FX compiler flow. This involves extracting a computation graph from the Pytorch model leveraging Pytorch FX (see details [here](https://pytorch.org/docs/stable/fx.html)), then running a few analysis and transformation passes on this graph until it's ready for emitting the Verilog code for the dataflow accelerator. First, we'll do this step-by-step, then we'll see how to automate all these operations with a single function call, using the `chop.pipelines` API. \n",
    "\n",
    "In either case, we start by generating the computation graph through a process called symbolic tracing. As discussed in the `torch.fx` documentation, this involves running a forward pass of the model using dedicated `fx.Proxy` objects as the arguments, instead of the regular `torch.Tensor`s. These proxies record every operation executed on them, which is then used to generate the computation graph. Each node in the generated graph can be a single sublayer, such as `nn.Linear`, or fine-grained function call such as `torch.matmul`. For the emit verilog flow, we require the graph to be at layer granularity, meaning the internal function calls of each layer are hidden in the graph. To achieve this, we pass a `custom_ops` dictionary to the MaseGraph constructor, which instructs the FX tracer to skip this layer during FX tracing. We also provide the desired implementation for the self attention layer, which is available in the [Mase Components](https://github.com/DeepWok/mase/tree/main/src/mase_components) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.ir import MaseGraph\n",
    "from mase_components import get_module_dependencies\n",
    "\n",
    "BERT_CUSTOM_OPS = {\n",
    "    \"modules\": {\n",
    "        BertSelfAttentionInteger: {\n",
    "            \"args\": {\n",
    "                \"hidden_states\": \"data_in\",\n",
    "                \"attention_mask\": None,\n",
    "                \"head_mask\": None,\n",
    "                \"encoder_hidden_states\": None,\n",
    "                \"encoder_attention_mask\": None,\n",
    "                \"past_key_value\": None,\n",
    "                \"output_attentions\": \"config\",\n",
    "            },\n",
    "            \"toolchain\": \"INTERNAL_RTL\",\n",
    "            \"module\": \"fixed_self_attention_single_precision_wrapper\",\n",
    "            \"dependence_files\": get_module_dependencies(\n",
    "                \"attention/fixed_self_attention_single_precision_wrapper\"\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    \"functions\": {},\n",
    "}\n",
    "\n",
    "mg = MaseGraph(model, custom_ops=BERT_CUSTOM_OPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the bert model graph is generated, we can start with the analysis passes, which annotate the graph with relevant information, without changing the topology of the nodes and edges. The `add_common_metadata_analysis_pass` performs shape propagation, i.e. running a forward pass on the model to annotate each node with tensor metadata for each of the operator's input and output tensors. `add_hardware_metadata_analysis_pass` builds on top of this, annotating each node with the verilog parameters which will later be used by the pass that emits the SystemVerilog code. One crucial aspect is the `max_parallelism` parameter, which corresponds to the number of arithmetic cores in each hardware submodule, affecting the resource consumption and latency performance of the resulting hardware. The `patch_metadata_transform_pass` pass annotates the fixed-point precision according to the quantiation configuration for a subset of nodes which are relevant for the control flow of the generated hardware. For more information about each pass, see the [pass API documentation](https://deepwok.github.io/mase/modules/api/passes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import chop.passes as passes\n",
    "\n",
    "# Redefine some configuration parameters\n",
    "CONFIG_BATCH_SIZE = 1\n",
    "CONFIG_SEQUENCE_LENGTH = 4\n",
    "MAX_PARALLELISM = 4\n",
    "WAIT_COUNT = 15\n",
    "WAIT_UNIT = \"ms\"\n",
    "\n",
    "\n",
    "mg, _ = passes.init_metadata_analysis_pass(mg)\n",
    "\n",
    "# * Add metadata analysis passes\n",
    "mg, _ = passes.add_common_metadata_analysis_pass(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"dummy_in\": {\n",
    "            \"input_ids\": torch.randn(\n",
    "                (CONFIG_BATCH_SIZE, CONFIG_SEQUENCE_LENGTH, config.hidden_size)\n",
    "            )\n",
    "        },\n",
    "        \"add_value\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "mg, _ = passes.patch_metadata_transform_pass(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"precision\": \"fixed\",\n",
    "        \"q_config\": q_config,\n",
    "    },\n",
    ")\n",
    "\n",
    "mg, _ = passes.add_hardware_metadata_analysis_pass(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"max_parallelism\": [MAX_PARALLELISM] * 4,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we are ready to execute the graph transformation passes, which use the annotated metadata to change the topology of the graph such that it is ready for verilog emit. The `emit_verilog_top_transform_pass` generates the SystemVerilog top-level file, while `emit_internal_rtl_transform_pass` copies the relevant submodules from the [Mase Components](https://github.com/DeepWok/mase/tree/main/src/mase_components) SystemVerilog library to the user's workarea. The `emit_bram_transform_pass` pass emits the BRAM modules which store the weights and biases on the FPGA for each layer in the model. A Cocotb testbench is generated in the `emit_cocotb_transform_pass`, which can be used for testing the generated hardware using real Pytorch datasets. Finally, `emit_vivado_project_transform_pass` prepares a Vivado project containing the emitted Verilog code, making it ready for Synthesis and Implementation on the FPGA board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timeout time for the generated testbench\n",
    "WAIT_COUNT = 15\n",
    "WAIT_UNIT = \"ms\"\n",
    "\n",
    "mg, _ = passes.emit_verilog_top_transform_pass(mg)\n",
    "mg, _ = passes.emit_bram_transform_pass(mg)\n",
    "mg, _ = passes.emit_internal_rtl_transform_pass(mg)\n",
    "mg, _ = passes.emit_cocotb_transform_pass(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"wait_time\": WAIT_COUNT,\n",
    "        \"wait_unit\": WAIT_UNIT,\n",
    "    },\n",
    ")\n",
    "mg, _ = passes.emit_vivado_project_transform_pass(mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoorah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emit SystemVerilog code for the accelerator: with automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've seen everything Mase does under the hood, but we don't want to write that much code each time we generate Verilog for a new model. Luckily, the workflow for every model is very similar, and can be abstracted into a pass manager, which runs a default set of passes. This is achieved through the AutoPipeline API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop import AutoPipelineForEmitVerilog\n",
    "\n",
    "# Redefine some configuration parameters\n",
    "CONFIG_BATCH_SIZE = 1\n",
    "CONFIG_SEQUENCE_LENGTH = 4\n",
    "WAIT_COUNT = 15\n",
    "WAIT_UNIT = \"ms\"\n",
    "MAX_PARALLELISM = 4\n",
    "\n",
    "mg = MaseGraph(model, custom_ops=BERT_CUSTOM_OPS)\n",
    "\n",
    "pipeline = AutoPipelineForEmitVerilog()\n",
    "mg = pipeline(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"add_common_metadata_analysis_pass\": {\n",
    "            \"dummy_in\": {\n",
    "                \"input_ids\": torch.randn(\n",
    "                    (\n",
    "                        CONFIG_BATCH_SIZE,\n",
    "                        CONFIG_SEQUENCE_LENGTH,\n",
    "                        config.hidden_size,\n",
    "                    )\n",
    "                )\n",
    "            },\n",
    "            \"add_value\": False,\n",
    "        },\n",
    "        \"patch_metadata_transform_pass\": {\n",
    "            \"q_config\": q_config,\n",
    "        },\n",
    "        \"add_hardware_metadata_analysis_pass\": {\n",
    "            \"max_parallelism\": [MAX_PARALLELISM] * 4,\n",
    "        },\n",
    "        \"report_node_meta_param_analysis_pass\": {\n",
    "            \"which\": [\"common\", \"hardware\"],\n",
    "            \"save_path\": \"llama_graph_meta_params.txt\",\n",
    "        },\n",
    "        \"emit_cocotb_transform_pass\": {\n",
    "            \"wait_time\": WAIT_COUNT,\n",
    "            \"wait_unit\": WAIT_UNIT,\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the generated accelerator\n",
    "\n",
    "Now everything is ready, and the generated Verilog files can be found under `~/.mase/top/hardware/rtl`. You can inspect the `top.sv` file to see how data is propagated from the inputs of the module through every layer in the original model. You can also find the emitted Cocotb test under `~/.mase/top/hardware/test.py`. Note that the Cocotb testbench class is not emitted as a text file, but rather pickled and stored as a .dill file, which is a compressed way of sharing the testbench. This is then unpickled and instantiated in the `test.py` file which is executed by the Cocotb runner. Now, simply run the `simulate` action to obtain the latency for a single batch inference pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chop.actions as actions\n",
    "\n",
    "os.environ[\"COCOTB_RESOLVE_X\"] = \"ZEROS\"\n",
    "actions.simulate(\n",
    "    skip_build=False, skip_test=False, gui=False, waves=False, simulator=\"questa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated the process of generating an FPGA accelerator for a quantized BERT model using the Mase framework. We began by loading a BERT model and defining its configuration and quantization parameters, then proceeded to quantize the model at the module level. Next, we walked through the detailed steps of emitting SystemVerilog code for the accelerator, which included generating a computation graph using Torch FX, performing various metadata analysis passes, and transforming the graph to be ready for Verilog emission. We showed how to automate these steps using the chop.pipelines API, greatly simplifying the workflow. Finally, we ran the generated Cocotb testbench to evaluate the performance of the accelerator, obtaining throughput and latency metrics.\n",
    "\n",
    "By following this tutorial, you should now have a solid understanding of how to optimize transformer models for FPGA deployment using Mase, from quantization to hardware code generation and performance evaluation. If you are interested in experimenting further, we propose the following suggested exercises.\n",
    "\n",
    "1. Re-run the flow by changing the q_config dictionary to try different fixed-point precisions. In each case, open the generated Vivado project and launch the synthesis flow to compare the resource consumption of the generated hardware. Create a plot of the LUT, FF and DSP utilization statistics for a range of fixed-point precisions.\n",
    "\n",
    "2. Repeat exercise 1, but this time experiment with the maximum parallelism parameter. Again, compare the resource consumption for a range of parallelism parameters. This time, also run the Cocotb testbench in each iteration to see how the parallelism affects the inference latency. Based on this analysis, can you suggest an optimal design point that trades off resource consumption with inference latency?\n",
    "\n",
    "If you are interested in contributing to the Mase project, we suggest the following extension task.\n",
    "\n",
    "3. Try to support this flow for a new model, such as Llama, Mistral or GPT. Follow the steps in the [documentation]() to import a new model into Mase from the HuggingFace hub, and try running the `AutoPipelineForVerilogEmit`. If that doesn't work directly, see the hints in the [debugging guide]() to support the new model, now you know the steps required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
