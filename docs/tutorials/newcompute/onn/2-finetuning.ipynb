{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba64c6d",
   "metadata": {},
   "source": [
    "# Fine-tuning HuggingFace Llama with Optical Transformer Transform\n",
    "\n",
    "This tutorial demonstrates how to:\n",
    "1. Load a pretrained HuggingFace Llama model\n",
    "2. Transform it using the optical transformer pass from MASE\n",
    "3. Run continual fine-tuning on the transformed model\n",
    "\n",
    "## Overview\n",
    "\n",
    "The optical transformer transform replaces standard PyTorch modules with their optical equivalents that simulate optical computing behavior. This enables:\n",
    "- Quantized matrix multiplication that models optical hardware\n",
    "- Noise-aware training for robust optical neural network deployment\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "pip install mase-triton transformers datasets accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943f547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zz7522/miniconda3/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from chop.passes.module.transforms.onn.transform import (\n",
    "    OtLinear,\n",
    "    OtLlamaAttention,\n",
    "    OtTransformConfig,\n",
    "    optical_transformer_module_transform_pass,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4c060",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained HuggingFace Llama Model\n",
    "\n",
    "We'll use a small Llama model for demonstration. You can replace this with any Llama-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b797b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"AICrossSim/clm-60m\"  # Small Llama model for demo\n",
    "BLOCK_SIZE = 128  # Sequence length (use smaller value for demo)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6ed596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: LlamaForCausalLM\n",
      "Number of parameters: 82,101,120\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    attn_implementation=\"eager\",  # Use eager attention for compatibility\n",
    ")\n",
    "\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e16d4",
   "metadata": {},
   "source": [
    "## 2. Configure and Apply Optical Transform\n",
    "\n",
    "We configure the optical transform with quantization parameters and apply it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968be7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONN Configuration:\n",
      "  q_levels: 256\n",
      "  q_lut_min: 0.02004\n",
      "  q_smooth_factor: 0.9\n",
      "  q_init_seed: 0\n",
      "  q_bypass: False\n"
     ]
    }
   ],
   "source": [
    "# Create ONN configuration\n",
    "onn_config = OtTransformConfig.create_default()\n",
    "\n",
    "# Customize configuration (optional)\n",
    "onn_config[\"q_levels\"] = 256  # Number of quantization levels\n",
    "onn_config[\"q_lut_min\"] = 0.020040  # Minimum LUT value\n",
    "onn_config[\"q_smooth_factor\"] = 0.9  # Statistics smoothing factor\n",
    "onn_config[\"q_bypass\"] = False  # Set to True to bypass optical quantization\n",
    "\n",
    "print(\"ONN Configuration:\")\n",
    "for k, v in onn_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c54884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform pass arguments configured\n"
     ]
    }
   ],
   "source": [
    "# Configure the transform pass\n",
    "# Use regex patterns to match layer names for transformation\n",
    "pass_args = {\n",
    "    \"by\": \"regex_name\",\n",
    "    # Transform all attention layers\n",
    "    r\"model\\.layers\\.\\d+\\.self_attn\": onn_config,\n",
    "    # Transform attention projections (Q, K, V, O)\n",
    "    r\"model\\.layers\\.\\d+\\.self_attn\\.(q|k|v|o)_proj\": onn_config,\n",
    "    # Transform MLP layers\n",
    "    r\"model\\.layers\\.\\d+\\.mlp\\.(gate|up|down)_proj\": onn_config,\n",
    "}\n",
    "\n",
    "print(\"Transform pass arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183195e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=1408, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=1408, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=256, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model transformed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Apply the optical transformer transform\n",
    "print(\"Transforming model...\")\n",
    "model = optical_transformer_module_transform_pass(model, pass_args)\n",
    "print(\"Model transformed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57dd533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed layers:\n",
      "  OtLinear: 154\n",
      "  OtLlamaAttention: 22\n"
     ]
    }
   ],
   "source": [
    "# Verify the transformation\n",
    "def count_transformed_layers(model):\n",
    "    ot_linear_count = 0\n",
    "    ot_attn_count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OtLinear):\n",
    "            ot_linear_count += 1\n",
    "        elif isinstance(module, OtLlamaAttention):\n",
    "            ot_attn_count += 1\n",
    "    return ot_linear_count, ot_attn_count\n",
    "\n",
    "ot_linear, ot_attn = count_transformed_layers(model)\n",
    "print(f\"Transformed layers:\")\n",
    "print(f\"  OtLinear: {ot_linear}\")\n",
    "print(f\"  OtLlamaAttention: {ot_attn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066772ea",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset for Fine-tuning\n",
    "\n",
    "We'll use a small subset of a text dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52002e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 36718\n",
      "Validation samples: 3760\n"
     ]
    }
   ],
   "source": [
    "# Load a small dataset for demonstration\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "print(f\"Train samples: {len(raw_datasets['train'])}\")\n",
    "print(f\"Validation samples: {len(raw_datasets['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12cb5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05df197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train chunks: 19848\n",
      "Validation chunks: 2074\n"
     ]
    }
   ],
   "source": [
    "# Group texts into chunks of block_size\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Drop the remainder\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    # Split into chunks\n",
    "    result = {\n",
    "        k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {BLOCK_SIZE}\",\n",
    ")\n",
    "\n",
    "print(f\"Train chunks: {len(lm_datasets['train'])}\")\n",
    "print(f\"Validation chunks: {len(lm_datasets['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee790dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 4962\n",
      "Eval batches: 519\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    lm_datasets[\"validation\"],\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Eval batches: {len(eval_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796a8b2",
   "metadata": {},
   "source": [
    "## 4. Setup Training\n",
    "\n",
    "Configure optimizer, scheduler, and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d69f8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Learning rate: 0.0002\n",
      "  Weight decay: 0.01\n",
      "  Max train steps: 100\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 1  # Use 1 epoch for demo\n",
    "MAX_TRAIN_STEPS = 100  # Limit steps for demo\n",
    "WARMUP_STEPS = 10\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Max train steps: {MAX_TRAIN_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2230cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 82,101,120\n"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Set all parameters trainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2dd98fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer configured\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer with weight decay\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "print(\"Optimizer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8803df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR scheduler configured\n"
     ]
    }
   ],
   "source": [
    "# Setup learning rate scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=MAX_TRAIN_STEPS,\n",
    ")\n",
    "\n",
    "print(\"LR scheduler configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3d417",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Run the fine-tuning loop with the transformed optical model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d6dd2",
   "metadata": {},
   "source": [
    "### Quantization Statistics Warmup\n",
    "\n",
    "**Important:** The optical transformer layers require calibration of their quantization statistics (min/max values) before they can work correctly. Without this warmup:\n",
    "- The statistics are initialized to `[inf, -inf]`\n",
    "- The quantized matmul operations produce NaN values\n",
    "- Loss and perplexity become NaN\n",
    "\n",
    "We run a few forward passes in **training mode** to let the layers collect statistics from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3653ac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running warmup to initialize quantization statistics...\n",
      "  Warmup batch 1/5\n",
      "  Warmup batch 2/5\n",
      "  Warmup batch 3/5\n",
      "  Warmup batch 4/5\n",
      "  Warmup batch 5/5\n",
      "Warmup complete! Quantization statistics initialized.\n"
     ]
    }
   ],
   "source": [
    "# Warmup: Run a few forward passes in training mode to initialize quantization statistics\n",
    "# This is necessary because the optical transformer layers need to calibrate their\n",
    "# min/max statistics before they can perform quantized operations correctly.\n",
    "\n",
    "print(\"Running warmup to initialize quantization statistics...\")\n",
    "model.train()  # Must be in training mode to update stats\n",
    "num_warmup_batches = 5\n",
    "\n",
    "with torch.no_grad():  # No need for gradients during warmup\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        if i >= num_warmup_batches:\n",
    "            break\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        _ = model(**batch)\n",
    "        print(f\"  Warmup batch {i+1}/{num_warmup_batches}\")\n",
    "\n",
    "print(\"Warmup complete! Quantization statistics initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2300b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(model, eval_dataloader, device):\n",
    "    \"\"\"Evaluate model and return perplexity.\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        losses.append(outputs.loss.item())\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd599352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating before training...\n",
      "Initial - Loss: 6.1221, Perplexity: 455.84\n"
     ]
    }
   ],
   "source": [
    "# Evaluate before training\n",
    "print(\"Evaluating before training...\")\n",
    "eval_loss, eval_ppl = evaluate(model, eval_dataloader, DEVICE)\n",
    "print(f\"Initial - Loss: {eval_loss:.4f}, Perplexity: {eval_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9132b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 100/100 [00:24<00:00,  4.10it/s, loss=5.2910]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed! Steps: 100\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "train_losses = []\n",
    "\n",
    "progress_bar = tqdm(range(MAX_TRAIN_STEPS), desc=\"Training\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "        # Log progress\n",
    "        if completed_steps % 20 == 0:\n",
    "            avg_loss = sum(train_losses[-20:]) / min(20, len(train_losses))\n",
    "            progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "        if completed_steps >= MAX_TRAIN_STEPS:\n",
    "            break\n",
    "\n",
    "    if completed_steps >= MAX_TRAIN_STEPS:\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining completed! Steps: {completed_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18251766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating after training...\n",
      "Final - Loss: 5.3070, Perplexity: 201.75\n"
     ]
    }
   ],
   "source": [
    "# Evaluate after training\n",
    "print(\"\\nEvaluating after training...\")\n",
    "eval_loss, eval_ppl = evaluate(model, eval_dataloader, DEVICE)\n",
    "print(f\"Final - Loss: {eval_loss:.4f}, Perplexity: {eval_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3e09b",
   "metadata": {},
   "source": [
    "## 6. Save the Fine-tuned Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "472d0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the model\n",
    "# OUTPUT_DIR = \"./ot-llama-finetuned\"\n",
    "# model.save_pretrained(OUTPUT_DIR)\n",
    "# tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "# print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de471e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading a HuggingFace Llama model** using `AutoModelForCausalLM`\n",
    "2. **Configuring the optical transform** with `OtTransformConfig`\n",
    "3. **Applying the transform pass** using `optical_transformer_module_transform_pass`\n",
    "4. **Preparing a dataset** for causal language modeling\n",
    "5. **Running fine-tuning** with the transformed optical model\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- Use `attn_implementation=\"eager\"` when loading the model for compatibility\n",
    "- The transform pass uses regex patterns to match layer names\n",
    "- Training mode (`model.train()`) enables statistics updates in optical layers\n",
    "- The optical quantization adds noise that the model learns to be robust against\n",
    "\n",
    "### References\n",
    "\n",
    "- [Optical Transformers Paper](https://arxiv.org/abs/2302.10360)\n",
    "- MASE ONN Transform: `src/chop/passes/module/transforms/onn/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
