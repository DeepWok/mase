{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a79ad55",
   "metadata": {},
   "source": [
    "# Optical Transformer Transform Pass\n",
    "\n",
    "This tutorial provides minimal documentation for the Optical Neural Network (ONN) transform pass and layer classes in MASE.\n",
    "\n",
    "The optical transformer implementation is based on the [Optical Transformers paper](https://arxiv.org/abs/2302.10360).\n",
    "\n",
    "## Overview\n",
    "\n",
    "The ONN transform pass replaces standard PyTorch modules with their optical transformer equivalents:\n",
    "\n",
    "| Original Module | Optical Equivalent |\n",
    "|-----------------|--------------------|\n",
    "| `torch.nn.Linear` | `OtLinear` |\n",
    "| `LlamaAttention` | `OtLlamaAttention` |\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The `mase-triton` package is required for ONN transforms:\n",
    "\n",
    "```bash\n",
    "pip install mase-triton\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984ca459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zz7522/miniconda3/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaConfig\n",
    "\n",
    "from chop.passes.module.transforms.onn.transform import (\n",
    "    OtLinear,\n",
    "    OtLlamaAttention,\n",
    "    OtTransformConfig,\n",
    "    optical_transformer_module_transform_pass,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4e439",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Use `OtTransformConfig` to configure the optical transform parameters:\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `q_levels` | int | 256 | Number of quantization levels, $2^n$ for n-bit quantization. |\n",
    "| `q_lut_min` | float | 0.020040 | Minimum LUT value for quantization |\n",
    "| `q_smooth_factor` | float | 0.9 | Smoothing factor for statistics updates in the training mode |\n",
    "| `q_init_seed` | int | 0 | Random seed for initialization (only used in triton kernels) |\n",
    "| `q_bypass` | bool | False | If True, bypass optical quantization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7c1bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default ONN config: {'q_levels': 256, 'q_lut_min': 0.02004, 'q_smooth_factor': 0.9, 'q_init_seed': 0, 'q_bypass': False}\n",
      "Modified ONN config: {'q_levels': 256, 'q_lut_min': 0.02004, 'q_smooth_factor': 0.1, 'q_init_seed': 0, 'q_bypass': False}\n"
     ]
    }
   ],
   "source": [
    "# Create default configuration\n",
    "onn_config = OtTransformConfig.create_default()\n",
    "print(\"Default ONN config:\", onn_config)\n",
    "\n",
    "# Customize configuration\n",
    "onn_config[\"q_levels\"] = 256 # 8-bit quantization\n",
    "onn_config[\"q_smooth_factor\"] = 0.1\n",
    "print(\"Modified ONN config:\", onn_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de082853",
   "metadata": {},
   "source": [
    "## OtLinear: Optical Linear Layer\n",
    "\n",
    "`OtLinear` is the optical equivalent of `torch.nn.Linear`. It applies quantized matrix multiplication that simulates optical computing behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8b9c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output shape: torch.Size([2, 64])\n",
      "Optical output shape: torch.Size([2, 64])\n",
      "Max absolute difference: 0.035205\n"
     ]
    }
   ],
   "source": [
    "# Create a standard linear layer\n",
    "linear = torch.nn.Linear(in_features=32, out_features=64)\n",
    "\n",
    "# Convert to optical linear layer\n",
    "onn_config = OtTransformConfig.create_default()\n",
    "linear_onn = OtLinear.from_linear(linear, **onn_config)\n",
    "\n",
    "# Compare outputs\n",
    "x = torch.randn(2, 32)\n",
    "y = linear(x)\n",
    "y_onn = linear_onn(x)\n",
    "\n",
    "print(f\"Original output shape: {y.shape}\")\n",
    "print(f\"Optical output shape: {y_onn.shape}\")\n",
    "print(f\"Max absolute difference: {(y - y_onn).abs().max().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e368f",
   "metadata": {},
   "source": [
    "## OtLlamaAttention: Optical Llama Attention\n",
    "\n",
    "`OtLlamaAttention` replaces the HuggingFace `LlamaAttention` with an optical-aware implementation that uses quantized scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7a6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output shape: torch.Size([1, 16, 384])\n",
      "Optical output shape: torch.Size([1, 16, 384])\n"
     ]
    }
   ],
   "source": [
    "# Setup Llama configuration\n",
    "model_name = \"AICrossSim/clm-60m\"\n",
    "hf_config = LlamaConfig.from_pretrained(model_name)\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 16\n",
    "head_dim = hf_config.hidden_size // hf_config.num_attention_heads\n",
    "\n",
    "# Create standard attention layer\n",
    "attn = LlamaAttention(config=hf_config, layer_idx=0)\n",
    "\n",
    "# Convert to optical attention\n",
    "onn_config = OtTransformConfig.create_default()\n",
    "onn_config[\"q_levels\"] = 512\n",
    "attn_onn = OtLlamaAttention.from_pretrained(attn, layer_idx=0, **onn_config)\n",
    "\n",
    "# Test forward pass\n",
    "pos_emb = torch.ones(batch_size, seq_len, head_dim)\n",
    "x = 3 * torch.randn(batch_size, seq_len, hf_config.hidden_size)\n",
    "\n",
    "y, _ = attn(x, (pos_emb, pos_emb), None)\n",
    "attn_onn.train()  # Enable statistics updates\n",
    "y_onn, _ = attn_onn(x, (pos_emb, pos_emb), None)\n",
    "\n",
    "print(f\"Original output shape: {y.shape}\")\n",
    "print(f\"Optical output shape: {y_onn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac49d15",
   "metadata": {},
   "source": [
    "## Transform Pass: Network-Level Transformation\n",
    "\n",
    "Use `optical_transformer_module_transform_pass` to transform an entire network. The pass replaces modules based on name matching.\n",
    "\n",
    "### Pass Arguments\n",
    "\n",
    "| Key | Description |\n",
    "|-----|-------------|\n",
    "| `by` | Matching mode: `\"name\"` (exact) or `\"regex_name\"` (regex pattern) |\n",
    "| `<layer_name>` | Configuration dict for layers matching the name/pattern |\n",
    "| `default` | Fallback configuration if no pattern matches |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdee282d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original network:\n",
      "SimpleNetwork(\n",
      "  (attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "    (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "    (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  )\n",
      "  (linear): Linear(in_features=384, out_features=384, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a simple network with attention and linear layers\n",
    "class SimpleNetwork(torch.nn.Module):\n",
    "    def __init__(self, hf_config):\n",
    "        super().__init__()\n",
    "        self.attn = LlamaAttention(config=hf_config, layer_idx=0)\n",
    "        self.linear = torch.nn.Linear(\n",
    "            in_features=hf_config.hidden_size,\n",
    "            out_features=hf_config.hidden_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pos_emb):\n",
    "        attn_output, _ = self.attn(x, (pos_emb, pos_emb), None)\n",
    "        output = self.linear(attn_output)\n",
    "        return output\n",
    "\n",
    "network = SimpleNetwork(hf_config)\n",
    "print(\"Original network:\")\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffcb8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing keys when loading state_dict: ['query_min_max', 'key_min_max', 'qk_min_max', 'attn_min_max', 'value_min_max', 'av_min_max', 'seed'] from LlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ") to OtLlamaAttention(\n",
      "  (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "  (k_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=384, out_features=128, bias=False)\n",
      "  (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      ")\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=128, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=False) to OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "WARNING:root:Missing keys when loading state_dict: ['q_min_max_quantile', 'x_min_max', 'w_min_max', 'out_min_max', 'seed'] from Linear(in_features=384, out_features=384, bias=True) to OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed network:\n",
      "SimpleNetwork(\n",
      "  (attn): OtLlamaAttention(\n",
      "    (q_proj): OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "    (k_proj): OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "    (v_proj): OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "    (o_proj): OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      "  )\n",
      "  (linear): OpticalTransformerLinear(q_bypass=False, q_levels=512, q_lut_min=0.02004, q_quantiles=[0.0010000000474974513, 0.9990000128746033], x_min_max=tensor([inf, -inf]), w_min_max=tensor([inf, -inf]), out_min_max=tensor([inf, -inf]), seed=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Configure the transform pass with regex patterns\n",
    "onn_config = OtTransformConfig.create_default()\n",
    "onn_config[\"q_levels\"] = 512\n",
    "\n",
    "pass_args = {\n",
    "    \"by\": \"regex_name\",  # Use regex matching\n",
    "    \"attn\": onn_config,  # Transform the attention layer\n",
    "    \"linear\": onn_config,  # Transform the linear layer\n",
    "    r\"attn\\.(q|k|v|o)_proj\": onn_config,  # Transform Q/K/V/O projections inside attention\n",
    "}\n",
    "\n",
    "# Apply the transform\n",
    "network_onn = optical_transformer_module_transform_pass(network, pass_args)\n",
    "\n",
    "print(\"\\nTransformed network:\")\n",
    "print(network_onn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3557ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification:\n",
      "  attn is OtLlamaAttention: True\n",
      "  linear is OtLinear: True\n",
      "  attn.q_proj is OtLinear: True\n",
      "  attn.k_proj is OtLinear: True\n",
      "  attn.v_proj is OtLinear: True\n",
      "  attn.o_proj is OtLinear: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the transformation\n",
    "print(\"Verification:\")\n",
    "print(f\"  attn is OtLlamaAttention: {isinstance(network_onn.attn, OtLlamaAttention)}\")\n",
    "print(f\"  linear is OtLinear: {isinstance(network_onn.linear, OtLinear)}\")\n",
    "print(f\"  attn.q_proj is OtLinear: {isinstance(network_onn.attn.q_proj, OtLinear)}\")\n",
    "print(f\"  attn.k_proj is OtLinear: {isinstance(network_onn.attn.k_proj, OtLinear)}\")\n",
    "print(f\"  attn.v_proj is OtLinear: {isinstance(network_onn.attn.v_proj, OtLinear)}\")\n",
    "print(f\"  attn.o_proj is OtLinear: {isinstance(network_onn.attn.o_proj, OtLinear)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355b1f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 16, 384])\n",
      "Max output error: 0.029137\n",
      "Output is finite: True\n"
     ]
    }
   ],
   "source": [
    "# Test the transformed network\n",
    "network_onn.train()  # Enable statistics updates\n",
    "\n",
    "pos_emb = torch.ones(batch_size, seq_len, head_dim)\n",
    "x = 3 * torch.randn(batch_size, seq_len, hf_config.hidden_size)\n",
    "\n",
    "y = network(x, pos_emb)\n",
    "y_onn = network_onn(x, pos_emb)\n",
    "print(f\"Output shape: {y_onn.shape}\")\n",
    "print(f\"Max output error: {(y - y_onn).abs().max().item():.6f}\")\n",
    "print(f\"Output is finite: {y_onn.isfinite().all().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
