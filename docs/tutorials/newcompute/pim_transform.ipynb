{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188ce84e",
   "metadata": {},
   "source": [
    "# PIM Simulation: PCM as an example \n",
    "This tutorial demonstrates how to use the Mase framework to model and simulate the models' behaviour on PIM(process in memory) devices. \n",
    "In this tutorial, we will focus on simulating the behaviour of PCM devices (phase change memory). About the detail explanation of the device simulation, \n",
    "we can see the full main documentation for more details.\n",
    "Here just show the simulation and evaluation pipeline with mase framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de28c6",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "You need a Python environment with `torch`, `transformers`, and `mase` installed. Please refer to the documentation installation for more details.\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc7bdc9",
   "metadata": {},
   "source": [
    "# Evaluation Settings\n",
    "Model: RoBERTa\n",
    "Dataset: \n",
    "\n",
    "Transform Config: \n",
    "```\n",
    "q_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"tile_type\": \"pcm\",\n",
    "            \"core_size\": 256,\n",
    "            \"num_bits\": 8,\n",
    "            \"programming_noise\": True,\n",
    "            \"read_noise\": True,\n",
    "            \"ir_drop\": True,\n",
    "            \"out_noise\": True,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5746011",
   "metadata": {},
   "source": [
    "# Transform configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc24621",
   "metadata": {},
   "source": [
    "# Evaluation with golden model\n",
    "In this section, we evaluate the baseline RoBERTa model on the MNLI (Multi-Genre Natural Language Inference) dataset. The MNLI dataset consists of pairs of sentences (a premise and a hypothesis) and the goal is to predict whether the premise entails, contradicts, or is neutral towards the hypothesis.\n",
    "\n",
    "We use the `JeremiahZ/roberta-base-mnli` model, which is a RoBERTa-base model fine-tuned on MNLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b1a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 614/614 [00:16<00:00, 36.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden Model Accuracy: 0.8728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from chop.dataset.nlp.text_entailment import TextEntailmentDatasetMNLI\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device).squeeze(-1)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "pretrained = \"JeremiahZ/roberta-base-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
    "model = RobertaForSequenceClassification.from_pretrained(pretrained)\n",
    "\n",
    "# Load a small subset of MNLI validation set for quick evaluation\n",
    "dataset = TextEntailmentDatasetMNLI(split=\"validation_matched\", tokenizer=tokenizer, max_token_len=128, num_workers=4)\n",
    "dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy = evaluate(model, dataloader, device)\n",
    "print(f\"Golden Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14ea94",
   "metadata": {},
   "source": [
    "# Evaluation with transformed model\n",
    "Now we apply the `pim_matmul_transform_pass` to simulate the PIM hardware. We configure the transform to use PCM (Phase Change Memory) tiles with a core size of 256. We can enable various non-idealities like programming noise and read noise to see how they impact the model's performance.\n",
    "\n",
    "### Configuration Details:\n",
    "For more detailed information, please refer to ...\n",
    "- `tile_type`: The type of PIM technology to simulate (e.g., 'pcm', 'sram', 'reram').\n",
    "- `core_size`: The size of the crossbar array (e.g., 256x256).\n",
    "- `num_bits`: The number of bits used for weights and activations.\n",
    "- `programming_noise`: Simulates variability during the programming of PIM cells.\n",
    "- `read_noise`: Simulates noise during the read-out process.\n",
    "- `ir_drop`: Simulates voltage drops along the crossbar lines.\n",
    "- `out_noise`: Simulates noise at the output of the crossbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67f7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 7/7 [00:05<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Model Accuracy (with PIM noise): 0.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from chop.passes.module.transforms import cim_matmul_transform_pass\n",
    "import copy\n",
    "\n",
    "q_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"tile_type\": \"pcm\",\n",
    "            \"core_size\": 256,\n",
    "            \"num_bits\": 8,\n",
    "            \"programming_noise\": True,\n",
    "            \"read_noise\": True,\n",
    "            \"ir_drop\": True,\n",
    "            \"out_noise\": True,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Apply the transform pass\n",
    "transformed_model = copy.deepcopy(model)\n",
    "qmodel, _ = cim_matmul_transform_pass(transformed_model, q_config)\n",
    "\n",
    "q_accuracy = evaluate(qmodel, dataloader, device)\n",
    "print(f\"Transformed Model Accuracy (with PIM noise): {q_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this tutorial, we demonstrated how to:\n",
    "1. Load a pretrained RoBERTa model and evaluate it on the MNLI dataset.\n",
    "2. Use `cim_matmul_transform_pass` to simulate hardware non-idealities for PIM devices.\n",
    "3. Evaluate the impact of these non-idealities on model accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
