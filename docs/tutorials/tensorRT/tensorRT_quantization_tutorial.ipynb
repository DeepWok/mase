{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome To the TensorRT Quantization Tutorial!\n",
    "\n",
    "This notebook is designed to show the features of the TensorRT passes integrated into MASE.\n",
    "\n",
    "## Section 1 - INT8 Quantization\n",
    "Firstly, we will show you how to do a INT8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model using the `Machop API`. The quantization process is split into the following stages, each their own individual pass:\n",
    "\n",
    "1. Fake quantization: using the `fake\n",
    "2. Calibration - \n",
    "3. Quantized Aware Training - \n",
    "4. Quantization - \n",
    "5. Analysis - \n",
    "\n",
    "\n",
    "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-16 15:25:08,775] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 15:25:10.852008 139941960832832 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import toml\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "# Figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "# Add directory to the PATH so that chop can be called\n",
    "new_path = \"../../../machop\"\n",
    "full_path = os.path.abspath(new_path)\n",
    "os.environ['PATH'] += os.pathsep + full_path\n",
    "\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "from chop.models import get_model_info, get_model\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.passes.graph import (\n",
    "    save_node_meta_param_interface_pass,\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    tensorrt_analysis_pass,\n",
    "    )\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml), or read the documentation on Mase [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your TOML file\n",
    "toml_file_path = '../../../machop/configs/tensorRT/jsc_toy_INT8_quantization_by_type.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(toml_file_path, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt_quantize' section and its children\n",
    "tensorrt_quantize_config = pass_args.get('passes', {}).get('tensorrt_quantize', {})\n",
    "# Extract the 'passes.tensorrt_fine_tune' section and its children\n",
    "tensorrt_train_config = pass_args.get('passes', {}).get('tensorrt_fine_tune', {})\n",
    "# Extract the 'passes.tensorrt_analysis' section and its children\n",
    "tensorrt_analysis_config = pass_args.get('passes', {}).get('tensorrt_analysis', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MaseGraph` by loading in a pre-trained model using the checkpoint provided and using the toml configuration model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\u001b[0m\n",
      "I0316 15:20:13.324839 140527413331776 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "# Load the basics in to \n",
    "model_name = pass_args['model']\n",
    "dataset_name = pass_args['dataset']\n",
    "max_epochs = pass_args['max_epochs']\n",
    "batch_size = pass_args['batch_size']\n",
    "learning_rate = pass_args['learning_rate']\n",
    "accelerator = pass_args['accelerator']\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we will copy the original `MaseGraph` to use for comparison during quantization analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_original = deepcopy_mase_graph(mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1 Fake Quantization\n",
    "\n",
    "First we fake quantize the module to perform calibration and fine tuning before we actually quantize - this is only required if we have INT8 calibration as other precisions are not currently supported within [pytorch-quantization](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#) library.\n",
    "\n",
    "This is acheived through the `tensorrt_fake_quantize_transform_pass` which goes through the model, either by type or by name, replaces each layer appropriately to a quantized form passing data samples to the quantizer and deciding the best amax for activations\n",
    "\n",
    "Calibrators can be added as a search space parameter to examine the best performing calibrator. The calibrators have been included in the toml as follows.\n",
    "For example: `calibrators = [\"percentile\", \"mse\", \"entropy\"]`\n",
    "\n",
    "Note: \n",
    "- To use `percentile` calibration, a list of percentiles must be given\n",
    "- To use `max` calibration, the `histogram` weight and input calibrators must be removed and replaced with `max`. This will use global maximum absolute value to calibrate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0316 15:20:15.177225 140527413331776 utils.py:132] Applying fake quantization to PyTorch model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0316 15:20:16.027739 140527413331776 utils.py:148] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0316 15:20:16.030015 140527413331776 calibrate.py:62] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 15:20:16.034828 140527413331776 calibrate.py:71] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 15:20:16.036426 140527413331776 calibrate.py:71] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 15:20:16.038123 140527413331776 calibrate.py:71] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 15:20:16.039608 140527413331776 calibrate.py:71] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 15:20:16.041212 140527413331776 calibrate.py:71] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 15:20:16.042683 140527413331776 calibrate.py:71] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 15:20:17.498440 140527413331776 calibrate.py:90] Enabling Quantization and Disabling Calibration\n",
      "W0316 15:20:17.501315 140527413331776 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 15:20:17.502069 140527413331776 calibrate.py:90] Enabling Quantization and Disabling Calibration\n",
      "W0316 15:20:17.503381 140527413331776 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 15:20:17.504220 140527413331776 calibrate.py:90] Enabling Quantization and Disabling Calibration\n",
      "W0316 15:20:17.505513 140527413331776 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 15:20:17.506152 140527413331776 calibrate.py:90] Enabling Quantization and Disabling Calibration\n",
      "W0316 15:20:17.507394 140527413331776 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 15:20:17.508199 140527413331776 calibrate.py:90] Enabling Quantization and Disabling Calibration\n",
      "W0316 15:20:17.509486 140527413331776 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 15:20:17.510794 140527413331776 calibrate.py:90] Enabling Quantization and Disabling Calibration\n",
      "W0316 15:20:17.513329 140527413331776 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0316 15:20:17.515537 140527413331776 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 15:20:17.516159 140527413331776 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.8638 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 15:20:17.517521 140527413331776 calibrate.py:57] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.8638 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 15:20:17.519428 140527413331776 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 15:20:17.520226 140527413331776 calibrate.py:57] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 15:20:17.522336 140527413331776 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.6883 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 15:20:17.523161 140527413331776 calibrate.py:57] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.6883 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 15:20:17.525291 140527413331776 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 15:20:17.526273 140527413331776 calibrate.py:57] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 15:20:17.528283 140527413331776 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0551 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 15:20:17.529116 140527413331776 calibrate.py:57] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=3.0551 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 15:20:17.531247 140527413331776 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 15:20:17.532145 140527413331776 calibrate.py:57] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPerforming post calibration analysis for {calib}...\u001b[0m\n",
      "I0316 15:20:17.535103 140527413331776 calibrate.py:117] Performing post calibration analysis for {calib}...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 15:20:25.072784 140527413331776 analysis.py:202] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.73137   |\n",
      "|      Average Precision       |   0.74323   |\n",
      "|        Average Recall        |   0.72981   |\n",
      "|       Average F1 Score       |   0.73286   |\n",
      "|         Average Loss         |   0.76124   |\n",
      "|       Average Latency        |  6.0004 ms  |\n",
      "|   Average GPU Power Usage    |  65.165 W   |\n",
      "| Inference Energy Consumption | 0.10861 mWh |\n",
      "+------------------------------+-------------+\u001b[0m\n",
      "I0316 15:20:27.538049 140527413331776 analysis.py:303] \n",
      "Results jsc-toy:\n",
      "+------------------------------+-------------+\n",
      "|            Metric            |    Value    |\n",
      "+------------------------------+-------------+\n",
      "|    Average Test Accuracy     |   0.73137   |\n",
      "|      Average Precision       |   0.74323   |\n",
      "|        Average Recall        |   0.72981   |\n",
      "|       Average F1 Score       |   0.73286   |\n",
      "|         Average Loss         |   0.76124   |\n",
      "|       Average Latency        |  6.0004 ms  |\n",
      "|   Average GPU Power Usage    |  65.165 W   |\n",
      "| Inference Energy Consumption | 0.10861 mWh |\n",
      "+------------------------------+-------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPost calibration analysis complete.\u001b[0m\n",
      "I0316 15:20:27.541378 140527413331776 calibrate.py:119] Post calibration analysis complete.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSucceeded in calibrating the model in PyTorch!\u001b[0m\n",
      "I0316 15:20:27.544898 140527413331776 calibrate.py:121] Succeeded in calibrating the model in PyTorch!\n",
      "I0316 15:20:47.294253 140527413331776 rank_zero.py:64] GPU available: True (cuda), used: True\n",
      "I0316 15:20:47.359323 140527413331776 rank_zero.py:64] TPU available: False, using: 0 TPU cores\n",
      "I0316 15:20:47.360296 140527413331776 rank_zero.py:64] IPU available: False, using: 0 IPUs\n",
      "I0316 15:20:47.361088 140527413331776 rank_zero.py:64] HPU available: False, using: 0 HPUs\n",
      "I0316 15:20:51.439555 140527413331776 cuda.py:61] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "I0316 15:20:51.470865 140527413331776 model_summary.py:94] \n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | GraphModule        | 327   \n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | acc_train | MulticlassAccuracy | 0     \n",
      "3 | loss_val  | MeanMetric         | 0     \n",
      "4 | loss_test | MeanMetric         | 0     \n",
      "-------------------------------------------------\n",
      "327       Trainable params\n",
      "0         Non-trainable params\n",
      "327       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 3084/3084 [02:12<00:00, 23.34it/s, v_num=2, train_acc_step=0.679, val_acc_epoch=0.733, val_loss_epoch=0.751]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0316 15:23:03.750215 140527413331776 rank_zero.py:64] `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 3084/3084 [02:12<00:00, 23.33it/s, v_num=2, train_acc_step=0.679, val_acc_epoch=0.733, val_loss_epoch=0.751]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to ONNX...\u001b[0m\n",
      "I0316 15:23:03.785155 140527413331776 quantize.py:129] Converting PyTorch model to ONNX...\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:363: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:366: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/opt/conda/envs/mase/lib/python3.11/site-packages/pytorch_quantization/tensor_quant.py:382: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_16/version_6/model.onnx\u001b[0m\n",
      "I0316 15:23:04.103081 140527413331776 quantize.py:152] ONNX Conversion Complete. Stored ONNX model to /root/mase/mase_output/TensorRT/Quantization/ONNX/2024_03_16/version_6/model.onnx\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mConverting PyTorch model to TensorRT...\u001b[0m\n",
      "I0316 15:23:04.105526 140527413331776 quantize.py:55] Converting PyTorch model to TensorRT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-15:23:13] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    }
   ],
   "source": [
    "configs = [tensorrt_quantize_config, tensorrt_train_config, tensorrt_analysis_config]\n",
    "for config in configs:\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3 Quantized Aware Training\n",
    "\n",
    "The `tensorrt_fine_tune_transform_pass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Convert and store to ONNX and then TensorRT\n",
    "mg, trt_meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-16 15:32:40,047] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO: Seed set to 0\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 15:32:42.672156 140607680886592 seed.py:54] Seed set to 0\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| Name                    |        Default         | Config. File |     Manual Override      |        Effective         |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "| task                    |     classification     |              |                          |      classification      |\n",
      "| load_name               |          \u001b[38;5;8mNone\u001b[0m          |              | /root/mase/docs/tutorial | /root/mase/docs/tutorial |\n",
      "|                         |                        |              | s/tensorRT/checkpoints/j | s/tensorRT/checkpoints/j |\n",
      "|                         |                        |              | sc-toy_classification_js | sc-toy_classification_js |\n",
      "|                         |                        |              | c/software/training_ckpt | c/software/training_ckpt |\n",
      "|                         |                        |              |       s/best.ckpt        |       s/best.ckpt        |\n",
      "| load_type               |           \u001b[38;5;8mmz\u001b[0m           |              |            pl            |            pl            |\n",
      "| batch_size              |          \u001b[38;5;8m128\u001b[0m           |     256      |                          |           256            |\n",
      "| to_debug                |         False          |              |                          |          False           |\n",
      "| log_level               |          info          |              |                          |           info           |\n",
      "| report_to               |      tensorboard       |              |                          |       tensorboard        |\n",
      "| seed                    |           0            |              |                          |            0             |\n",
      "| quant_config            |          None          |              |                          |           None           |\n",
      "| training_optimizer      |          adam          |              |                          |           adam           |\n",
      "| trainer_precision       |        16-mixed        |              |                          |         16-mixed         |\n",
      "| learning_rate           |         \u001b[38;5;8m1e-05\u001b[0m          |    0.001     |                          |          0.001           |\n",
      "| weight_decay            |           0            |              |                          |            0             |\n",
      "| max_epochs              |           \u001b[38;5;8m20\u001b[0m           |      10      |                          |            10            |\n",
      "| max_steps               |           -1           |              |                          |            -1            |\n",
      "| accumulate_grad_batches |           1            |              |                          |            1             |\n",
      "| log_every_n_steps       |           50           |              |                          |            50            |\n",
      "| num_workers             |           48           |              |                          |            48            |\n",
      "| num_devices             |           1            |              |                          |            1             |\n",
      "| num_nodes               |           1            |              |                          |            1             |\n",
      "| accelerator             |          \u001b[38;5;8mauto\u001b[0m          |     gpu      |                          |           gpu            |\n",
      "| strategy                |          auto          |              |                          |           auto           |\n",
      "| is_to_auto_requeue      |         False          |              |                          |          False           |\n",
      "| github_ci               |         False          |              |                          |          False           |\n",
      "| disable_dataset_cache   |         False          |              |                          |          False           |\n",
      "| target                  |  xcu250-figd2104-2L-e  |              |                          |   xcu250-figd2104-2L-e   |\n",
      "| num_targets             |          100           |              |                          |           100            |\n",
      "| is_pretrained           |         False          |              |                          |          False           |\n",
      "| max_token_len           |          512           |              |                          |           512            |\n",
      "| project_dir             | /root/mase/mase_output |              |                          |  /root/mase/mase_output  |\n",
      "| project                 |          None          |              |                          |           None           |\n",
      "| model                   |          \u001b[38;5;8mNone\u001b[0m          |   jsc-toy    |                          |         jsc-toy          |\n",
      "| dataset                 |          \u001b[38;5;8mNone\u001b[0m          |     jsc      |                          |           jsc            |\n",
      "| t_max                   |           20           |              |                          |            20            |\n",
      "| eta_min                 |         1e-06          |              |                          |          1e-06           |\n",
      "+-------------------------+------------------------+--------------+--------------------------+--------------------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'jsc-toy'...\u001b[0m\n",
      "I0316 15:32:43.016940 140607680886592 cli.py:841] Initialising model 'jsc-toy'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mInitialising dataset 'jsc'...\u001b[0m\n",
      "I0316 15:32:43.039944 140607680886592 cli.py:869] Initialising dataset 'jsc'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mProject will be created at /root/mase/mase_output/jsc-toy_classification_jsc_2024-03-16\u001b[0m\n",
      "I0316 15:32:43.040448 140607680886592 cli.py:905] Project will be created at /root/mase/mase_output/jsc-toy_classification_jsc_2024-03-16\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTransforming model 'jsc-toy'...\u001b[0m\n",
      "I0316 15:32:43.098519 140607680886592 cli.py:365] Transforming model 'jsc-toy'...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /root/mase/docs/tutorials/tensorRT/checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\u001b[0m\n",
      "I0316 15:32:48.684521 140607680886592 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /root/mase/docs/tutorials/tensorRT/checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\n",
      "> /root/mase/machop/chop/actions/transform.py(103)transform()\n",
      "-> pass_config['accelerator'] = accelerator.type\n",
      "(Pdb) \n",
      "--KeyboardInterrupt--\n",
      "(Pdb) "
     ]
    }
   ],
   "source": [
    "!ch transform --config ../../../machop/configs/tensorRT/jsc_toy_INT8_quantization_by_type.toml --load checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\u001b[0m\n",
      "I0316 15:12:06.617402 140528819410752 analysis.py:128] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 15:12:06.620040 140528819410752 analysis.py:202] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-15:12:06] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73309    |\n",
      "|      Average Precision       |   0.74935    |\n",
      "|        Average Recall        |   0.73329    |\n",
      "|       Average F1 Score       |   0.73688    |\n",
      "|         Average Loss         |    0.7531    |\n",
      "|       Average Latency        |  0.2472 ms   |\n",
      "|   Average GPU Power Usage    |   67.63 W    |\n",
      "| Inference Energy Consumption | 0.004644 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0316 15:12:13.257380 140528819410752 analysis.py:303] \n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73309    |\n",
      "|      Average Precision       |   0.74935    |\n",
      "|        Average Recall        |   0.73329    |\n",
      "|       Average F1 Score       |   0.73688    |\n",
      "|         Average Loss         |    0.7531    |\n",
      "|       Average Latency        |  0.2472 ms   |\n",
      "|   Average GPU Power Usage    |   67.63 W    |\n",
      "| Inference Energy Consumption | 0.004644 mWh |\n",
      "+------------------------------+--------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 15:12:13.266318 140528819410752 analysis.py:202] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73038    |\n",
      "|      Average Precision       |    0.7435    |\n",
      "|        Average Recall        |   0.72899    |\n",
      "|       Average F1 Score       |   0.73233    |\n",
      "|         Average Loss         |   0.76702    |\n",
      "|       Average Latency        |  4.8933 ms   |\n",
      "|   Average GPU Power Usage    |   68.022 W   |\n",
      "| Inference Energy Consumption | 0.092458 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0316 15:12:21.873455 140528819410752 analysis.py:303] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73038    |\n",
      "|      Average Precision       |    0.7435    |\n",
      "|        Average Recall        |   0.72899    |\n",
      "|       Average F1 Score       |   0.73233    |\n",
      "|         Average Loss         |   0.76702    |\n",
      "|       Average Latency        |  4.8933 ms   |\n",
      "|   Average GPU Power Usage    |   68.022 W   |\n",
      "| Inference Energy Consumption | 0.092458 mWh |\n",
      "+------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "trt_meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy INT8 only quantization \n",
    "trt_meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_0/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(trt_meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\u001b[0m\n",
      "I0316 11:24:17.414859 140497372997440 analysis.py:128] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:24:17.417420 140497372997440 analysis.py:202] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-11:24:17] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73325    |\n",
      "|      Average Precision       |    0.74924    |\n",
      "|        Average Recall        |    0.73356    |\n",
      "|       Average F1 Score       |    0.73732    |\n",
      "|         Average Loss         |    0.75403    |\n",
      "|       Average Latency        |   0.2609 ms   |\n",
      "|   Average GPU Power Usage    |   68.244 W    |\n",
      "| Inference Energy Consumption | 0.0049457 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "I0316 11:24:23.903356 140497372997440 analysis.py:306] \n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73325    |\n",
      "|      Average Precision       |    0.74924    |\n",
      "|        Average Recall        |    0.73356    |\n",
      "|       Average F1 Score       |    0.73732    |\n",
      "|         Average Loss         |    0.75403    |\n",
      "|       Average Latency        |   0.2609 ms   |\n",
      "|   Average GPU Power Usage    |   68.244 W    |\n",
      "| Inference Energy Consumption | 0.0049457 mWh |\n",
      "+------------------------------+---------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:24:23.913443 140497372997440 analysis.py:202] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73071    |\n",
      "|      Average Precision       |   0.74392    |\n",
      "|        Average Recall        |   0.72935    |\n",
      "|       Average F1 Score       |   0.73267    |\n",
      "|         Average Loss         |   0.76629    |\n",
      "|       Average Latency        |  5.0403 ms   |\n",
      "|   Average GPU Power Usage    |   68.492 W   |\n",
      "| Inference Energy Consumption | 0.095894 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0316 11:24:32.237802 140497372997440 analysis.py:306] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73071    |\n",
      "|      Average Precision       |   0.74392    |\n",
      "|        Average Recall        |   0.72935    |\n",
      "|       Average F1 Score       |   0.73267    |\n",
      "|         Average Loss         |   0.76629    |\n",
      "|       Average Latency        |  5.0403 ms   |\n",
      "|   Average GPU Power Usage    |   68.492 W   |\n",
      "| Inference Energy Consumption | 0.095894 mWh |\n",
      "+------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "trt_meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy FP16 only quantization \n",
    "trt_meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_1/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(trt_meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
