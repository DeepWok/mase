{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome To the TensorRT Quantization Tutorial!\n",
    "\n",
    "This notebook is designed to show the features of the TensorRT passes integrated into MASE.\n",
    "\n",
    "## Section 1\n",
    "Firstly, we will show you how to do a INT8 quantization of a simple model, `jsc-toy`, and compare the quantized model to the original model. \n",
    "\n",
    "We start by loading in the required libraries and passes required for the notebook as well as ensuring the correct path is set for machop to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mase/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-16 13:46:41,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0316 13:46:43.832612 140639127025472 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import toml\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "# Figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "# Add directory to the PATH so that chop can be called\n",
    "new_path = \"../../../machop\"\n",
    "full_path = os.path.abspath(new_path)\n",
    "os.environ['PATH'] += os.pathsep + full_path\n",
    "\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "from chop.passes.graph.utils import deepcopy_mase_graph\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "from chop.models import get_model_info, get_model\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.passes.graph import (\n",
    "    save_node_meta_param_interface_pass,\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    tensorrt_calibrate_transform_pass,\n",
    "    tensorrt_fake_quantize_transform_pass,\n",
    "    tensorrt_fine_tune_transform_pass,\n",
    "    tensorrt_engine_interface_pass,\n",
    "    tensorrt_analysis_pass,\n",
    "    )\n",
    "\n",
    "set_logging_verbosity(\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the toml file used for quantization. To view the configuration, click [here](../../machop/configs/tensorrt/jsc_toy_INT8_quantization_by_type.toml), or read the documentation on Mase [here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your TOML file\n",
    "toml_file_path = '../../../machop/configs/tensorRT/jsc_toy_INT8_quantization_by_type.toml'\n",
    "\n",
    "# Reading TOML file and converting it into a Python dictionary\n",
    "with open(toml_file_path, 'r') as toml_file:\n",
    "    pass_args = toml.load(toml_file)\n",
    "\n",
    "# Extract the 'passes.tensorrt_quantize' section and its children\n",
    "tensorrt_quantize_config = pass_args.get('passes', {}).get('tensorrt_quantize', {})\n",
    "# Extract the 'passes.tensorrt_fine_tune' section and its children\n",
    "tensorrt_train_config = pass_args.get('passes', {}).get('tensorrt_fine_tune', {})\n",
    "# Extract the 'passes.tensorrt_analysis' section and its children\n",
    "tensorrt_analysis_config = pass_args.get('passes', {}).get('tensorrt_analysis', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a `MaseGraph` by loading in a pre-trained model using the checkpoint provided and using the toml configuration model arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\u001b[0m\n",
      "I0316 13:46:49.444988 140639127025472 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt\"\n",
    "\n",
    "# Load the basics in to \n",
    "model_name = pass_args['model']\n",
    "dataset_name = pass_args['dataset']\n",
    "max_epochs = pass_args['max_epochs']\n",
    "batch_size = pass_args['batch_size']\n",
    "learning_rate = pass_args['learning_rate']\n",
    "accelerator = pass_args['accelerator']\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fake quantize the module to perform calibration and fine tuning - this is only required if we have INT8 calibration as currently, other precisions are not included within [pytorch-quantization](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.25s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "[2024-03-16 13:47:03,021] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "usage: ch [--config PATH] [--task TASK] [--load PATH] [--load-type]\n",
      "          [--batch-size NUM] [--debug] [--log-level]\n",
      "          [--report-to {wandb,tensorboard}] [--seed NUM] [--quant-config TOML]\n",
      "          [--training-optimizer TYPE] [--trainer-precision TYPE]\n",
      "          [--learning-rate NUM] [--weight-decay NUM] [--max-epochs NUM]\n",
      "          [--max-steps NUM] [--accumulate-grad-batches NUM]\n",
      "          [--log-every-n-steps NUM] [--cpu NUM] [--gpu NUM] [--nodes NUM]\n",
      "          [--accelerator TYPE] [--strategy TYPE] [--auto-requeue]\n",
      "          [--github-ci] [--disable-dataset-cache] [--target STR]\n",
      "          [--num-targets NUM] [--run-emit] [--skip-build] [--skip-test]\n",
      "          [--pretrained] [--max-token-len NUM] [--project-dir DIR]\n",
      "          [--project NAME] [--profile] [--no-warnings] [-h] [-V]\n",
      "          [--info [TYPE]]\n",
      "          action [model] [dataset]\n",
      "ch: error: argument --config: file not found\n"
     ]
    }
   ],
   "source": [
    "!ch transform --config configs/tensorRT/jsc_toy_INT8_quantization_by_type.toml --load ../../docs/tutorials/tensorRT/checkpoints/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ch transform --config ../../../machop/configs/tensorRT/jsc_toy_INT8_quantization_by_type.toml --load checkpoints/jsc-toy_classification_jsc/software/training_ckpts/best.ckpt --load-type pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling passes via python - to be removed from tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mApplying fake quantization to PyTorch model...\u001b[0m\n",
      "I0316 13:47:08.065116 140639127025472 utils.py:132] Applying fake quantization to PyTorch model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mFake quantization applied to PyTorch model.\u001b[0m\n",
      "I0316 13:47:08.969787 140639127025472 utils.py:148] Fake quantization applied to PyTorch model.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting calibration of the model in PyTorch...\u001b[0m\n",
      "I0316 13:47:08.971959 140639127025472 calibrate.py:81] Starting calibration of the model in PyTorch...\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 13:47:08.995031 140639127025472 calibrate.py:90] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 13:47:08.997697 140639127025472 calibrate.py:90] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 13:47:09.000303 140639127025472 calibrate.py:90] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 13:47:09.002133 140639127025472 calibrate.py:90] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 13:47:09.003904 140639127025472 calibrate.py:90] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mDisabling Quantization and Enabling Calibration\u001b[0m\n",
      "I0316 13:47:09.005647 140639127025472 calibrate.py:90] Disabling Quantization and Enabling Calibration\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 13:56:14.869713 140639127025472 calibrate.py:103] Enabling Quantization and Disabling Calibration\n",
      "W0316 13:56:14.871980 140639127025472 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 13:56:14.873425 140639127025472 calibrate.py:103] Enabling Quantization and Disabling Calibration\n",
      "W0316 13:56:14.875901 140639127025472 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 13:56:14.877211 140639127025472 calibrate.py:103] Enabling Quantization and Disabling Calibration\n",
      "W0316 13:56:14.879528 140639127025472 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 13:56:14.880782 140639127025472 calibrate.py:103] Enabling Quantization and Disabling Calibration\n",
      "W0316 13:56:14.883042 140639127025472 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 13:56:14.884415 140639127025472 calibrate.py:103] Enabling Quantization and Disabling Calibration\n",
      "W0316 13:56:14.886533 140639127025472 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mEnabling Quantization and Disabling Calibration\u001b[0m\n",
      "I0316 13:56:14.887748 140639127025472 calibrate.py:103] Enabling Quantization and Disabling Calibration\n",
      "W0316 13:56:14.889895 140639127025472 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0316 13:56:14.896854 140639127025472 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0316 13:56:14.898027 140639127025472 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.8045 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 13:56:14.899389 140639127025472 calibrate.py:76] seq_blocks.2._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.8045 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 13:56:14.902061 140639127025472 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 13:56:14.902941 140639127025472 calibrate.py:76] seq_blocks.2._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.7462 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 13:56:14.905179 140639127025472 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.1126 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 13:56:14.906018 140639127025472 calibrate.py:76] seq_blocks.5._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.1126 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 13:56:14.908212 140639127025472 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 13:56:14.909110 140639127025472 calibrate.py:76] seq_blocks.5._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 13:56:14.911533 140639127025472 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.6165 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 13:56:14.912394 140639127025472 calibrate.py:76] seq_blocks.8._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=2.6165 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "W0316 13:56:14.914792 140639127025472 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mseq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\u001b[0m\n",
      "I0316 13:56:14.915645 140639127025472 calibrate.py:76] seq_blocks.8._weight_quantizer          : TensorQuantizer(8bit fake per-tensor amax=0.5546 calibrator=HistogramCalibrator scale=1.0 quant)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Calibrator.evaluate() missing 1 required positional argument: 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calibrate model (passing data samples to the quantizer and deciding the best amax for activations)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m mg, _ \u001b[38;5;241m=\u001b[39m tensorrt_fake_quantize_transform_pass(mg, pass_args\u001b[38;5;241m=\u001b[39mtensorrt_quantize_config)\n\u001b[0;32m---> 24\u001b[0m mg, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtensorrt_calibrate_transform_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorrt_quantize_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Conduct QAT\u001b[39;00m\n\u001b[1;32m     27\u001b[0m mg, _ \u001b[38;5;241m=\u001b[39m tensorrt_fine_tune_transform_pass(mg, pass_args\u001b[38;5;241m=\u001b[39mtensorrt_quantize_config)\n",
      "File \u001b[0;32m~/mase/machop/chop/passes/graph/transforms/tensorrt/quantize/calibrate.py:34\u001b[0m, in \u001b[0;36mtensorrt_calibrate_transform_pass\u001b[0;34m(graph, pass_args)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensorrt_calibrate_transform_pass\u001b[39m(graph, pass_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     33\u001b[0m     calibrator \u001b[38;5;241m=\u001b[39m Calibrator(pass_args)\n\u001b[0;32m---> 34\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mcalibrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibrate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     graph\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule(graph\u001b[38;5;241m.\u001b[39mmodel, graph\u001b[38;5;241m.\u001b[39mfx_graph)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, {}\n",
      "File \u001b[0;32m~/mase/machop/chop/passes/graph/transforms/tensorrt/quantize/calibrate.py:127\u001b[0m, in \u001b[0;36mcalibrate_model\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mTypeError\u001b[0m: Calibrator.evaluate() missing 1 required positional argument: 'graph'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-tiny_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-trt_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/jsc-toy_classification_jsc_2024-03-05/software/training_ckpts/best.ckpt --load-type pl\n",
    "./ch transform --config configs/examples/jsc_trt_quantization.toml --load ../TensorRTDev/vgg7-test-accu-0.9332.ckpt --load-type pl\n",
    "'''\n",
    "\n",
    "configs = [tensorrt_quantize_config, tensorrt_train_config, tensorrt_analysis_config]\n",
    "for config in configs:\n",
    "    config['batch_size'] = pass_args['batch_size']\n",
    "    config['model'] = pass_args['model']\n",
    "    config['data_module'] = data_module\n",
    "    config['accelerator'] = 'cuda' if pass_args['accelerator'] == 'gpu' else pass_args['accelerator']\n",
    "    if config['accelerator'] == 'gpu':\n",
    "        os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
    "\n",
    "# Restart kernel and re-run previous cells to avoid any issues\n",
    "# mg_og = deepcopy_mase_graph(mg)\n",
    "mg_original = deepcopy(mg.model)\n",
    "\n",
    "# Calibrate model (passing data samples to the quantizer and deciding the best amax for activations)\n",
    "mg, _ = tensorrt_fake_quantize_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "mg, _ = tensorrt_calibrate_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Conduct QAT\n",
    "mg, _ = tensorrt_fine_tune_transform_pass(mg, pass_args=tensorrt_quantize_config)\n",
    "\n",
    "# Convert and store to ONNX and then TensorRT\n",
    "mg, trt_meta = tensorrt_engine_interface_pass(mg, pass_args=tensorrt_quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\u001b[0m\n",
      "I0316 11:19:28.639614 140089548523328 analysis.py:128] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:19:28.641288 140089548523328 analysis.py:202] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-11:19:28] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73309    |\n",
      "|      Average Precision       |    0.74935    |\n",
      "|        Average Recall        |    0.73329    |\n",
      "|       Average F1 Score       |    0.73688    |\n",
      "|         Average Loss         |    0.7531     |\n",
      "|       Average Latency        |  0.18148 ms   |\n",
      "|   Average GPU Power Usage    |   64.879 W    |\n",
      "| Inference Energy Consumption | 0.0032707 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "I0316 11:19:32.303160 140089548523328 analysis.py:306] \n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73309    |\n",
      "|      Average Precision       |    0.74935    |\n",
      "|        Average Recall        |    0.73329    |\n",
      "|       Average F1 Score       |    0.73688    |\n",
      "|         Average Loss         |    0.7531     |\n",
      "|       Average Latency        |  0.18148 ms   |\n",
      "|   Average GPU Power Usage    |   64.879 W    |\n",
      "| Inference Energy Consumption | 0.0032707 mWh |\n",
      "+------------------------------+---------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:19:32.310961 140089548523328 analysis.py:202] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73168    |\n",
      "|      Average Precision       |   0.74472    |\n",
      "|        Average Recall        |   0.73037    |\n",
      "|       Average F1 Score       |   0.73369    |\n",
      "|         Average Loss         |   0.76315    |\n",
      "|       Average Latency        |  0.94611 ms  |\n",
      "|   Average GPU Power Usage    |   65.397 W   |\n",
      "| Inference Energy Consumption | 0.017187 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0316 11:19:36.270318 140089548523328 analysis.py:306] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73168    |\n",
      "|      Average Precision       |   0.74472    |\n",
      "|        Average Recall        |   0.73037    |\n",
      "|       Average F1 Score       |   0.73369    |\n",
      "|         Average Loss         |   0.76315    |\n",
      "|       Average Latency        |  0.94611 ms  |\n",
      "|   Average GPU Power Usage    |   65.397 W   |\n",
      "| Inference Energy Consumption | 0.017187 mWh |\n",
      "+------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "trt_meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy INT8 only quantization \n",
    "trt_meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_0/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(trt_meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\u001b[0m\n",
      "I0316 11:24:17.414859 140497372997440 analysis.py:128] \n",
      "TensorRT Engine Input/Output Information:\n",
      "Index | Type    | DataType | Static Shape         | Dynamic Shape        | Name\n",
      "------|---------|----------|----------------------|----------------------|-----------------------\n",
      "0     | Input   | FLOAT    | (256, 16)              | (256, 16)              | input\n",
      "1     | Output  | FLOAT    | (256, 5)               | (256, 5)               | 109\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:24:17.417420 140497372997440 analysis.py:202] Starting TensorRT transformation analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/16/2024-11:24:17] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73325    |\n",
      "|      Average Precision       |    0.74924    |\n",
      "|        Average Recall        |    0.73356    |\n",
      "|       Average F1 Score       |    0.73732    |\n",
      "|         Average Loss         |    0.75403    |\n",
      "|       Average Latency        |   0.2609 ms   |\n",
      "|   Average GPU Power Usage    |   68.244 W    |\n",
      "| Inference Energy Consumption | 0.0049457 mWh |\n",
      "+------------------------------+---------------+\u001b[0m\n",
      "I0316 11:24:23.903356 140497372997440 analysis.py:306] \n",
      "Results jsc-toy-quantized:\n",
      "+------------------------------+---------------+\n",
      "|            Metric            |     Value     |\n",
      "+------------------------------+---------------+\n",
      "|    Average Test Accuracy     |    0.73325    |\n",
      "|      Average Precision       |    0.74924    |\n",
      "|        Average Recall        |    0.73356    |\n",
      "|       Average F1 Score       |    0.73732    |\n",
      "|         Average Loss         |    0.75403    |\n",
      "|       Average Latency        |   0.2609 ms   |\n",
      "|   Average GPU Power Usage    |   68.244 W    |\n",
      "| Inference Energy Consumption | 0.0049457 mWh |\n",
      "+------------------------------+---------------+\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mStarting TensorRT transformation analysis\u001b[0m\n",
      "I0316 11:24:23.913443 140497372997440 analysis.py:202] Starting TensorRT transformation analysis\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34m\n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73071    |\n",
      "|      Average Precision       |   0.74392    |\n",
      "|        Average Recall        |   0.72935    |\n",
      "|       Average F1 Score       |   0.73267    |\n",
      "|         Average Loss         |   0.76629    |\n",
      "|       Average Latency        |  5.0403 ms   |\n",
      "|   Average GPU Power Usage    |   68.492 W   |\n",
      "| Inference Energy Consumption | 0.095894 mWh |\n",
      "+------------------------------+--------------+\u001b[0m\n",
      "I0316 11:24:32.237802 140497372997440 analysis.py:306] \n",
      "Results jsc-toy:\n",
      "+------------------------------+--------------+\n",
      "|            Metric            |    Value     |\n",
      "+------------------------------+--------------+\n",
      "|    Average Test Accuracy     |   0.73071    |\n",
      "|      Average Precision       |   0.74392    |\n",
      "|        Average Recall        |   0.72935    |\n",
      "|       Average F1 Score       |   0.73267    |\n",
      "|         Average Loss         |   0.76629    |\n",
      "|       Average Latency        |  5.0403 ms   |\n",
      "|   Average GPU Power Usage    |   68.492 W   |\n",
      "| Inference Energy Consumption | 0.095894 mWh |\n",
      "+------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# Compare original tensorrt with quantized graph\n",
    "trt_meta = {}\n",
    "from pathlib import PosixPath\n",
    "\n",
    "# JSC-Toy FP16 only quantization \n",
    "trt_meta['graph_path'] = PosixPath('/root/mase/mase_output/TensorRT/Quantization/TRT/2024_03_16/version_1/model.trt')\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(trt_meta['graph_path'], pass_args=tensorrt_analysis_config)\n",
    "\n",
    "_, _ = tensorrt_analysis_pass(mg, pass_args=tensorrt_analysis_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
