{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qizhu/miniconda3/envs/mase/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.0.1+cu118 available.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "INFO:chop:Set logging level to info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /home/qizhu/Desktop/Work/mase/mase_output/vgg7_classification_cifar10_2024-02-21/software/training_ckpts/best.ckpt\u001b[0m\n",
      "INFO:chop.tools.checkpoint_load:Loaded pytorch lightning checkpoint from /home/qizhu/Desktop/Work/mase/mase_output/vgg7_classification_cifar10_2024-02-21/software/training_ckpts/best.ckpt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch_tensorrt\n",
    "\n",
    "machop_path = Path(\".\").resolve().parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "\n",
    "from chop.passes.graph import (\n",
    "    save_node_meta_param_interface_pass,\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    ")\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model\n",
    "\n",
    "set_logging_verbosity(\"info\")\n",
    "\n",
    "# load dataset\n",
    "batch_size = 8\n",
    "model_name = \"vgg7\"\n",
    "dataset_name = \"cifar10\"\n",
    "\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# load model\n",
    "CHECKPOINT_PATH = \"/home/qizhu/Desktop/Work/mase/mase_output/vgg7_classification_cifar10_2024-02-21/software/training_ckpts/best.ckpt\"\n",
    "model_info = get_model_info(model_name)\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)\n",
    "model.eval()\n",
    "\n",
    "# create input generator\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "# a demonstration of how to feed an input value to the model\n",
    "dummy_in = next(iter(input_generator))\n",
    "_ = model(**dummy_in)\n",
    "\n",
    "# generate the mase graph and initialize node metadata\n",
    "mg = MaseGraph(model=model)\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 32, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_in['x'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Check verbose logs for the list of affected weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 11 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 4 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "WARNING: [Torch-TensorRT] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    }
   ],
   "source": [
    "testing_dataloader = data_module.test_dataloader()\n",
    "\n",
    "calibrator = torch_tensorrt.ptq.DataLoaderCalibrator(\n",
    "    testing_dataloader,\n",
    "    cache_file=\"./calibration.cache\",\n",
    "    use_cache=False,\n",
    "    algo_type=torch_tensorrt.ptq.CalibrationAlgo.ENTROPY_CALIBRATION_2,\n",
    "    device=torch.device(\"cuda:0\"),\n",
    ")\n",
    "\n",
    "trt_mod = torch_tensorrt.compile(model, inputs=[torch_tensorrt.Input((8, 3, 32, 32))],\n",
    "                                    enabled_precisions={torch.float, torch.half, torch.int8},\n",
    "                                    calibrator=calibrator,\n",
    "                                    device={\n",
    "                                         \"device_type\": torch_tensorrt.DeviceType.GPU,\n",
    "                                         \"gpu_id\": 0,\n",
    "                                         \"dla_core\": 0,\n",
    "                                         \"allow_gpu_fallback\": False,\n",
    "                                         \"disable_tf32\": False\n",
    "                                     })\n",
    "\n",
    "# optimized_model = torch.compile(model, backend=\"torch_tensorrt\")\n",
    "\n",
    "\n",
    "# trt_mod = torch_tensorrt.compile(model, inputs = [torch_tensorrt.Input((1, 16), dtype=torch.float32)],\n",
    "#     enabled_precisions = {torch.half}, # Run with FP32\n",
    "#     workspace_size = 1 << 22\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "WARNING: [Torch-TensorRT] - CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    torch_tensorrt.Input(\n",
    "        min_shape=[1, 16],\n",
    "        opt_shape=[50, 16],\n",
    "        max_shape=[100, 16],\n",
    "        dtype=torch.half,\n",
    "    )\n",
    "]\n",
    "enabled_precisions = {torch.float, torch.half}  # Run with fp16\n",
    "\n",
    "trt_ts_module = torch_tensorrt.compile(\n",
    "    model, inputs=inputs, enabled_precisions=enabled_precisions\n",
    ")\n",
    "\n",
    "input_data = torch.randn(100, 16).to('cuda')\n",
    "input_data = input_data.to(\"cuda\").half()\n",
    "result = trt_ts_module(input_data)\n",
    "torch.jit.save(trt_ts_module, \"trt_ts_module.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_model = model.half()\n",
    "half_model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.002302885055541992, 0.0005021095275878906]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "inputs = torch.randn(8, 3, 32, 32).to('cuda')\n",
    "timings = []\n",
    "\n",
    "start_time = time.time()\n",
    "features = model.to('cuda')(inputs)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "timings.append(end_time - start_time)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "features = trt_mod(inputs)\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "timings.append(end_time - start_time)\n",
    "\n",
    "timings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.model = trt_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MaseGraph' object has no attribute 'onnx_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_model\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaseGraph' object has no attribute 'onnx_model'"
     ]
    }
   ],
   "source": [
    "mg.fx_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.fx.graph'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg.fx_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
