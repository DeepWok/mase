{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mSet logging level to info\u001b[0m\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0317 19:53:44.704674 140265329104704 logger.py:44] Set logging level to info\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# # figure out the correct path\n",
    "machop_path = Path(\".\").resolve().parent.parent /\"machop\"\n",
    "assert machop_path.exists(), \"Failed to find machop at: {}\".format(machop_path)\n",
    "sys.path.append(str(machop_path))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch_tensorrt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(pytorch_quantization.__version__)\n",
    "\n",
    "from chop.dataset import MaseDataModule, get_dataset_info\n",
    "from chop.tools.logger import set_logging_verbosity\n",
    "\n",
    "from chop.tools import get_cf_args, get_dummy_input, load_config\n",
    "from chop.passes.graph import (\n",
    "    save_node_meta_param_interface_pass,\n",
    "    report_node_meta_param_analysis_pass,\n",
    "    profile_statistics_analysis_pass,\n",
    "    add_common_metadata_analysis_pass,\n",
    "    init_metadata_analysis_pass,\n",
    "    add_software_metadata_analysis_pass,\n",
    "    quantize_tensorrt_transform_pass,\n",
    "    test_quantize_tensorrt_transform_pass,\n",
    "    fake_quantize_transform_pass,\n",
    "    graph_calibration_pass,\n",
    "    evaluate_fake_quantize_pass,\n",
    "    fake_quantize_to_trt_pass\n",
    ")\n",
    "from chop.tools.get_input import InputGenerator\n",
    "from chop.tools.checkpoint_load import load_model\n",
    "from chop.ir import MaseGraph\n",
    "\n",
    "from chop.models import get_model_info, get_model, get_tokenizer\n",
    "\n",
    "set_logging_verbosity(\"info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "model_name = \"vgg7\"\n",
    "dataset_name = \"cifar10\"\n",
    "\n",
    "# batch_size = 1\n",
    "# model_name = \"facebook/opt-125m:patched\"\n",
    "# dataset_name = \"cola\"\n",
    "\n",
    "\n",
    "data_module = MaseDataModule(\n",
    "    name=dataset_name,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    num_workers=0,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded pytorch lightning checkpoint from /home/qizhu/Desktop/Work/mase/mase_output/test-accu-0.9332.ckpt\u001b[0m\n",
      "I0317 09:34:58.133078 140293346346816 checkpoint_load.py:85] Loaded pytorch lightning checkpoint from /home/qizhu/Desktop/Work/mase/mase_output/test-accu-0.9332.ckpt\n"
     ]
    }
   ],
   "source": [
    "# üìùÔ∏è change this CHECKPOINT_PATH to the one you trained in Lab1\n",
    "CHECKPOINT_PATH = \"/home/qizhu/Desktop/Work/mase/mase_output/test-accu-0.9332.ckpt\"\n",
    "# CHECKPOINT_PATH = \"/home/qizhu/Desktop/Work/mase/mase_output/opt125.ckpt\"\n",
    "\n",
    "model_info = get_model_info(model_name)\n",
    "# quant_modules.initialize()\n",
    "model = get_model(\n",
    "    model_name,\n",
    "    task=\"cls\",\n",
    "    dataset_info=data_module.dataset_info,\n",
    "    pretrained=False)\n",
    "\n",
    "model = load_model(load_name=CHECKPOINT_PATH, load_type=\"pl\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = MaseGraph(model=model)\n",
    "ori_mg = MaseGraph(model=model)\n",
    "\n",
    "# get the input generator\n",
    "input_generator = InputGenerator(\n",
    "    data_module=data_module,\n",
    "    model_info=model_info,\n",
    "    task=\"cls\",\n",
    "    which_dataloader=\"train\",\n",
    ")\n",
    "\n",
    "# a demonstration of how to feed an input value to the model\n",
    "dummy_in = next(iter(input_generator))\n",
    "# _ = model(**dummy_in)\n",
    "\n",
    "\n",
    "mg, _ = init_metadata_analysis_pass(mg, None)\n",
    "mg, _ = add_common_metadata_analysis_pass(mg, {\"dummy_in\": dummy_in})\n",
    "mg, _ = add_software_metadata_analysis_pass(mg, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execute time for one batch: 1.96ms\n",
      "Total accuracy: 90.20%\n"
     ]
    }
   ],
   "source": [
    "pass_args = {\n",
    "    \"by\": \"name\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "}\n",
    "\n",
    "mg = fake_quantize_transform_pass(mg, pass_args)\n",
    "pass_args_eval = {\n",
    "    \"data_module\": data_module,\n",
    "}\n",
    "\n",
    "mg = evaluate_fake_quantize_pass(mg, pass_args_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_args = {\n",
    "    \"by\": \"name\",\n",
    "    \"default\": {\"config\": {\"name\": None}},\n",
    "    \"feature_layers_0\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"int\",\n",
    "            \"input\": {\n",
    "                \"precesion\": 8,\n",
    "                \"calibrator\": \"max\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"calibrator\": \"histogram\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "        }\n",
    "    },    \n",
    "    \"classifier_0\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"fp16\",\n",
    "            \"input\": {\n",
    "                \"precesion\": 8,\n",
    "                \"calibrator\": \"histogram\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"calibrator\": \"max\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"classifier_1\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"fp16\",\n",
    "            \"input\": {\n",
    "                \"precesion\": 8,\n",
    "                \"calibrator\": \"histogram\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"calibrator\": \"max\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"classifier_2\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"fp16\",\n",
    "            \"input\": {\n",
    "                \"precesion\": 8,\n",
    "                \"calibrator\": \"histogram\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"calibrator\": \"max\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"classifier_3\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"fp16\",\n",
    "            \"input\": {\n",
    "                \"precesion\": 8,\n",
    "                \"calibrator\": \"histogram\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "            \"weight\": {\n",
    "                \"calibrator\": \"max\",\n",
    "                \"quantize_axis\": None,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "mg = fake_quantize_transform_pass(mg, pass_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_args_calibrate = {\n",
    "    \"calibrator\": \"percentile\",\n",
    "    \"percentiles\": [99],\n",
    "    \"data_module\": data_module,\n",
    "    \"num_batches\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for node in mg.fx_graph.nodes:\n",
    "    print(node.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgraph_calibration_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpass_args_calibrate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pass_args_eval \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_module\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_module,\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      6\u001b[0m mg \u001b[38;5;241m=\u001b[39m evaluate_fake_quantize_pass(mg, pass_args_eval)\n",
      "File \u001b[0;32m~/Desktop/Work/mase/machop/chop/passes/graph/transforms/quantize_tensorRT/calibrator.py:77\u001b[0m, in \u001b[0;36mgraph_calibration_pass\u001b[0;34m(graph, pass_args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# quant_modules.initialize()\u001b[39;00m\n\u001b[1;32m     75\u001b[0m graph\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 77\u001b[0m \u001b[43mcollect_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_module\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_batches\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pass_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalibrator\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpercentile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m percentile \u001b[38;5;129;01min\u001b[39;00m pass_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentiles\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Work/mase/machop/chop/passes/graph/transforms/quantize_tensorRT/calibrator.py:57\u001b[0m, in \u001b[0;36mcollect_stats\u001b[0;34m(model, data_loader, num_batches)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxTrain\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# turn off calibration tool\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m~/miniconda3/envs/mase/lib/python3.10/site-packages/torch/fx/graph_module.py:662\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mase/lib/python3.10/site-packages/torch/fx/graph_module.py:281\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/mase/lib/python3.10/site-packages/torch/fx/graph_module.py:271\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/mase/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.0:27\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m feature_layers_20 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_layers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20\u001b[39m\u001b[38;5;124m\"\u001b[39m)(feature_layers_19);  feature_layers_19 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     26\u001b[0m view \u001b[38;5;241m=\u001b[39m feature_layers_20\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8192\u001b[39m);  feature_layers_20 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m classifier_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview\u001b[49m\u001b[43m)\u001b[49m;  view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     28\u001b[0m classifier_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)(classifier_0);  classifier_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m classifier_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m)(classifier_1);  classifier_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mase/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mase/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "graph_calibration_pass(mg,  pass_args_calibrate)\n",
    "pass_args_eval = {\n",
    "    \"data_module\": data_module,\n",
    "}\n",
    "\n",
    "mg = evaluate_fake_quantize_pass(mg, pass_args_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (feature_layers): Module(\n",
       "    (0): QuantConv2d(\n",
       "      3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=4.9597 calibrator=MaxCalibrator scale=1.0 quant)\n",
       "      (_weight_quantizer): TensorQuantizer(8bit fake per-tensor amax=0.2797 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    )\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Module(\n",
       "    (0): QuantLinear(\n",
       "      in_features=8192, out_features=1024, bias=True\n",
       "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.2718 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "      (_weight_quantizer): TensorQuantizer(8bit fake per-tensor amax=0.4703 calibrator=MaxCalibrator scale=1.0 quant)\n",
       "    )\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): QuantLinear(\n",
       "      in_features=1024, out_features=1024, bias=True\n",
       "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=9.5105 calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "      (_weight_quantizer): TensorQuantizer(8bit fake per-tensor amax=0.3434 calibrator=MaxCalibrator scale=1.0 quant)\n",
       "    )\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (last_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qizhu/miniconda3/envs/mase/lib/python3.10/site-packages/pytorch_quantization-2.1.3-py3.10-linux-x86_64.egg/pytorch_quantization/tensor_quant.py:378: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "/home/qizhu/miniconda3/envs/mase/lib/python3.10/site-packages/pytorch_quantization-2.1.3-py3.10-linux-x86_64.egg/pytorch_quantization/tensor_quant.py:381: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "/home/qizhu/miniconda3/envs/mase/lib/python3.10/site-packages/pytorch_quantization-2.1.3-py3.10-linux-x86_64.egg/pytorch_quantization/tensor_quant.py:391: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n",
      "/home/qizhu/miniconda3/envs/mase/lib/python3.10/site-packages/pytorch_quantization-2.1.3-py3.10-linux-x86_64.egg/pytorch_quantization/tensor_quant.py:397: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Succeeded finding ONNX file!\n",
      "Succeeded parsing .onnx file!\n",
      "Succeeded building engine!\n",
      "[ 0]Input -> DataType.FLOAT (-1, 3, 32, 32) (8, 3, 32, 32) input\n",
      "[ 1]Output-> DataType.FLOAT (-1, 10) (8, 10) output\n",
      "Succeeded running model in TensorRT!\n",
      "Average execute time for one batch: 0.09ms\n",
      "Total accuracy: 86.37%\n"
     ]
    }
   ],
   "source": [
    "pass_args = {\n",
    "    \"onnxFile\": \"onnx_test.onnx\",\n",
    "    \"engineFile\": \"engine_test.plan\",\n",
    "    \"dataloader\": data_module.test_dataloader,\n",
    "}\n",
    "mg = fake_quantize_to_trt_pass(mg, pass_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execute time for one batch: 1.13ms\n",
      "Total accuracy: 90.11%\n"
     ]
    }
   ],
   "source": [
    "pass_args_eval = {\n",
    "    \"data_module\": data_module,\n",
    "}\n",
    "\n",
    "mg = evaluate_fake_quantize_pass(mg, pass_args_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "feature_layers_0\n",
      "feature_layers_1\n",
      "feature_layers_2\n",
      "feature_layers_3\n",
      "feature_layers_4\n",
      "feature_layers_5\n",
      "feature_layers_6\n",
      "feature_layers_7\n",
      "feature_layers_8\n",
      "feature_layers_9\n",
      "feature_layers_10\n",
      "feature_layers_11\n",
      "feature_layers_12\n",
      "feature_layers_13\n",
      "feature_layers_14\n",
      "feature_layers_15\n",
      "feature_layers_16\n",
      "feature_layers_17\n",
      "feature_layers_18\n",
      "feature_layers_19\n",
      "feature_layers_20\n",
      "view\n",
      "classifier_0\n",
      "classifier_1\n",
      "classifier_2\n",
      "classifier_3\n",
      "last_layer\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "for node in mg.fx_graph.nodes:\n",
    "    print(node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPT-125M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qizhu/miniconda3/envs/mase/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data\n"
     ]
    }
   ],
   "source": [
    "wikitext_info = get_dataset_info(\"wikitext2\")\n",
    "opt = get_model(\n",
    "    \"facebook/opt-125m:patched\",\n",
    "    task=\"lm\",\n",
    "    dataset_info=wikitext_info,\n",
    "    pretrained=True,\n",
    ")\n",
    "opt_tokenizer = get_tokenizer(\"facebook/opt-125m:patched\")\n",
    "\n",
    "print(f\"prepare data\")\n",
    "# Get data module for dummy inputs\n",
    "data_module = MaseDataModule(\n",
    "    name=\"wikitext2\",\n",
    "    batch_size=2,\n",
    "    num_workers=os.cpu_count(),\n",
    "    max_token_len=128,\n",
    "    tokenizer=opt_tokenizer,\n",
    "    load_from_cache_file=True,\n",
    "    model_name=\"facebook/opt-125m@patched\",\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = get_model_info(\"facebook/opt-125m:patched\")\n",
    "cf_args = get_cf_args(model_info=model_info, task=\"lm\", model=opt)\n",
    "\n",
    "mg = MaseGraph(model=opt, cf_args=cf_args)\n",
    "\n",
    "dummy_in = get_dummy_input(model_info, data_module=data_module, task=\"lm\")\n",
    "if len(mg.model.additional_inputs) > 0:\n",
    "    dummy_in = dummy_in | mg.model.additional_inputs\n",
    "\n",
    "# Generate graph and initialize metadata\n",
    "# print(f\"init metadata\")\n",
    "mg, _ = init_metadata_analysis_pass(mg, pass_args=None)\n",
    "# mg, _ = add_common_metadata_analysis_pass(mg, pass_args={\"dummy_in\": dummy_in})\n",
    "# mg, _ = add_software_metadata_analysis_pass(mg, None)\n",
    "\n",
    "# for node in mg.fx_graph.nodes:\n",
    "#     print(node.meta['mase'].module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(..., device='meta', size=(1, 128), dtype=torch.int64)\n",
      "tensor(..., device='meta', size=(1, 128), dtype=torch.int64)\n",
      "None\n",
      "None\n",
      "None\n",
      "tensor(..., device='meta', size=(1, 128), dtype=torch.int64)\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for node in mg.fx_graph.nodes:\n",
    "    args, kwargs = None, None\n",
    "    if node.op == \"placeholder\":\n",
    "        result = dummy_in[node.name]\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "placeholder\n",
      "placeholder\n",
      "placeholder\n",
      "call_function\n",
      "placeholder\n",
      "call_function\n",
      "placeholder\n",
      "call_function\n",
      "placeholder\n",
      "placeholder\n",
      "call_function\n",
      "call_function\n",
      "placeholder\n",
      "call_function\n",
      "call_function\n",
      "placeholder\n",
      "call_function\n",
      "call_function\n",
      "placeholder\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_module\n",
      "call_function\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_module\n",
      "call_function\n",
      "call_function\n",
      "call_function\n",
      "call_method\n",
      "call_module\n",
      "call_module\n",
      "call_method\n",
      "call_function\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "for node in mg.fx_graph.nodes:\n",
    "    # if node.op == \"call_module\":\n",
    "    print(node.op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 took 0.29590582847595215 seconds\n",
      "Batch 1 took 0.3456096649169922 seconds\n",
      "Batch 2 took 0.2067277431488037 seconds\n",
      "Batch 3 took 0.23676538467407227 seconds\n",
      "Batch 4 took 0.2209935188293457 seconds\n",
      "Batch 5 took 0.19042754173278809 seconds\n",
      "Batch 6 took 0.16930031776428223 seconds\n",
      "Batch 7 took 0.2097775936126709 seconds\n",
      "Batch 8 took 0.15608453750610352 seconds\n",
      "Batch 9 took 0.17119336128234863 seconds\n",
      "Batch 10 took 0.15328526496887207 seconds\n",
      "Batch 11 took 0.1912400722503662 seconds\n",
      "Batch 12 took 0.20645642280578613 seconds\n",
      "Batch 13 took 0.15901398658752441 seconds\n",
      "Batch 14 took 0.16031408309936523 seconds\n",
      "Batch 15 took 0.19242572784423828 seconds\n",
      "Batch 16 took 0.1795520782470703 seconds\n",
      "Batch 17 took 0.16906023025512695 seconds\n",
      "Batch 18 took 0.1770777702331543 seconds\n",
      "Batch 19 took 0.18641257286071777 seconds\n",
      "Batch 20 took 0.15935230255126953 seconds\n",
      "Batch 21 took 0.15850019454956055 seconds\n",
      "Batch 22 took 0.16030383110046387 seconds\n",
      "Batch 23 took 0.15561127662658691 seconds\n",
      "Batch 24 took 0.16117143630981445 seconds\n",
      "Batch 25 took 0.1559598445892334 seconds\n",
      "Batch 26 took 0.15922808647155762 seconds\n",
      "Batch 27 took 0.15452337265014648 seconds\n",
      "Batch 28 took 0.15856051445007324 seconds\n",
      "Batch 29 took 0.15594816207885742 seconds\n",
      "Batch 30 took 0.1556391716003418 seconds\n",
      "Batch 31 took 0.16843771934509277 seconds\n",
      "Batch 32 took 0.17085981369018555 seconds\n",
      "Batch 33 took 0.15386104583740234 seconds\n",
      "Batch 34 took 0.15723657608032227 seconds\n",
      "Batch 35 took 0.1543433666229248 seconds\n",
      "Batch 36 took 0.15638947486877441 seconds\n",
      "Batch 37 took 0.1538240909576416 seconds\n",
      "Batch 38 took 0.15732073783874512 seconds\n",
      "Batch 39 took 0.15383458137512207 seconds\n",
      "Batch 40 took 0.15702462196350098 seconds\n",
      "Batch 41 took 0.15749144554138184 seconds\n",
      "Batch 42 took 0.15719294548034668 seconds\n",
      "Batch 43 took 0.15658044815063477 seconds\n",
      "Batch 44 took 0.17191457748413086 seconds\n",
      "Batch 45 took 0.22964072227478027 seconds\n",
      "Batch 46 took 0.1761157512664795 seconds\n",
      "Batch 47 took 0.15763282775878906 seconds\n",
      "Batch 48 took 0.15366435050964355 seconds\n",
      "Batch 49 took 0.15616393089294434 seconds\n",
      "Batch 50 took 0.15628337860107422 seconds\n",
      "Batch 51 took 0.15669584274291992 seconds\n",
      "Batch 52 took 0.1548323631286621 seconds\n",
      "Batch 53 took 0.1571671962738037 seconds\n",
      "Batch 54 took 0.15642786026000977 seconds\n",
      "Batch 55 took 0.15509700775146484 seconds\n",
      "Batch 56 took 0.15533661842346191 seconds\n",
      "Batch 57 took 0.18503665924072266 seconds\n",
      "Batch 58 took 0.1570432186126709 seconds\n",
      "Batch 59 took 0.1544651985168457 seconds\n",
      "Batch 60 took 0.1583237648010254 seconds\n",
      "Batch 61 took 0.1554412841796875 seconds\n",
      "Batch 62 took 0.15428638458251953 seconds\n",
      "Batch 63 took 0.15967679023742676 seconds\n",
      "Batch 64 took 0.15311813354492188 seconds\n",
      "Batch 65 took 0.15574383735656738 seconds\n",
      "Batch 66 took 0.15623116493225098 seconds\n",
      "Batch 67 took 0.1579596996307373 seconds\n",
      "Batch 68 took 0.15395140647888184 seconds\n",
      "Batch 69 took 0.15756940841674805 seconds\n",
      "Batch 70 took 0.18265080451965332 seconds\n",
      "Batch 71 took 0.16070914268493652 seconds\n",
      "Batch 72 took 0.15419745445251465 seconds\n",
      "Batch 73 took 0.15494465827941895 seconds\n",
      "Batch 74 took 0.15413117408752441 seconds\n",
      "Batch 75 took 0.157606840133667 seconds\n",
      "Batch 76 took 0.1540968418121338 seconds\n",
      "Batch 77 took 0.15814900398254395 seconds\n",
      "Batch 78 took 0.15189075469970703 seconds\n",
      "Batch 79 took 0.15827727317810059 seconds\n",
      "Batch 80 took 0.15543127059936523 seconds\n",
      "Batch 81 took 0.15807533264160156 seconds\n",
      "Batch 82 took 0.1540844440460205 seconds\n",
      "Batch 83 took 0.1884777545928955 seconds\n",
      "Batch 84 took 0.18477487564086914 seconds\n",
      "Batch 85 took 0.16820049285888672 seconds\n",
      "Batch 86 took 0.15644335746765137 seconds\n",
      "Batch 87 took 0.15570616722106934 seconds\n",
      "Batch 88 took 0.15344738960266113 seconds\n",
      "Batch 89 took 0.15479660034179688 seconds\n",
      "Batch 90 took 0.1738882064819336 seconds\n",
      "Batch 91 took 0.15718650817871094 seconds\n",
      "Batch 92 took 0.15520215034484863 seconds\n",
      "Batch 93 took 0.15852093696594238 seconds\n",
      "Batch 94 took 0.1524045467376709 seconds\n",
      "Batch 95 took 0.16186189651489258 seconds\n",
      "Batch 96 took 0.18124604225158691 seconds\n",
      "Batch 97 took 0.16112375259399414 seconds\n",
      "Batch 98 took 0.1540515422821045 seconds\n",
      "Batch 99 took 0.15607833862304688 seconds\n",
      "Batch 100 took 0.15518474578857422 seconds\n",
      "Batch 101 took 0.15672659873962402 seconds\n",
      "Batch 102 took 0.1556718349456787 seconds\n",
      "Batch 103 took 0.1560680866241455 seconds\n",
      "Batch 104 took 0.15542078018188477 seconds\n",
      "Batch 105 took 0.15678000450134277 seconds\n",
      "Batch 106 took 0.15446853637695312 seconds\n",
      "Batch 107 took 0.15668678283691406 seconds\n",
      "Batch 108 took 0.1585080623626709 seconds\n",
      "Batch 109 took 0.22517657279968262 seconds\n",
      "Batch 110 took 0.179793119430542 seconds\n",
      "Batch 111 took 0.15618133544921875 seconds\n",
      "Batch 112 took 0.15631818771362305 seconds\n",
      "Batch 113 took 0.2020256519317627 seconds\n",
      "Batch 114 took 0.15542292594909668 seconds\n",
      "Batch 115 took 0.16010403633117676 seconds\n",
      "Batch 116 took 0.15694403648376465 seconds\n",
      "Batch 117 took 0.1587541103363037 seconds\n",
      "Batch 118 took 0.1632068157196045 seconds\n",
      "Batch 119 took 0.17492389678955078 seconds\n",
      "Batch 120 took 0.1691882610321045 seconds\n",
      "Batch 121 took 0.17824244499206543 seconds\n",
      "Batch 122 took 0.16388487815856934 seconds\n",
      "Batch 123 took 0.15706944465637207 seconds\n",
      "Batch 124 took 0.15393757820129395 seconds\n",
      "Batch 125 took 0.15851783752441406 seconds\n",
      "Batch 126 took 0.15359926223754883 seconds\n",
      "Batch 127 took 0.15922284126281738 seconds\n",
      "Batch 128 took 0.1535496711730957 seconds\n",
      "Batch 129 took 0.15839767456054688 seconds\n",
      "Batch 130 took 0.1579585075378418 seconds\n",
      "Batch 131 took 0.16251015663146973 seconds\n",
      "Batch 132 took 0.15651249885559082 seconds\n",
      "Batch 133 took 0.16071867942810059 seconds\n",
      "Batch 134 took 0.1702120304107666 seconds\n",
      "Batch 135 took 0.16974782943725586 seconds\n",
      "Batch 136 took 0.15611672401428223 seconds\n",
      "Batch 137 took 0.15814590454101562 seconds\n",
      "Batch 138 took 0.15453839302062988 seconds\n",
      "Batch 139 took 0.16016054153442383 seconds\n",
      "Batch 140 took 0.15763497352600098 seconds\n",
      "Batch 141 took 0.15800237655639648 seconds\n",
      "Batch 142 took 0.15530991554260254 seconds\n",
      "Batch 143 took 0.1582348346710205 seconds\n",
      "Batch 144 took 0.15417695045471191 seconds\n",
      "Batch 145 took 0.15696501731872559 seconds\n",
      "Batch 146 took 0.162064790725708 seconds\n",
      "Batch 147 took 0.2290496826171875 seconds\n",
      "Batch 148 took 0.1636216640472412 seconds\n",
      "Batch 149 took 0.15541625022888184 seconds\n",
      "Batch 150 took 0.1581110954284668 seconds\n",
      "Batch 151 took 0.1635127067565918 seconds\n",
      "Batch 152 took 0.15486717224121094 seconds\n",
      "Batch 153 took 0.15699005126953125 seconds\n",
      "Batch 154 took 0.1590437889099121 seconds\n",
      "Batch 155 took 0.1585233211517334 seconds\n",
      "Batch 156 took 0.15383362770080566 seconds\n",
      "Batch 157 took 0.17273640632629395 seconds\n",
      "Batch 158 took 0.15372872352600098 seconds\n",
      "Batch 159 took 0.16889357566833496 seconds\n",
      "Batch 160 took 0.19287729263305664 seconds\n",
      "Batch 161 took 0.22113585472106934 seconds\n",
      "Batch 162 took 0.15158510208129883 seconds\n",
      "Batch 163 took 0.1573193073272705 seconds\n",
      "Batch 164 took 0.15282440185546875 seconds\n",
      "Batch 165 took 0.15733647346496582 seconds\n",
      "Batch 166 took 0.15381169319152832 seconds\n",
      "Batch 167 took 0.15452027320861816 seconds\n",
      "Batch 168 took 0.15415358543395996 seconds\n",
      "Batch 169 took 0.15962982177734375 seconds\n",
      "Batch 170 took 0.1535196304321289 seconds\n",
      "Batch 171 took 0.16855573654174805 seconds\n",
      "Batch 172 took 0.1711750030517578 seconds\n",
      "Batch 173 took 0.21151518821716309 seconds\n",
      "Batch 174 took 0.16759324073791504 seconds\n",
      "Batch 175 took 0.15985941886901855 seconds\n",
      "Batch 176 took 0.15393686294555664 seconds\n",
      "Batch 177 took 0.1578381061553955 seconds\n",
      "Batch 178 took 0.16778230667114258 seconds\n",
      "Batch 179 took 0.16026878356933594 seconds\n",
      "Batch 180 took 0.15326738357543945 seconds\n",
      "Batch 181 took 0.1579124927520752 seconds\n",
      "Batch 182 took 0.15595412254333496 seconds\n",
      "Batch 183 took 0.15847373008728027 seconds\n",
      "Batch 184 took 0.1646256446838379 seconds\n",
      "Batch 185 took 0.18195533752441406 seconds\n",
      "Batch 186 took 0.1539301872253418 seconds\n",
      "Batch 187 took 0.15633368492126465 seconds\n",
      "Batch 188 took 0.15628671646118164 seconds\n",
      "Batch 189 took 0.16142058372497559 seconds\n",
      "Batch 190 took 0.16984224319458008 seconds\n",
      "Batch 191 took 0.19448447227478027 seconds\n",
      "Batch 192 took 0.15189576148986816 seconds\n",
      "Batch 193 took 0.17156457901000977 seconds\n",
      "Batch 194 took 0.15276670455932617 seconds\n",
      "Batch 195 took 0.15796279907226562 seconds\n",
      "Batch 196 took 0.16516590118408203 seconds\n",
      "Batch 197 took 0.17650318145751953 seconds\n",
      "Batch 198 took 0.17122149467468262 seconds\n",
      "Batch 199 took 0.16026782989501953 seconds\n",
      "Batch 200 took 0.1517343521118164 seconds\n",
      "Batch 201 took 0.15893173217773438 seconds\n",
      "Batch 202 took 0.1515510082244873 seconds\n",
      "Batch 203 took 0.15789389610290527 seconds\n",
      "Batch 204 took 0.1554856300354004 seconds\n",
      "Batch 205 took 0.15571808815002441 seconds\n",
      "Batch 206 took 0.15809893608093262 seconds\n",
      "Batch 207 took 0.15424585342407227 seconds\n",
      "Batch 208 took 0.15448665618896484 seconds\n",
      "Batch 209 took 0.20134568214416504 seconds\n",
      "Batch 210 took 0.22313928604125977 seconds\n",
      "Batch 211 took 0.1765146255493164 seconds\n",
      "Batch 212 took 0.15424799919128418 seconds\n",
      "Batch 213 took 0.15747427940368652 seconds\n",
      "Batch 214 took 0.15518736839294434 seconds\n",
      "Batch 215 took 0.15460777282714844 seconds\n",
      "Batch 216 took 0.15414810180664062 seconds\n",
      "Batch 217 took 0.15537524223327637 seconds\n",
      "Batch 218 took 0.15740084648132324 seconds\n",
      "Batch 219 took 0.15658926963806152 seconds\n",
      "Batch 220 took 0.15577912330627441 seconds\n",
      "Batch 221 took 0.16867399215698242 seconds\n",
      "Batch 222 took 0.16233253479003906 seconds\n",
      "Batch 223 took 0.1741185188293457 seconds\n",
      "Batch 224 took 0.15453696250915527 seconds\n",
      "Batch 225 took 0.1541900634765625 seconds\n",
      "Batch 226 took 0.15501189231872559 seconds\n",
      "Batch 227 took 0.17183995246887207 seconds\n",
      "Batch 228 took 0.1554698944091797 seconds\n",
      "Batch 229 took 0.15530753135681152 seconds\n",
      "Batch 230 took 0.16557025909423828 seconds\n",
      "Batch 231 took 0.15444350242614746 seconds\n",
      "Batch 232 took 0.15551137924194336 seconds\n",
      "Batch 233 took 0.15517759323120117 seconds\n",
      "Batch 234 took 0.16903424263000488 seconds\n",
      "Batch 235 took 0.16684174537658691 seconds\n",
      "Batch 236 took 0.16991209983825684 seconds\n",
      "Batch 237 took 0.1759960651397705 seconds\n",
      "Batch 238 took 0.15583443641662598 seconds\n",
      "Batch 239 took 0.15498733520507812 seconds\n",
      "Batch 240 took 0.15546655654907227 seconds\n",
      "Batch 241 took 0.15276741981506348 seconds\n",
      "Batch 242 took 0.15572762489318848 seconds\n",
      "Batch 243 took 0.15286540985107422 seconds\n",
      "Batch 244 took 0.15839910507202148 seconds\n",
      "Batch 245 took 0.15502262115478516 seconds\n",
      "Batch 246 took 0.15562868118286133 seconds\n",
      "Batch 247 took 0.16831564903259277 seconds\n",
      "Batch 248 took 0.16860270500183105 seconds\n",
      "Batch 249 took 0.1884157657623291 seconds\n",
      "Batch 250 took 0.15822196006774902 seconds\n",
      "Batch 251 took 0.15345168113708496 seconds\n",
      "Batch 252 took 0.15848755836486816 seconds\n",
      "Batch 253 took 0.15585947036743164 seconds\n",
      "Batch 254 took 0.15628504753112793 seconds\n",
      "Batch 255 took 0.15418052673339844 seconds\n",
      "Batch 256 took 0.1544177532196045 seconds\n",
      "Batch 257 took 0.15523672103881836 seconds\n",
      "Batch 258 took 0.15432071685791016 seconds\n",
      "Batch 259 took 0.17607402801513672 seconds\n",
      "Batch 260 took 0.17525815963745117 seconds\n",
      "Batch 261 took 0.1789867877960205 seconds\n",
      "Batch 262 took 0.20519590377807617 seconds\n",
      "Batch 263 took 0.15587830543518066 seconds\n",
      "Batch 264 took 0.157958984375 seconds\n",
      "Batch 265 took 0.15580415725708008 seconds\n",
      "Batch 266 took 0.1565401554107666 seconds\n",
      "Batch 267 took 0.16672825813293457 seconds\n",
      "Batch 268 took 0.17301559448242188 seconds\n",
      "Batch 269 took 0.17494916915893555 seconds\n",
      "Batch 270 took 0.15671753883361816 seconds\n",
      "Batch 271 took 0.16223692893981934 seconds\n",
      "Batch 272 took 0.164933443069458 seconds\n",
      "Batch 273 took 0.1687638759613037 seconds\n",
      "Batch 274 took 0.2122178077697754 seconds\n",
      "Batch 275 took 0.15329527854919434 seconds\n",
      "Batch 276 took 0.1571967601776123 seconds\n",
      "Batch 277 took 0.1521611213684082 seconds\n",
      "Batch 278 took 0.1595304012298584 seconds\n",
      "Batch 279 took 0.1550436019897461 seconds\n",
      "Batch 280 took 0.1569206714630127 seconds\n",
      "Batch 281 took 0.15669703483581543 seconds\n",
      "Batch 282 took 0.15579724311828613 seconds\n",
      "Batch 283 took 0.15557384490966797 seconds\n",
      "Batch 284 took 0.16317200660705566 seconds\n",
      "Batch 285 took 0.15617632865905762 seconds\n",
      "Batch 286 took 0.17611932754516602 seconds\n",
      "Batch 287 took 0.1604597568511963 seconds\n",
      "Batch 288 took 0.15844964981079102 seconds\n",
      "Batch 289 took 0.15641307830810547 seconds\n",
      "Batch 290 took 0.1547718048095703 seconds\n",
      "Batch 291 took 0.15706896781921387 seconds\n",
      "Batch 292 took 0.1540083885192871 seconds\n",
      "Batch 293 took 0.15585708618164062 seconds\n",
      "Batch 294 took 0.15588760375976562 seconds\n",
      "Batch 295 took 0.15798020362854004 seconds\n",
      "Batch 296 took 0.18050575256347656 seconds\n",
      "Batch 297 took 0.18625855445861816 seconds\n",
      "Batch 298 took 0.1576368808746338 seconds\n",
      "Batch 299 took 0.18358826637268066 seconds\n",
      "Batch 300 took 0.15659475326538086 seconds\n",
      "Batch 301 took 0.15607976913452148 seconds\n",
      "Batch 302 took 0.1540684700012207 seconds\n",
      "Batch 303 took 0.16855788230895996 seconds\n",
      "Batch 304 took 0.16374897956848145 seconds\n",
      "Batch 305 took 0.1546475887298584 seconds\n",
      "Batch 306 took 0.15511608123779297 seconds\n",
      "Batch 307 took 0.15650057792663574 seconds\n",
      "Batch 308 took 0.15563344955444336 seconds\n",
      "Batch 309 took 0.158097505569458 seconds\n",
      "Batch 310 took 0.16396641731262207 seconds\n",
      "Batch 311 took 0.1574547290802002 seconds\n",
      "Batch 312 took 0.18451905250549316 seconds\n",
      "Batch 313 took 0.15967082977294922 seconds\n",
      "Batch 314 took 0.1555182933807373 seconds\n",
      "Batch 315 took 0.1564478874206543 seconds\n",
      "Batch 316 took 0.1549983024597168 seconds\n",
      "Batch 317 took 0.15662288665771484 seconds\n",
      "Batch 318 took 0.15374445915222168 seconds\n",
      "Batch 319 took 0.17140626907348633 seconds\n",
      "Batch 320 took 0.15831613540649414 seconds\n",
      "Batch 321 took 0.15550875663757324 seconds\n",
      "Batch 322 took 0.15942788124084473 seconds\n",
      "Batch 323 took 0.15997576713562012 seconds\n",
      "Batch 324 took 0.16218256950378418 seconds\n",
      "Batch 325 took 0.17794060707092285 seconds\n",
      "Batch 326 took 0.1577005386352539 seconds\n",
      "Batch 327 took 0.15757393836975098 seconds\n",
      "Batch 328 took 0.15820884704589844 seconds\n",
      "Batch 329 took 0.15724873542785645 seconds\n",
      "Batch 330 took 0.15358829498291016 seconds\n",
      "Batch 331 took 0.15616631507873535 seconds\n",
      "Batch 332 took 0.15732979774475098 seconds\n",
      "Batch 333 took 0.1609959602355957 seconds\n",
      "Batch 334 took 0.20360779762268066 seconds\n",
      "Batch 335 took 0.21980953216552734 seconds\n",
      "Batch 336 took 0.19180583953857422 seconds\n",
      "Batch 337 took 0.1804332733154297 seconds\n",
      "Batch 338 took 0.15783405303955078 seconds\n",
      "Batch 339 took 0.15597176551818848 seconds\n",
      "Batch 340 took 0.1687633991241455 seconds\n",
      "Batch 341 took 0.15705347061157227 seconds\n",
      "Batch 342 took 0.15430426597595215 seconds\n",
      "Batch 343 took 0.22179508209228516 seconds\n",
      "Batch 344 took 0.17823100090026855 seconds\n",
      "Batch 345 took 0.15652966499328613 seconds\n",
      "Batch 346 took 0.1602027416229248 seconds\n",
      "Batch 347 took 0.23607230186462402 seconds\n",
      "Batch 348 took 0.19381308555603027 seconds\n",
      "Batch 349 took 0.23737788200378418 seconds\n",
      "Batch 350 took 0.21714186668395996 seconds\n",
      "Batch 351 took 0.1951463222503662 seconds\n",
      "Batch 352 took 0.15590906143188477 seconds\n",
      "Batch 353 took 0.15546751022338867 seconds\n",
      "Batch 354 took 0.1652841567993164 seconds\n",
      "Batch 355 took 0.15305113792419434 seconds\n",
      "Batch 356 took 0.17186665534973145 seconds\n",
      "Batch 357 took 0.18607115745544434 seconds\n",
      "Batch 358 took 0.18900084495544434 seconds\n",
      "Batch 359 took 0.18349790573120117 seconds\n",
      "Batch 360 took 0.2004256248474121 seconds\n",
      "Batch 361 took 0.237518310546875 seconds\n",
      "Batch 362 took 0.22431302070617676 seconds\n",
      "Batch 363 took 0.22262907028198242 seconds\n",
      "Batch 364 took 0.16270017623901367 seconds\n",
      "Batch 365 took 0.15666675567626953 seconds\n",
      "Batch 366 took 0.1527388095855713 seconds\n",
      "Batch 367 took 0.18788695335388184 seconds\n",
      "Batch 368 took 0.222367525100708 seconds\n",
      "Batch 369 took 0.16758370399475098 seconds\n",
      "Batch 370 took 0.21906638145446777 seconds\n",
      "Batch 371 took 0.18700814247131348 seconds\n",
      "Batch 372 took 0.16035127639770508 seconds\n",
      "Batch 373 took 0.1599290370941162 seconds\n",
      "Batch 374 took 0.1563854217529297 seconds\n",
      "Batch 375 took 0.15451741218566895 seconds\n",
      "Batch 376 took 0.15658330917358398 seconds\n",
      "Batch 377 took 0.15451860427856445 seconds\n",
      "Batch 378 took 0.15644502639770508 seconds\n",
      "Batch 379 took 0.15516352653503418 seconds\n",
      "Batch 380 took 0.1665174961090088 seconds\n",
      "Batch 381 took 0.15784978866577148 seconds\n",
      "Batch 382 took 0.15598487854003906 seconds\n",
      "Batch 383 took 0.18265390396118164 seconds\n",
      "Batch 384 took 0.15780043601989746 seconds\n",
      "Batch 385 took 0.15639567375183105 seconds\n",
      "Batch 386 took 0.18477892875671387 seconds\n",
      "Batch 387 took 0.21427583694458008 seconds\n",
      "Batch 388 took 0.21400690078735352 seconds\n",
      "Batch 389 took 0.15465807914733887 seconds\n",
      "Batch 390 took 0.15874099731445312 seconds\n",
      "Batch 391 took 0.154188871383667 seconds\n",
      "Batch 392 took 0.1650552749633789 seconds\n",
      "Batch 393 took 0.16522693634033203 seconds\n",
      "Batch 394 took 0.15999460220336914 seconds\n",
      "Batch 395 took 0.18450927734375 seconds\n",
      "Batch 396 took 0.15699386596679688 seconds\n",
      "Batch 397 took 0.15345239639282227 seconds\n",
      "Batch 398 took 0.1566905975341797 seconds\n",
      "Batch 399 took 0.22457218170166016 seconds\n",
      "Batch 400 took 0.19231796264648438 seconds\n",
      "Batch 401 took 0.155958890914917 seconds\n",
      "Batch 402 took 0.15382051467895508 seconds\n",
      "Batch 403 took 0.15727734565734863 seconds\n",
      "Batch 404 took 0.15722060203552246 seconds\n",
      "Batch 405 took 0.16539335250854492 seconds\n",
      "Batch 406 took 0.15558648109436035 seconds\n",
      "Batch 407 took 0.1868889331817627 seconds\n",
      "Batch 408 took 0.22594785690307617 seconds\n",
      "Batch 409 took 0.22246122360229492 seconds\n",
      "Batch 410 took 0.16990208625793457 seconds\n",
      "Batch 411 took 0.16966700553894043 seconds\n",
      "Batch 412 took 0.16845178604125977 seconds\n",
      "Batch 413 took 0.17163896560668945 seconds\n",
      "Batch 414 took 0.16296005249023438 seconds\n",
      "Batch 415 took 0.16785335540771484 seconds\n",
      "Batch 416 took 0.17007040977478027 seconds\n",
      "Batch 417 took 0.16440391540527344 seconds\n",
      "Batch 418 took 0.1639692783355713 seconds\n",
      "Batch 419 took 0.18026185035705566 seconds\n",
      "Batch 420 took 0.15261578559875488 seconds\n",
      "Batch 421 took 0.1566169261932373 seconds\n",
      "Batch 422 took 0.15597224235534668 seconds\n",
      "Batch 423 took 0.17176222801208496 seconds\n",
      "Batch 424 took 0.15101170539855957 seconds\n",
      "Batch 425 took 0.15733981132507324 seconds\n",
      "Batch 426 took 0.1524977684020996 seconds\n",
      "Batch 427 took 0.1557309627532959 seconds\n",
      "Batch 428 took 0.17606067657470703 seconds\n",
      "Batch 429 took 0.1831340789794922 seconds\n",
      "Batch 430 took 0.16904640197753906 seconds\n",
      "Batch 431 took 0.17231225967407227 seconds\n",
      "Batch 432 took 0.16349101066589355 seconds\n",
      "Batch 433 took 0.15673136711120605 seconds\n",
      "Batch 434 took 0.15119576454162598 seconds\n",
      "Batch 435 took 0.15900564193725586 seconds\n",
      "Batch 436 took 0.1514906883239746 seconds\n",
      "Batch 437 took 0.1587810516357422 seconds\n",
      "Batch 438 took 0.15381574630737305 seconds\n",
      "Batch 439 took 0.15711283683776855 seconds\n",
      "Batch 440 took 0.15555405616760254 seconds\n",
      "Batch 441 took 0.15674281120300293 seconds\n",
      "Batch 442 took 0.16660547256469727 seconds\n",
      "Batch 443 took 0.15662670135498047 seconds\n",
      "Batch 444 took 0.16685128211975098 seconds\n",
      "Batch 445 took 0.17115449905395508 seconds\n",
      "Batch 446 took 0.1554727554321289 seconds\n",
      "Batch 447 took 0.15666437149047852 seconds\n",
      "Batch 448 took 0.15506362915039062 seconds\n",
      "Batch 449 took 0.15906596183776855 seconds\n",
      "Batch 450 took 0.156846284866333 seconds\n",
      "Batch 451 took 0.1622467041015625 seconds\n",
      "Batch 452 took 0.15482568740844727 seconds\n",
      "Batch 453 took 0.15764141082763672 seconds\n",
      "Batch 454 took 0.1598949432373047 seconds\n",
      "Batch 455 took 0.16474699974060059 seconds\n",
      "Batch 456 took 0.15491056442260742 seconds\n",
      "Batch 457 took 0.17236876487731934 seconds\n",
      "Batch 458 took 0.168670654296875 seconds\n",
      "Batch 459 took 0.21531152725219727 seconds\n",
      "Batch 460 took 0.1641068458557129 seconds\n",
      "Batch 461 took 0.16848111152648926 seconds\n",
      "Batch 462 took 0.15704965591430664 seconds\n",
      "Batch 463 took 0.17326140403747559 seconds\n",
      "Batch 464 took 0.15388202667236328 seconds\n",
      "Batch 465 took 0.15592098236083984 seconds\n",
      "Batch 466 took 0.17377519607543945 seconds\n",
      "Batch 467 took 0.19472599029541016 seconds\n",
      "Batch 468 took 0.15613341331481934 seconds\n",
      "Batch 469 took 0.1622624397277832 seconds\n",
      "Batch 470 took 0.17116522789001465 seconds\n",
      "Batch 471 took 0.17320561408996582 seconds\n",
      "Batch 472 took 0.1540968418121338 seconds\n",
      "Batch 473 took 0.21614861488342285 seconds\n",
      "Batch 474 took 0.15607714653015137 seconds\n",
      "Batch 475 took 0.16875529289245605 seconds\n",
      "Batch 476 took 0.1577162742614746 seconds\n",
      "Batch 477 took 0.15869450569152832 seconds\n",
      "Batch 478 took 0.1569511890411377 seconds\n",
      "Batch 479 took 0.16717863082885742 seconds\n",
      "Batch 480 took 0.15492010116577148 seconds\n",
      "Batch 481 took 0.15714144706726074 seconds\n",
      "Batch 482 took 0.17992281913757324 seconds\n",
      "Batch 483 took 0.15635418891906738 seconds\n",
      "Batch 484 took 0.15515613555908203 seconds\n",
      "Batch 485 took 0.16257452964782715 seconds\n",
      "Batch 486 took 0.1552903652191162 seconds\n",
      "Batch 487 took 0.15642833709716797 seconds\n",
      "Batch 488 took 0.15428566932678223 seconds\n",
      "Batch 489 took 0.15752840042114258 seconds\n",
      "Batch 490 took 0.1708676815032959 seconds\n",
      "Batch 491 took 0.17860651016235352 seconds\n",
      "Batch 492 took 0.15720224380493164 seconds\n",
      "Batch 493 took 0.15499186515808105 seconds\n",
      "Batch 494 took 0.16569232940673828 seconds\n",
      "Batch 495 took 0.1671903133392334 seconds\n",
      "Batch 496 took 0.15676164627075195 seconds\n",
      "Batch 497 took 0.15396595001220703 seconds\n",
      "Batch 498 took 0.15581846237182617 seconds\n",
      "Batch 499 took 0.16731476783752441 seconds\n",
      "Batch 500 took 0.1589806079864502 seconds\n",
      "Batch 501 took 0.199845552444458 seconds\n",
      "Batch 502 took 0.15723133087158203 seconds\n",
      "Batch 503 took 0.16042518615722656 seconds\n",
      "Batch 504 took 0.1699056625366211 seconds\n",
      "Batch 505 took 0.1554584503173828 seconds\n",
      "Batch 506 took 0.15618228912353516 seconds\n",
      "Batch 507 took 0.18198609352111816 seconds\n",
      "Batch 508 took 0.154296875 seconds\n",
      "Batch 509 took 0.20680499076843262 seconds\n",
      "Batch 510 took 0.15537405014038086 seconds\n",
      "Batch 511 took 0.15483379364013672 seconds\n",
      "Batch 512 took 0.15565228462219238 seconds\n",
      "Batch 513 took 0.15527701377868652 seconds\n",
      "Batch 514 took 0.1550002098083496 seconds\n",
      "Batch 515 took 0.1654338836669922 seconds\n",
      "Batch 516 took 0.166290283203125 seconds\n",
      "Batch 517 took 0.1575183868408203 seconds\n",
      "Batch 518 took 0.1552138328552246 seconds\n",
      "Batch 519 took 0.16016435623168945 seconds\n",
      "Batch 520 took 0.17902183532714844 seconds\n",
      "Batch 521 took 0.16069889068603516 seconds\n",
      "Batch 522 took 0.15528035163879395 seconds\n",
      "Batch 523 took 0.15741920471191406 seconds\n",
      "Batch 524 took 0.1551954746246338 seconds\n",
      "Batch 525 took 0.16252899169921875 seconds\n",
      "Batch 526 took 0.15489816665649414 seconds\n",
      "Batch 527 took 0.16012167930603027 seconds\n",
      "Batch 528 took 0.15953969955444336 seconds\n",
      "Batch 529 took 0.1710357666015625 seconds\n",
      "Batch 530 took 0.15656709671020508 seconds\n",
      "Batch 531 took 0.1580498218536377 seconds\n",
      "Batch 532 took 0.15756845474243164 seconds\n",
      "Batch 533 took 0.18011927604675293 seconds\n",
      "Batch 534 took 0.15784883499145508 seconds\n",
      "Batch 535 took 0.15917134284973145 seconds\n",
      "Batch 536 took 0.17158818244934082 seconds\n",
      "Batch 537 took 0.16004729270935059 seconds\n",
      "Batch 538 took 0.15279603004455566 seconds\n",
      "Batch 539 took 0.15840458869934082 seconds\n",
      "Batch 540 took 0.1570587158203125 seconds\n",
      "Batch 541 took 0.1970994472503662 seconds\n",
      "Batch 542 took 0.16624140739440918 seconds\n",
      "Batch 543 took 0.1567378044128418 seconds\n",
      "Batch 544 took 0.1553504467010498 seconds\n",
      "Batch 545 took 0.17496705055236816 seconds\n",
      "Batch 546 took 0.1645662784576416 seconds\n",
      "Batch 547 took 0.15834569931030273 seconds\n",
      "Batch 548 took 0.15492820739746094 seconds\n",
      "Batch 549 took 0.16042232513427734 seconds\n",
      "Batch 550 took 0.15394997596740723 seconds\n",
      "Batch 551 took 0.15924668312072754 seconds\n",
      "Batch 552 took 0.15482020378112793 seconds\n",
      "Batch 553 took 0.15743255615234375 seconds\n",
      "Batch 554 took 0.15905070304870605 seconds\n",
      "Batch 555 took 0.16507983207702637 seconds\n",
      "Batch 556 took 0.15653157234191895 seconds\n",
      "Batch 557 took 0.160567045211792 seconds\n",
      "Batch 558 took 0.2265300750732422 seconds\n",
      "Batch 559 took 0.161574125289917 seconds\n",
      "Batch 560 took 0.15637826919555664 seconds\n",
      "Batch 561 took 0.15816569328308105 seconds\n",
      "Batch 562 took 0.15594244003295898 seconds\n",
      "Batch 563 took 0.15697479248046875 seconds\n",
      "Batch 564 took 0.15879154205322266 seconds\n",
      "Batch 565 took 0.1558845043182373 seconds\n",
      "Batch 566 took 0.15807175636291504 seconds\n",
      "Batch 567 took 0.16996431350708008 seconds\n",
      "Batch 568 took 0.1594836711883545 seconds\n",
      "Batch 569 took 0.15466904640197754 seconds\n",
      "Batch 570 took 0.15828776359558105 seconds\n",
      "Batch 571 took 0.18266987800598145 seconds\n",
      "Batch 572 took 0.19608521461486816 seconds\n",
      "Batch 573 took 0.16971921920776367 seconds\n",
      "Batch 574 took 0.15852952003479004 seconds\n",
      "Batch 575 took 0.15758609771728516 seconds\n",
      "Batch 576 took 0.15791893005371094 seconds\n",
      "Batch 577 took 0.15607476234436035 seconds\n",
      "Batch 578 took 0.1575024127960205 seconds\n",
      "Batch 579 took 0.16182780265808105 seconds\n",
      "Batch 580 took 0.16405749320983887 seconds\n",
      "Batch 581 took 0.17753982543945312 seconds\n",
      "Batch 582 took 0.1611175537109375 seconds\n",
      "Batch 583 took 0.18623924255371094 seconds\n",
      "Batch 584 took 0.15822243690490723 seconds\n",
      "Batch 585 took 0.1593630313873291 seconds\n",
      "Batch 586 took 0.1590118408203125 seconds\n",
      "Batch 587 took 0.15584301948547363 seconds\n",
      "Batch 588 took 0.16055083274841309 seconds\n",
      "Batch 589 took 0.15807819366455078 seconds\n",
      "Batch 590 took 0.15897274017333984 seconds\n",
      "Batch 591 took 0.1569652557373047 seconds\n",
      "Batch 592 took 0.16946840286254883 seconds\n",
      "Batch 593 took 0.15651941299438477 seconds\n",
      "Batch 594 took 0.15678977966308594 seconds\n",
      "Batch 595 took 0.18036389350891113 seconds\n",
      "Batch 596 took 0.19327259063720703 seconds\n",
      "Batch 597 took 0.15851211547851562 seconds\n",
      "Batch 598 took 0.16178512573242188 seconds\n",
      "Batch 599 took 0.15689659118652344 seconds\n",
      "Batch 600 took 0.15961718559265137 seconds\n",
      "Batch 601 took 0.15849852561950684 seconds\n",
      "Batch 602 took 0.15893864631652832 seconds\n",
      "Batch 603 took 0.15638256072998047 seconds\n",
      "Batch 604 took 0.16202163696289062 seconds\n",
      "Batch 605 took 0.1735365390777588 seconds\n",
      "Batch 606 took 0.17285537719726562 seconds\n",
      "Batch 607 took 0.15825772285461426 seconds\n",
      "Batch 608 took 0.16675043106079102 seconds\n",
      "Batch 609 took 0.17270112037658691 seconds\n",
      "Batch 610 took 0.16966676712036133 seconds\n",
      "Batch 611 took 0.15589570999145508 seconds\n",
      "Batch 612 took 0.1555778980255127 seconds\n",
      "Batch 613 took 0.15825939178466797 seconds\n",
      "Batch 614 took 0.15933513641357422 seconds\n",
      "Batch 615 took 0.175689697265625 seconds\n",
      "Batch 616 took 0.17258095741271973 seconds\n",
      "Batch 617 took 0.2034463882446289 seconds\n",
      "Batch 618 took 0.1869792938232422 seconds\n",
      "Batch 619 took 0.16058778762817383 seconds\n",
      "Batch 620 took 0.16218137741088867 seconds\n",
      "Batch 621 took 0.1767439842224121 seconds\n",
      "Batch 622 took 0.1585226058959961 seconds\n",
      "Batch 623 took 0.1608870029449463 seconds\n",
      "Batch 624 took 0.1575336456298828 seconds\n",
      "Batch 625 took 0.16235065460205078 seconds\n",
      "Batch 626 took 0.15449070930480957 seconds\n",
      "Batch 627 took 0.16072511672973633 seconds\n",
      "Batch 628 took 0.17162561416625977 seconds\n",
      "Batch 629 took 0.1776294708251953 seconds\n",
      "Batch 630 took 0.1568899154663086 seconds\n",
      "Batch 631 took 0.15855050086975098 seconds\n",
      "Batch 632 took 0.15636777877807617 seconds\n",
      "Batch 633 took 0.17526507377624512 seconds\n",
      "Batch 634 took 0.15875697135925293 seconds\n",
      "Batch 635 took 0.16207242012023926 seconds\n",
      "Batch 636 took 0.15656042098999023 seconds\n",
      "Batch 637 took 0.1580808162689209 seconds\n",
      "Batch 638 took 0.15652894973754883 seconds\n",
      "Batch 639 took 0.16066908836364746 seconds\n",
      "Batch 640 took 0.15468811988830566 seconds\n",
      "Batch 641 took 0.16048717498779297 seconds\n",
      "Batch 642 took 0.16910719871520996 seconds\n",
      "Batch 643 took 0.16142606735229492 seconds\n",
      "Batch 644 took 0.15775394439697266 seconds\n",
      "Batch 645 took 0.16074609756469727 seconds\n",
      "Batch 646 took 0.19064712524414062 seconds\n",
      "Batch 647 took 0.16362571716308594 seconds\n",
      "Batch 648 took 0.15531063079833984 seconds\n",
      "Batch 649 took 0.1755361557006836 seconds\n",
      "Batch 650 took 0.1536705493927002 seconds\n",
      "Batch 651 took 0.1594862937927246 seconds\n",
      "Batch 652 took 0.15569233894348145 seconds\n",
      "Batch 653 took 0.1611192226409912 seconds\n",
      "Batch 654 took 0.16478562355041504 seconds\n",
      "Batch 655 took 0.1645653247833252 seconds\n",
      "Batch 656 took 0.15835213661193848 seconds\n",
      "Batch 657 took 0.16400694847106934 seconds\n",
      "Batch 658 took 0.15803289413452148 seconds\n",
      "Batch 659 took 0.1837308406829834 seconds\n",
      "Batch 660 took 0.15688276290893555 seconds\n",
      "Batch 661 took 0.15823841094970703 seconds\n",
      "Batch 662 took 0.15741348266601562 seconds\n",
      "Batch 663 took 0.15899157524108887 seconds\n",
      "Batch 664 took 0.1576375961303711 seconds\n",
      "Batch 665 took 0.16179347038269043 seconds\n",
      "Batch 666 took 0.15593481063842773 seconds\n",
      "Batch 667 took 0.17216205596923828 seconds\n",
      "Batch 668 took 0.17263340950012207 seconds\n",
      "Batch 669 took 0.16647744178771973 seconds\n",
      "Batch 670 took 0.16657257080078125 seconds\n",
      "Batch 671 took 0.1732311248779297 seconds\n",
      "Batch 672 took 0.17451786994934082 seconds\n",
      "Batch 673 took 0.1624009609222412 seconds\n",
      "Batch 674 took 0.1553964614868164 seconds\n",
      "Batch 675 took 0.2115921974182129 seconds\n",
      "Batch 676 took 0.15996050834655762 seconds\n",
      "Batch 677 took 0.1592411994934082 seconds\n",
      "Batch 678 took 0.15787243843078613 seconds\n",
      "Batch 679 took 0.1755070686340332 seconds\n",
      "Batch 680 took 0.15605998039245605 seconds\n",
      "Batch 681 took 0.15959978103637695 seconds\n",
      "Batch 682 took 0.17020916938781738 seconds\n",
      "Batch 683 took 0.16013264656066895 seconds\n",
      "Batch 684 took 0.19449400901794434 seconds\n",
      "Batch 685 took 0.1598985195159912 seconds\n",
      "Batch 686 took 0.15465903282165527 seconds\n",
      "Batch 687 took 0.16199231147766113 seconds\n",
      "Batch 688 took 0.16198277473449707 seconds\n",
      "Batch 689 took 0.16103005409240723 seconds\n",
      "Batch 690 took 0.15465235710144043 seconds\n",
      "Batch 691 took 0.16333317756652832 seconds\n",
      "Batch 692 took 0.1698746681213379 seconds\n",
      "Batch 693 took 0.15980982780456543 seconds\n",
      "Batch 694 took 0.15795516967773438 seconds\n",
      "Batch 695 took 0.16102218627929688 seconds\n",
      "Batch 696 took 0.16298604011535645 seconds\n",
      "Batch 697 took 0.2339932918548584 seconds\n",
      "Batch 698 took 0.15194392204284668 seconds\n",
      "Batch 699 took 0.16193795204162598 seconds\n",
      "Batch 700 took 0.15549969673156738 seconds\n",
      "Batch 701 took 0.16130876541137695 seconds\n",
      "Batch 702 took 0.15558171272277832 seconds\n",
      "Batch 703 took 0.16155743598937988 seconds\n",
      "Batch 704 took 0.17167162895202637 seconds\n",
      "Batch 705 took 0.16059660911560059 seconds\n",
      "Batch 706 took 0.15582609176635742 seconds\n",
      "Batch 707 took 0.16575002670288086 seconds\n",
      "Batch 708 took 0.15500497817993164 seconds\n",
      "Batch 709 took 0.18824458122253418 seconds\n",
      "Batch 710 took 0.1561882495880127 seconds\n",
      "Batch 711 took 0.158982515335083 seconds\n",
      "Batch 712 took 0.15791034698486328 seconds\n",
      "Batch 713 took 0.16057252883911133 seconds\n",
      "Batch 714 took 0.15612363815307617 seconds\n",
      "Batch 715 took 0.16016292572021484 seconds\n",
      "Batch 716 took 0.1596822738647461 seconds\n",
      "Batch 717 took 0.17859292030334473 seconds\n",
      "Batch 718 took 0.15612483024597168 seconds\n",
      "Batch 719 took 0.17115139961242676 seconds\n",
      "Batch 720 took 0.15613293647766113 seconds\n",
      "Batch 721 took 0.16487812995910645 seconds\n",
      "Batch 722 took 0.17563533782958984 seconds\n",
      "Batch 723 took 0.16073369979858398 seconds\n",
      "Batch 724 took 0.15515542030334473 seconds\n",
      "Batch 725 took 0.16226673126220703 seconds\n",
      "Batch 726 took 0.15582680702209473 seconds\n",
      "Batch 727 took 0.16003680229187012 seconds\n",
      "Batch 728 took 0.1566624641418457 seconds\n",
      "Batch 729 took 0.1739366054534912 seconds\n",
      "Batch 730 took 0.15624022483825684 seconds\n",
      "Batch 731 took 0.16178441047668457 seconds\n",
      "Batch 732 took 0.15638303756713867 seconds\n",
      "Batch 733 took 0.16141414642333984 seconds\n",
      "Batch 734 took 0.1718308925628662 seconds\n",
      "Batch 735 took 0.16926860809326172 seconds\n",
      "Batch 736 took 0.15845465660095215 seconds\n",
      "Batch 737 took 0.1600792407989502 seconds\n",
      "Batch 738 took 0.15928268432617188 seconds\n",
      "Batch 739 took 0.16204333305358887 seconds\n",
      "Batch 740 took 0.18544983863830566 seconds\n",
      "Batch 741 took 0.1740868091583252 seconds\n",
      "Batch 742 took 0.191650390625 seconds\n",
      "Batch 743 took 0.1703200340270996 seconds\n",
      "Batch 744 took 0.15942668914794922 seconds\n",
      "Batch 745 took 0.17137742042541504 seconds\n",
      "Batch 746 took 0.17541861534118652 seconds\n",
      "Batch 747 took 0.18183040618896484 seconds\n",
      "Batch 748 took 0.16322898864746094 seconds\n",
      "Batch 749 took 0.17321157455444336 seconds\n",
      "Batch 750 took 0.1848461627960205 seconds\n",
      "Batch 751 took 0.1899418830871582 seconds\n",
      "Batch 752 took 0.2483508586883545 seconds\n",
      "Batch 753 took 0.18245959281921387 seconds\n",
      "Batch 754 took 0.16959404945373535 seconds\n",
      "Batch 755 took 0.17946743965148926 seconds\n",
      "Batch 756 took 0.16045188903808594 seconds\n",
      "Batch 757 took 0.15704679489135742 seconds\n",
      "Batch 758 took 0.19076299667358398 seconds\n",
      "Batch 759 took 0.1924269199371338 seconds\n",
      "Batch 760 took 0.18410277366638184 seconds\n",
      "Batch 761 took 0.16635513305664062 seconds\n",
      "Batch 762 took 0.17645263671875 seconds\n",
      "Batch 763 took 0.1690075397491455 seconds\n",
      "Batch 764 took 0.18657374382019043 seconds\n",
      "Batch 765 took 0.2299964427947998 seconds\n",
      "Batch 766 took 0.18587255477905273 seconds\n",
      "Batch 767 took 0.15948891639709473 seconds\n",
      "Batch 768 took 0.1565685272216797 seconds\n",
      "Batch 769 took 0.17122840881347656 seconds\n",
      "Batch 770 took 0.1697690486907959 seconds\n",
      "Batch 771 took 0.16525053977966309 seconds\n",
      "Batch 772 took 0.20409846305847168 seconds\n",
      "Batch 773 took 0.1735227108001709 seconds\n",
      "Batch 774 took 0.15722942352294922 seconds\n",
      "Batch 775 took 0.16884851455688477 seconds\n",
      "Batch 776 took 0.18301606178283691 seconds\n",
      "Batch 777 took 0.16080307960510254 seconds\n",
      "Batch 778 took 0.16757822036743164 seconds\n",
      "Batch 779 took 0.18131780624389648 seconds\n",
      "Batch 780 took 0.17392611503601074 seconds\n",
      "Batch 781 took 0.2117021083831787 seconds\n",
      "Batch 782 took 0.2171013355255127 seconds\n",
      "Batch 783 took 0.16158127784729004 seconds\n",
      "Batch 784 took 0.17359113693237305 seconds\n",
      "Batch 785 took 0.1588296890258789 seconds\n",
      "Batch 786 took 0.15790796279907227 seconds\n",
      "Batch 787 took 0.16124176979064941 seconds\n",
      "Batch 788 took 0.1799468994140625 seconds\n",
      "Batch 789 took 0.17273497581481934 seconds\n",
      "Batch 790 took 0.16961312294006348 seconds\n",
      "Batch 791 took 0.19180750846862793 seconds\n",
      "Batch 792 took 0.22305727005004883 seconds\n",
      "Batch 793 took 0.1965951919555664 seconds\n",
      "Batch 794 took 0.19414067268371582 seconds\n",
      "Batch 795 took 0.19678282737731934 seconds\n",
      "Batch 796 took 0.24583673477172852 seconds\n",
      "Batch 797 took 0.19727134704589844 seconds\n",
      "Batch 798 took 0.19982266426086426 seconds\n",
      "Batch 799 took 0.1728229522705078 seconds\n",
      "Batch 800 took 0.1675090789794922 seconds\n",
      "Batch 801 took 0.1684262752532959 seconds\n",
      "Batch 802 took 0.162095308303833 seconds\n",
      "Batch 803 took 0.16010355949401855 seconds\n",
      "Batch 804 took 0.18602538108825684 seconds\n",
      "Batch 805 took 0.1599118709564209 seconds\n",
      "Batch 806 took 0.16032981872558594 seconds\n",
      "Batch 807 took 0.18862390518188477 seconds\n",
      "Batch 808 took 0.1735227108001709 seconds\n",
      "Batch 809 took 0.16596579551696777 seconds\n",
      "Batch 810 took 0.15731024742126465 seconds\n",
      "Batch 811 took 0.17912888526916504 seconds\n",
      "Batch 812 took 0.16244196891784668 seconds\n",
      "Batch 813 took 0.16064047813415527 seconds\n",
      "Batch 814 took 0.15949511528015137 seconds\n",
      "Batch 815 took 0.16400432586669922 seconds\n",
      "Batch 816 took 0.2053985595703125 seconds\n",
      "Batch 817 took 0.1555638313293457 seconds\n",
      "Batch 818 took 0.1667478084564209 seconds\n",
      "Batch 819 took 0.1575777530670166 seconds\n",
      "Batch 820 took 0.16450071334838867 seconds\n",
      "Batch 821 took 0.1680300235748291 seconds\n",
      "Batch 822 took 0.17421627044677734 seconds\n",
      "Batch 823 took 0.18782758712768555 seconds\n",
      "Batch 824 took 0.16245222091674805 seconds\n",
      "Batch 825 took 0.16498374938964844 seconds\n",
      "Batch 826 took 0.1876373291015625 seconds\n",
      "Batch 827 took 0.2614097595214844 seconds\n",
      "Batch 828 took 0.2758674621582031 seconds\n",
      "Batch 829 took 0.2598109245300293 seconds\n",
      "Batch 830 took 0.29592061042785645 seconds\n",
      "Batch 831 took 0.24081635475158691 seconds\n",
      "Batch 832 took 0.25487208366394043 seconds\n",
      "Batch 833 took 0.24253487586975098 seconds\n",
      "Batch 834 took 0.2400355339050293 seconds\n",
      "Batch 835 took 0.21158337593078613 seconds\n",
      "Batch 836 took 0.21476221084594727 seconds\n",
      "Batch 837 took 0.19105935096740723 seconds\n",
      "Batch 838 took 0.15009522438049316 seconds\n",
      "Batch 839 took 0.19469547271728516 seconds\n",
      "Batch 840 took 0.18039512634277344 seconds\n",
      "Batch 841 took 0.16299939155578613 seconds\n",
      "Batch 842 took 0.1761019229888916 seconds\n",
      "Batch 843 took 0.15784907341003418 seconds\n",
      "Batch 844 took 0.16432952880859375 seconds\n",
      "Batch 845 took 0.16547942161560059 seconds\n",
      "Batch 846 took 0.16741442680358887 seconds\n",
      "Batch 847 took 0.15240883827209473 seconds\n",
      "Batch 848 took 0.19838166236877441 seconds\n",
      "Batch 849 took 0.17131447792053223 seconds\n",
      "Batch 850 took 0.1622145175933838 seconds\n",
      "Batch 851 took 0.16180896759033203 seconds\n",
      "Batch 852 took 0.1718907356262207 seconds\n",
      "Batch 853 took 0.16528892517089844 seconds\n",
      "Batch 854 took 0.16896939277648926 seconds\n",
      "Batch 855 took 0.1569957733154297 seconds\n",
      "Batch 856 took 0.17651128768920898 seconds\n",
      "Batch 857 took 0.16364836692810059 seconds\n",
      "Batch 858 took 0.16966676712036133 seconds\n",
      "Batch 859 took 0.1584327220916748 seconds\n",
      "Batch 860 took 0.20240998268127441 seconds\n",
      "Batch 861 took 0.17824578285217285 seconds\n",
      "Batch 862 took 0.16809344291687012 seconds\n",
      "Batch 863 took 0.1782388687133789 seconds\n",
      "Batch 864 took 0.1677234172821045 seconds\n",
      "Batch 865 took 0.16489744186401367 seconds\n",
      "Batch 866 took 0.1889333724975586 seconds\n",
      "Batch 867 took 0.16874194145202637 seconds\n",
      "Batch 868 took 0.17229747772216797 seconds\n",
      "Batch 869 took 0.16287851333618164 seconds\n",
      "Batch 870 took 0.1700296401977539 seconds\n",
      "Batch 871 took 0.17137718200683594 seconds\n",
      "Batch 872 took 0.18961572647094727 seconds\n",
      "Batch 873 took 0.16673612594604492 seconds\n",
      "Batch 874 took 0.17695140838623047 seconds\n",
      "Batch 875 took 0.19119644165039062 seconds\n",
      "Batch 876 took 0.19386863708496094 seconds\n",
      "Batch 877 took 0.1945018768310547 seconds\n",
      "Batch 878 took 0.18265700340270996 seconds\n",
      "Batch 879 took 0.15941977500915527 seconds\n",
      "Batch 880 took 0.16549134254455566 seconds\n",
      "Batch 881 took 0.16013598442077637 seconds\n",
      "Batch 882 took 0.1617598533630371 seconds\n",
      "Batch 883 took 0.16468524932861328 seconds\n",
      "Batch 884 took 0.17397093772888184 seconds\n",
      "Batch 885 took 0.15678739547729492 seconds\n",
      "Batch 886 took 0.15808463096618652 seconds\n",
      "Batch 887 took 0.15834450721740723 seconds\n",
      "Batch 888 took 0.1614532470703125 seconds\n",
      "Batch 889 took 0.1622753143310547 seconds\n",
      "Batch 890 took 0.16133356094360352 seconds\n",
      "Batch 891 took 0.1641402244567871 seconds\n",
      "Batch 892 took 0.1635737419128418 seconds\n",
      "Batch 893 took 0.1575307846069336 seconds\n",
      "Batch 894 took 0.16832709312438965 seconds\n",
      "Batch 895 took 0.15821599960327148 seconds\n",
      "Batch 896 took 0.26920151710510254 seconds\n",
      "Batch 897 took 0.23585247993469238 seconds\n",
      "Batch 898 took 0.15353965759277344 seconds\n",
      "Batch 899 took 0.1548140048980713 seconds\n",
      "Batch 900 took 0.15849852561950684 seconds\n",
      "Batch 901 took 0.1548328399658203 seconds\n",
      "Batch 902 took 0.17598438262939453 seconds\n",
      "Batch 903 took 0.16336321830749512 seconds\n",
      "Batch 904 took 0.15201330184936523 seconds\n",
      "Batch 905 took 0.15746521949768066 seconds\n",
      "Batch 906 took 0.15865159034729004 seconds\n",
      "Batch 907 took 0.16238665580749512 seconds\n",
      "Batch 908 took 0.1896522045135498 seconds\n",
      "Batch 909 took 0.22733688354492188 seconds\n",
      "Batch 910 took 0.18998026847839355 seconds\n",
      "Batch 911 took 0.17501473426818848 seconds\n",
      "Batch 912 took 0.1591336727142334 seconds\n",
      "Batch 913 took 0.16376399993896484 seconds\n",
      "Batch 914 took 0.17096996307373047 seconds\n",
      "Batch 915 took 0.16831493377685547 seconds\n",
      "Batch 916 took 0.15882134437561035 seconds\n",
      "Batch 917 took 0.16696453094482422 seconds\n",
      "Batch 918 took 0.20757842063903809 seconds\n",
      "Batch 919 took 0.1540694236755371 seconds\n",
      "Batch 920 took 0.18940448760986328 seconds\n",
      "Batch 921 took 0.15878844261169434 seconds\n",
      "Batch 922 took 0.17935633659362793 seconds\n",
      "Batch 923 took 0.1544637680053711 seconds\n",
      "Batch 924 took 0.15698027610778809 seconds\n",
      "Batch 925 took 0.17279815673828125 seconds\n",
      "Batch 926 took 0.16582918167114258 seconds\n",
      "Batch 927 took 0.16467070579528809 seconds\n",
      "Batch 928 took 0.157423734664917 seconds\n",
      "Batch 929 took 0.15862226486206055 seconds\n",
      "Batch 930 took 0.15584993362426758 seconds\n",
      "Batch 931 took 0.23316407203674316 seconds\n",
      "Batch 932 took 0.2332782745361328 seconds\n",
      "Batch 933 took 0.15281319618225098 seconds\n",
      "Batch 934 took 0.15445756912231445 seconds\n",
      "Batch 935 took 0.1670844554901123 seconds\n",
      "Batch 936 took 0.1581439971923828 seconds\n",
      "Batch 937 took 0.18067502975463867 seconds\n",
      "Batch 938 took 0.18086576461791992 seconds\n",
      "Batch 939 took 0.17409968376159668 seconds\n",
      "Batch 940 took 0.17037534713745117 seconds\n",
      "Batch 941 took 0.16600346565246582 seconds\n",
      "Batch 942 took 0.16918087005615234 seconds\n",
      "Batch 943 took 0.1754608154296875 seconds\n",
      "Batch 944 took 0.18453192710876465 seconds\n",
      "Batch 945 took 0.1663646697998047 seconds\n",
      "Batch 946 took 0.16876649856567383 seconds\n",
      "Batch 947 took 0.19983625411987305 seconds\n",
      "Batch 948 took 0.1953446865081787 seconds\n",
      "Batch 949 took 0.23597955703735352 seconds\n",
      "Batch 950 took 0.2322862148284912 seconds\n",
      "Batch 951 took 0.17804646492004395 seconds\n",
      "Batch 952 took 0.17917728424072266 seconds\n",
      "Batch 953 took 0.17453289031982422 seconds\n",
      "Batch 954 took 0.23146533966064453 seconds\n",
      "Batch 955 took 0.18174171447753906 seconds\n",
      "Batch 956 took 0.17731833457946777 seconds\n",
      "Batch 957 took 0.16741228103637695 seconds\n",
      "Batch 958 took 0.1818394660949707 seconds\n",
      "Batch 959 took 0.16477608680725098 seconds\n",
      "Batch 960 took 0.16995716094970703 seconds\n",
      "Batch 961 took 0.16888761520385742 seconds\n",
      "Batch 962 took 0.17300939559936523 seconds\n",
      "Batch 963 took 0.1678316593170166 seconds\n",
      "Batch 964 took 0.19954466819763184 seconds\n",
      "Batch 965 took 0.16869711875915527 seconds\n",
      "Batch 966 took 0.1977376937866211 seconds\n",
      "Batch 967 took 0.17797541618347168 seconds\n",
      "Batch 968 took 0.16599822044372559 seconds\n",
      "Batch 969 took 0.17101144790649414 seconds\n",
      "Batch 970 took 0.2147502899169922 seconds\n",
      "Batch 971 took 0.20319581031799316 seconds\n",
      "Batch 972 took 0.17854094505310059 seconds\n",
      "Batch 973 took 0.16955304145812988 seconds\n",
      "Batch 974 took 0.16219735145568848 seconds\n",
      "Batch 975 took 0.17230677604675293 seconds\n",
      "Batch 976 took 0.19947385787963867 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i, batch in enumerate(data_module.val_dataloader()):\n",
    "    # print(i, batch.keys())\n",
    "    if len(mg.model.additional_inputs) > 0:\n",
    "        batch = batch | mg.model.additional_inputs\n",
    "    curr_time = time.time()\n",
    "    outputs = mg.model(**batch)\n",
    "    print(f\"Batch {i} took {time.time() - curr_time} seconds\")\n",
    "    # print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dummy_in \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m              k: v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dummy_in\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m      4\u001b[0m        }\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m dummy_in \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m              k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dummy_in\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m      4\u001b[0m        }\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "dummy_in = {\n",
    "    k: v.cuda() if isinstance(v, torch.Tensor) else v\n",
    "    for k, v in dummy_in.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m pass_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint8\u001b[39m\u001b[38;5;124m'\u001b[39m,                                                     \u001b[38;5;66;03m# collect weight statistics for linear layers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnCalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m,                                                \u001b[38;5;66;03m# collect activation statistics for relu layers\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengineFile\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.plan\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_tensorrt_transform_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Work/mase/machop/chop/passes/graph/transforms/quantize_tensorRT/quantize_tensorrt.py:182\u001b[0m, in \u001b[0;36mquantize_tensorrt_transform_pass\u001b[0;34m(graph, pass_args)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mQuantize the graph using TensorRT.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m:rtype: MaseGraph\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Convert model to ONNX\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m \u001b[43mconvert_model_to_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdummy_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnxFile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monnxFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Create a TensorRT engine\u001b[39;00m\n\u001b[1;32m    185\u001b[0m engine \u001b[38;5;241m=\u001b[39m build_trt_engine(pass_args)\n",
      "File \u001b[0;32m~/Desktop/Work/mase/machop/chop/passes/graph/transforms/quantize_tensorRT/quantize_tensorrt.py:91\u001b[0m, in \u001b[0;36mconvert_model_to_onnx\u001b[0;34m(graph, dummy_in, onnxFile)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Create a dummy input\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# dummy_in = next(iter(input_generator))['x']\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# dummy_input = Variable(dummy_in.cuda(), requires_grad=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# torch.onnx.export(graph.model, dummy_input, \"model.onnx\", verbose=True)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# torch.onnx.export(graph.model.cuda(), dummy_input, onnxFile, verbose=True)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dummy_in, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 91\u001b[0m     dummy_in \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     92\u001b[0m             k: v\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dummy_in\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     94\u001b[0m         }\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     dummy_in \u001b[38;5;241m=\u001b[39m dummy_in\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/Desktop/Work/mase/machop/chop/passes/graph/transforms/quantize_tensorRT/quantize_tensorrt.py:92\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Create a dummy input\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# dummy_in = next(iter(input_generator))['x']\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# dummy_input = Variable(dummy_in.cuda(), requires_grad=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# torch.onnx.export(graph.model, dummy_input, \"model.onnx\", verbose=True)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# torch.onnx.export(graph.model.cuda(), dummy_input, onnxFile, verbose=True)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dummy_in, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     91\u001b[0m     dummy_in \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 92\u001b[0m             k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dummy_in\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     94\u001b[0m         }\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     dummy_in \u001b[38;5;241m=\u001b[39m dummy_in\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "pass_args = {\n",
    "    \"precision\": 'int8',                                                     # collect weight statistics for linear layers\n",
    "    \"nCalibration\": 10,                                                # collect activation statistics for relu layers\n",
    "    \"dummy_in\": dummy_in,\n",
    "    \"onnxFile\": 'model.onnx',\n",
    "    \"cacheFile\": 'model.INT8Cache',  \n",
    "    \"engineFile\": 'model.plan'\n",
    "}\n",
    "engine = quantize_tensorrt_transform_pass(mg, pass_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
