{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff1bS2_YZ3hB"
      },
      "source": [
        "# Custom Kernel Tutorial\n",
        "> Revision\n",
        "> - Created on 21/01/2025, Cheng Zhang: PyTorch 2.5.0, CUDA 12.3\n",
        "> - Fixed and Tested on 05/02/2025, Cheng Zhang: PyTorch 2.6.0, CUDA 12.5\n",
        "\n",
        "## Env Setup in Colab\n",
        "\n",
        "Check if Colab is connected to a NVIDIA Tesla T4 or Ada L4 GPU (L4 is faster), if not, change Colab runtime to T4 or L4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBGkK1R2NVyT",
        "outputId": "44937788-c4da-4011-f991-e5c46bab37b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb  5 00:10:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   62C    P8             14W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('âŒ Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B66afpunzTQp"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VUhihX3iOYF9",
        "outputId": "9aeb7d27-8168-4a98-9aa5-4bfd5869322d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tox\n",
            "  Downloading tox-4.24.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting rust-just\n",
            "  Downloading rust_just-1.39.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (120 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: cachetools>=5.5 in /usr/local/lib/python3.11/dist-packages (from tox) (5.5.1)\n",
            "Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.11/dist-packages (from tox) (5.2.0)\n",
            "Collecting colorama>=0.4.6 (from tox)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from tox) (3.17.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from tox) (24.2)\n",
            "Requirement already satisfied: platformdirs>=4.3.6 in /usr/local/lib/python3.11/dist-packages (from tox) (4.3.6)\n",
            "Requirement already satisfied: pluggy>=1.5 in /usr/local/lib/python3.11/dist-packages (from tox) (1.5.0)\n",
            "Collecting pyproject-api>=1.8 (from tox)\n",
            "  Downloading pyproject_api-1.9.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting virtualenv>=20.27.1 (from tox)\n",
            "  Downloading virtualenv-20.29.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.27.1->tox)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading tox-4.24.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m171.8/171.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rust_just-1.39.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading pyproject_api-1.9.0-py3-none-any.whl (13 kB)\n",
            "Downloading virtualenv-20.29.1-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv, rust-just, pyproject-api, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, colorama, tox, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed colorama-0.4.6 distlib-0.3.9 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyproject-api-1.9.0 rust-just-1.39.0 tox-4.24.1 virtualenv-20.29.1\n",
            "Collecting build\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build) (24.2)\n",
            "Collecting pyproject_hooks (from build)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pyproject_hooks, build\n",
            "Successfully installed build-1.2.2.post1 pyproject_hooks-1.2.0\n",
            "Cloning into 'mase-cuda'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (270/270), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 270 (delta 91), reused 266 (delta 88), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (270/270), 62.99 KiB | 528.00 KiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Submodule 'submodules/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'submodules/cutlass'\n",
            "Submodule 'submodules/nlohmann_json' (https://github.com/nlohmann/json.git) registered for path 'submodules/nlohmann_json'\n",
            "Cloning into '/content/mase-cuda/submodules/cutlass'...\n",
            "remote: Enumerating objects: 31469, done.        \n",
            "remote: Counting objects: 100% (19/19), done.        \n",
            "remote: Compressing objects: 100% (19/19), done.        \n",
            "remote: Total 31469 (delta 6), reused 0 (delta 0), pack-reused 31450 (from 2)        \n",
            "Receiving objects: 100% (31469/31469), 47.47 MiB | 23.71 MiB/s, done.\n",
            "Resolving deltas: 100% (23927/23927), done.\n",
            "Cloning into '/content/mase-cuda/submodules/nlohmann_json'...\n",
            "remote: Enumerating objects: 47834, done.        \n",
            "remote: Counting objects: 100% (153/153), done.        \n",
            "remote: Compressing objects: 100% (121/121), done.        \n",
            "remote: Total 47834 (delta 90), reused 32 (delta 32), pack-reused 47681 (from 4)        \n",
            "Receiving objects: 100% (47834/47834), 199.98 MiB | 18.01 MiB/s, done.\n",
            "Resolving deltas: 100% (28462/28462), done.\n",
            "Submodule path 'submodules/cutlass': checked out '08101d9d0ca68fbdd4ed8833a9fa66dc3948b77d'\n",
            "Submodule path 'submodules/nlohmann_json': checked out 'ac8b22180db393d56f5f2954eb353967fad254e3'\n",
            "/content/mase-cuda\n"
          ]
        }
      ],
      "source": [
        "! pip install tox ninja torch numpy scipy rust-just pytest transformers\n",
        "! pip install -U build\n",
        "\n",
        "git_token = \"Your Git Token\"\n",
        "! git clone --recurse-submodules https://{git_token}@github.com/DeepWok/mase-cuda.git\n",
        "%cd mase-cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJ6j3wRaoc2"
      },
      "source": [
        "## Build & Run C++ Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fIMUKJWLJo10",
        "outputId": "121845c1-a9b9-48f3-d303-a598d24f9478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m# python\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/dist ]; then rm -r /content/mase-cuda/dist; fi\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/src/mase_cuda.egg-info ]; then rm -r /content/mase-cuda/src/mase_cuda.egg-info; fi\u001b[0m\n",
            "\u001b[1m# all\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/build ]; then rm -r /content/mase-cuda/build; fi\u001b[0m\n",
            "\u001b[1mecho $(which cmake)\u001b[0m\n",
            "/usr/local/bin/cmake\n",
            "\u001b[1mcmake -D BUILD_TESTING=ON -D CUDA_ARCHITECTURES=native -B build -S .\u001b[0m\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- CUDA_ARCHITECTURES: native\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.11.11\") found components: Interpreter Development Development.Module Development.Embed\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.5\") \n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.5.82\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Caffe2: CUDA detected: 12.5\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.5\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.11.11\") found components: Interpreter\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\n",
            "  Failed to compute shorthash for libnvrtc.so\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/lib/python3.11/dist-packages/cmake/data/share/cmake-3.31/Modules/FindPackageHandleStandardArgs.cmake:441 (message):\n",
            "  The package name passed to `find_package_handle_standard_args` (nvtx3) does\n",
            "  not match the name of the calling package (Caffe2).  This can lead to\n",
            "  problems in calling code that expects `find_package` result variables\n",
            "  (e.g., `_FOUND`) to follow a certain pattern.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could NOT find nvtx3 (missing: nvtx3_dir) \n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\n",
            "  Cannot find NVTX3, find old NVTX instead\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
            "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
            "-- USE_CUDSS is set to 0. Compiling without cuDSS support\n",
            "-- USE_CUFILE is set to 0. Compiling without cuFile support\n",
            "-- Autodetected CUDA architecture(s):  8.9\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_89,code=sm_89\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
            "  static library kineto_LIBRARY-NOTFOUND not found.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch: /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so\n",
            "-- Configuring done (9.8s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/mase-cuda/build\n",
            "\u001b[1mif [ -z  ]; then cmake --build /content/mase-cuda/build -j 12 ; else cmake --build /content/mase-cuda/build --target  -j 12 ; fi\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CUDA object test/cu/mxint/dequantize/CMakeFiles/test_mxint8_dequantize1d.dir/test_mxint8_dequantize1d.cu.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CUDA executable test_mxint8_dequantize1d\u001b[0m\n",
            "[100%] Built target test_mxint8_dequantize1d\n"
          ]
        }
      ],
      "source": [
        "!just build-cu-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BajynrmayrY"
      },
      "source": [
        "Run test executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIxj1uQGZQcj",
        "outputId": "7c31a616-58f3-46dc-912c-0b0b3b9ca152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d [m] [group_size] [is_random]\n",
            "m=4096, group_size=128, num_groups=32, is_random=0\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "! ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdO3h_06a99Q"
      },
      "source": [
        "## Build & Try mase-cuda Package\n",
        "\n",
        "The building process can be slow. NVIDIA T4's compuate capability is 7.5, and L4 is 8.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v01rAwvkZaf6",
        "outputId": "6586b852-4d61-4ad6-8da6-e01c3b8f2190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m* Creating isolated environment: venv+pip...\u001b[0m\n",
            "\u001b[1m* Installing packages in isolated environment:\u001b[0m\n",
            "  - numpy\n",
            "  - setuptools\n",
            "  - torch\n",
            "\u001b[1m* Getting build dependencies for wheel...\u001b[0m\n",
            "running egg_info\n",
            "creating src/mase_cuda.egg-info\n",
            "writing src/mase_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to src/mase_cuda.egg-info/dependency_links.txt\n",
            "writing requirements to src/mase_cuda.egg-info/requires.txt\n",
            "writing top-level names to src/mase_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "\u001b[1m* Building wheel...\u001b[0m\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/utils.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/constants.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/__init__.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/dequantize.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/quantize.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/__init__.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/linear.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda/tools\n",
            "copying src/mase_cuda/tools/bin_repr.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/tools\n",
            "running egg_info\n",
            "writing src/mase_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to src/mase_cuda.egg-info/dependency_links.txt\n",
            "writing requirements to src/mase_cuda.egg-info/requires.txt\n",
            "writing top-level names to src/mase_cuda.egg-info/top_level.txt\n",
            "reading manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "creating build/lib.linux-x86_64-cpython-311/csrc\n",
            "copying src/csrc/bind.cu -> build/lib.linux-x86_64-cpython-311/csrc\n",
            "running build_ext\n",
            "/tmp/build-env-lns31uxx/lib/python3.11/site-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/tmp/build-env-lns31uxx/lib/python3.11/site-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'mase_cuda_ext' extension\n",
            "creating /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc\n",
            "Emitting ninja build file /content/mase-cuda/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "Compiling objects...\n",
            "Using envvar MAX_JOBS (12) as the number of workers...\n",
            "[1/1] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o.d -I/content/mase-cuda/submodules/cutlass/include -I/content/mase-cuda/submodules/cutlass/tools/util/include -I/tmp/build-env-lns31uxx/lib/python3.11/site-packages/torch/include -I/tmp/build-env-lns31uxx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/build-env-lns31uxx/lib/python3.11/site-packages/torch/include/TH -I/tmp/build-env-lns31uxx/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda/include -I/tmp/build-env-lns31uxx/include -I/usr/include/python3.11 -c -c /content/mase-cuda/src/csrc/bind.cu -o /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DNDEBUG -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=mase_cuda_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_89,code=sm_89\n",
            "x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o -L/tmp/build-env-lns31uxx/lib/python3.11/site-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lcuda -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/mase_cuda_ext.cpython-311-x86_64-linux-gnu.so\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/dequantize.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/quantize.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/__init__.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/linear.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/utils.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda/tools\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/tools/bin_repr.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/tools\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/constants.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/__init__.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/csrc\n",
            "copying build/lib.linux-x86_64-cpython-311/csrc/bind.cu -> build/bdist.linux-x86_64/wheel/./csrc\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda_ext.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "running install_egg_info\n",
            "Copying src/mase_cuda.egg-info to build/bdist.linux-x86_64/wheel/./mase_cuda-0.0.1-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda-0.0.1.dist-info/WHEEL\n",
            "creating '/content/mase-cuda/dist/.tmp-1jnyreyk/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'mase_cuda_ext.cpython-311-x86_64-linux-gnu.so'\n",
            "adding 'csrc/bind.cu'\n",
            "adding 'mase_cuda/__init__.py'\n",
            "adding 'mase_cuda/constants.py'\n",
            "adding 'mase_cuda/utils.py'\n",
            "adding 'mase_cuda/mxint8/__init__.py'\n",
            "adding 'mase_cuda/mxint8/dequantize.py'\n",
            "adding 'mase_cuda/mxint8/linear.py'\n",
            "adding 'mase_cuda/mxint8/quantize.py'\n",
            "adding 'mase_cuda/tools/bin_repr.py'\n",
            "adding 'mase_cuda-0.0.1.dist-info/METADATA'\n",
            "adding 'mase_cuda-0.0.1.dist-info/WHEEL'\n",
            "adding 'mase_cuda-0.0.1.dist-info/top_level.txt'\n",
            "adding 'mase_cuda-0.0.1.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[1m\u001b[92mSuccessfully built \u001b[4mmase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl\u001b[0m\u001b[1m\u001b[92m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! TORCH_CUDA_ARCH_LIST=\"7.5 8.9\" MAX_JOBS=$(nproc --all) python -m build --wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ©¹ We create a new env and run experiments there to avoid errors like `cuda pytorch undefined symbol` raied by the Colab's built-in Python\n",
        "\n",
        "When the wheel is built, install the mase-cuda wheel into **a new dev env**\n",
        "\n",
        "- Open **Colab termimal** and run the following commands to create a dev env and install mase-cuda:\n",
        "\n",
        "  ```bash\n",
        "  cd mase-cuda\n",
        "  tox -e dev # create dev env\n",
        "  . .tox/dev/bin/activate # activate dev env\n",
        "  which pip # ensure this is the pip in .tox/dev\n",
        "  pip install dist/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl # install mase-cuda\n",
        "  ```\n",
        "- Colab terminal: Running the following command to profile dequantization latency (CPU vs GPU). This is slow:\n",
        "\n",
        "  ```bash\n",
        "  pytest -v --log-cli-level INFO test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency\n",
        "  ```\n",
        "\n",
        "The output looks like this\n",
        "\n",
        "```bash\n",
        "============================================================== test session starts ===============================================================\n",
        "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0 -- /content/mase-cuda/.tox/dev/bin/python\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /content/mase-cuda\n",
        "configfile: tox.ini\n",
        "collected 1 item                                                                                                                                 \n",
        "\n",
        "test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency\n",
        "----------------------------------------------------------------- live log call ------------------------------------------------------------------\n",
        "INFO     test_dequantize1d:test_dequantize1d.py:203\n",
        "+-----------+------------+------------------------+------------------------+---------------------+\n",
        "|     m     | group_size |      latency_cpu       |      latency_gpu       |     GPU speedup     |\n",
        "+-----------+------------+------------------------+------------------------+---------------------+\n",
        "|   1024    |     8      | 1.8215179443359376e-05 | 5.830879891291261e-05  | 0.31239160783546144 |\n",
        "|   1024    |     16     | 1.1014938354492188e-05 | 2.411839971318841e-05  | 0.45670270355744236 |\n",
        "|   1024    |     32     | 1.0752677917480469e-05 | 2.3455999884754422e-05 | 0.45841908127179576 |\n",
        "|   1024    |     64     | 1.0418891906738282e-05 | 2.366719990968704e-05  | 0.44022495041645404 |\n",
        "|   1024    |    128     | 1.043081283569336e-05  | 2.370399972423911e-05  | 0.44004442106987857 |\n",
        "|   1024    |    256     | 9.298324584960938e-06  | 2.200479982420802e-05  | 0.42255892619989316 |\n",
        "|   1024    |    512     | 9.393692016601562e-06  | 2.2529599815607072e-05 | 0.41694890692617675 |\n",
        "|  2097152  |     8      |  0.019188427925109865  | 3.4844799526035784e-05 |  550.682689701584   |\n",
        "|  2097152  |     16     |  0.018679165840148927  | 3.453759923577308e-05  |  540.8356762910598  |\n",
        "|  2097152  |     32     |  0.018654394149780273  | 3.327519977465272e-05  |  560.6095313059608  |\n",
        "|  2097152  |     64     |  0.01862926483154297   | 3.344159927219152e-05  |  557.068598302181   |\n",
        "...\n",
        "| 234881024 |     32     |   2.2112363457679747   |  0.008763614416122436  | 252.32013194235924  |\n",
        "| 234881024 |     64     |   2.3624018669128417   |  0.008794196844100953  | 268.63190678947717  |\n",
        "| 234881024 |    128     |   2.402256155014038    |  0.008851744079589843  |  271.3878907268803  |\n",
        "| 234881024 |    256     |   2.4408880949020384   |  0.008949855947494508  |  272.7293164517759  |\n",
        "| 234881024 |    512     |   2.4862973570823668   |  0.004568324756622315  |  544.2470685732645  |\n",
        "+-----------+------------+------------------------+------------------------+---------------------+\n",
        "PASSED                                                                                                                                     [100%]\n",
        "\n",
        "========================================================= 1 passed in 466.97s (0:07:46) ==========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "UbFWFS7BWowY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90MO9ajpz54_"
      },
      "source": [
        "### FP32 Deberta Demo\n",
        "\n",
        "- Colab Terminal: Install transformers in the dev env: `pip install transformers`\n",
        "\n",
        "- demo.py: Copy the following codes into a new file demo.py\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
        "# if you meet OOM error, try this smaller model, but the quantization effect may not be obvious later\n",
        "# model_name = \"AnkitAI/deberta-v3-small-base-emotions-classifier\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).cuda().eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
        "\n",
        "# Example usage\n",
        "@torch.no_grad()\n",
        "def predict_emotion(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = logits.argmax(dim=1)\n",
        "    predictions = label2emotion[predictions.item()]\n",
        "    top3_values, top3_indices = torch.topk(logits, 3)\n",
        "    top3_values = top3_values.cpu().tolist()\n",
        "    top3_indices = top3_indices.cpu().tolist()\n",
        "    return predictions, (top3_values, top3_indices)\n",
        "\n",
        "\n",
        "text = \"I'm so happy with the results!\"\n",
        "emotion, top3 = predict_emotion(model, tokenizer, text)\n",
        "\n",
        "print(\"Index to Emotion Mapping:\", label2emotion)\n",
        "print(\"Input text:\", text)\n",
        "print(\"Detected Emotion:\", emotion)\n",
        "print(f\"top3 logits: {top3[0]}, top3 indices: {top3[1]}\")\n",
        "```\n",
        "\n",
        "- Colab Terminal: run demo.py in the dev env\n",
        "\n",
        "The output looks like this:\n",
        "\n",
        "```bash\n",
        "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.12k/1.12k [00:00<00:00, 7.73MB/s]\n",
        "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.04G/3.04G [01:12<00:00, 42.0MB/s]\n",
        "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31k/1.31k [00:00<00:00, 12.6MB/s]\n",
        "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798k/798k [00:00<00:00, 5.79MB/s]\n",
        "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 3.35MB/s]\n",
        "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.11M/2.11M [00:00<00:00, 10.3MB/s]\n",
        "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 969/969 [00:00<00:00, 8.67MB/s]\n",
        "Index to Emotion Mapping: {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
        "Input text: I'm so happy with the results!\n",
        "Detected Emotion: joy\n",
        "top3 logits: [[7.345228672027588, -1.4850201606750488, -1.6403964757919312]], top3 indices: [[1, 4, 0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XrSk9bfz_nZ"
      },
      "source": [
        "### MXINT8 Deberta\n",
        "- demo-q.py: Copy the following codes into a new file demo-q.py. This file creates the quantized Deberta and compare GPU memory usage of MXINT8 model with FP32 model\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "from mase_cuda.mxint8.linear import QLinearPacked\n",
        "\n",
        "init_memory = torch.cuda.memory_allocated()  # in bytes\n",
        "model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float32).cuda().eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
        "\n",
        "mxint8_group_size = 32\n",
        "assert model.config.hidden_size % mxint8_group_size == 0\n",
        "assert model.config.intermediate_size % mxint8_group_size == 0\n",
        "\n",
        "text = \"I'm so happy with the results!\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_emotion(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = logits.argmax(dim=1)\n",
        "    predictions = label2emotion[predictions.item()]\n",
        "    top3_values, top3_indices = torch.topk(logits, 3)\n",
        "    top3_values = top3_values.cpu().tolist()\n",
        "    top3_indices = top3_indices.cpu().tolist()\n",
        "    return predictions, (top3_values, top3_indices)\n",
        "\n",
        "# check the GPU memory usage of FP32 model\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "emotion_fp32, top3_fp32 = predict_emotion(model, tokenizer, text)\n",
        "peak_memory_fp32 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
        "\n",
        "\n",
        "def set_layer_by_name(module: torch.nn.Module, name: str, new_layer: torch.nn.Module):\n",
        "    levels = name.split(\".\")\n",
        "    if len(levels) > 1:\n",
        "        mod_ = module\n",
        "        for l_idx in range(len(levels) - 1):\n",
        "            if levels[l_idx].isdigit():\n",
        "                mod_ = mod_[int(levels[l_idx])]\n",
        "            else:\n",
        "                mod_ = getattr(mod_, levels[l_idx])\n",
        "        setattr(mod_, levels[-1], new_layer)\n",
        "    else:\n",
        "        setattr(module, name, new_layer)\n",
        "\n",
        "\n",
        "for layer_name, layer in model.named_modules():\n",
        "    if not isinstance(layer, torch.nn.Linear):\n",
        "        continue\n",
        "    if \"classifier\" in layer_name:\n",
        "        continue\n",
        "    layer.cuda()\n",
        "    layer_q = QLinearPacked.build_from_linear(layer, group_size=mxint8_group_size)\n",
        "    set_layer_by_name(model, layer_name, layer_q)\n",
        "    del layer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "emotion_mxint8, top3_mxint8 = predict_emotion(model, tokenizer, text)\n",
        "peak_memory_mxint8 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
        "\n",
        "print(f\"FP32 model peak memory: {peak_memory_fp32/1024**2:.4f} MB\")\n",
        "print(f\"PF32 prediction: {emotion_fp32}\")\n",
        "print(f\"FP32 top3 logits: {top3_fp32[0]}, indices: {top3_fp32[1]}\")\n",
        "\n",
        "print(f\"MXINT8 model peak memory: {peak_memory_mxint8/1024**2:.4f} MB\")\n",
        "print(f\"MXINT8 prediction: {emotion_mxint8}\")\n",
        "print(f\"MXINT8 top3 logits: {top3_mxint8[0]}, indices: {top3_mxint8[1]}\")\n",
        "```\n",
        "\n",
        "- Colab Terminal: Run demo-q.py in the dev env\n",
        "\n",
        "The output looks like this:\n",
        "\n",
        "```bash\n",
        "FP32 model peak memory: 2906.1997 MB\n",
        "PF32 prediction: joy\n",
        "FP32 top3 logits: [[7.345228672027588, -1.4850201606750488, -1.6403964757919312]], indices: [[1, 4, 0]]\n",
        "MXINT8 model peak memory: 976.1616 MB\n",
        "MXINT8 prediction: joy\n",
        "MXINT8 top3 logits: [[7.350157737731934, -1.488325834274292, -1.649757981300354]], indices: [[1, 4, 0]]\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}