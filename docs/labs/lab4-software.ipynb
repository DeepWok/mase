{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://deepwok.github.io/\">\n",
    "    <img src=\"../imgs/deepwok.png\" alt=\"Logo\" width=\"160\" height=\"160\">\n",
    "  </a>\n",
    "\n",
    "  <h1 align=\"center\">Lab 4 for Advanced Deep Learning Systems (ADLS) - Software Stream</h1>\n",
    "\n",
    "  <p align=\"center\">\n",
    "    ELEC70109/EE9-AML3-10/EE9-AO25\n",
    "    <br />\n",
    "\t\tWritten by\n",
    "    <a href=\"https://aaron-zhao123.github.io/\">Aaron Zhao, Cheng Zhang, and Pedro Gimenes </a>\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General introduction\n",
    "\n",
    "In this lab, you will learn how to optimize performance for torch based models. \n",
    "\n",
    "We will cover the following topics:\n",
    "1. Automatic performance tuning using existing high-level flows (eg. `torch.compile`), understand its main building blocks and how it works.\n",
    "2. Manual performance enhancement with techniques such as kernel fusion. \n",
    "3. Manual performance enhancement with low-level custom kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.compile\n",
    "\n",
    "`torch.compile` makes PyTorch models run faster by optimizing the model and the input data. It is a just-in-time compiler that optimizes the model and the input data for the specific hardware. It is a part of the PyTorch ecosystem.\n",
    "\n",
    "JIT compilation is a technique that converts the Python code into machine code at runtime. This technique is used to improve the performance of the Python code. JIT compilation is used in many programming languages, including Python, Java, and C#. JIT compiler typically continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation or recompilation would outweigh the overhead of compiling that code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to use `torch.compile` to optimize the performance of some arbitrary functions and also a model. The usage of `torch.compile` is very simple. You just need to add `torch.compile` as a decorator before the function or model you want to optimize, or you wrap it on top of the function or model.\n",
    "\n",
    "We first instantiate the helper functions below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from chop.models import get_model\n",
    "from chop.dataset import get_dataset_info\n",
    "\n",
    "\n",
    "def timed_gpu(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "\n",
    "def timed_cpu(fn):\n",
    "    start = time.time()\n",
    "    result = fn()\n",
    "    return result, time.time() - start\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    return torch.randn(128, 3, 224, 224)\n",
    "\n",
    "\n",
    "def time_model(fn, n=1000, device=\"cpu\"):\n",
    "    times = []\n",
    "    data = get_data().to(device)\n",
    "    for _ in range(n):\n",
    "        if device == \"cpu\":\n",
    "            _, t = timed_cpu(lambda: fn(data.cpu()))\n",
    "        else:\n",
    "            _, t = timed_gpu(lambda: fn(data))\n",
    "        times.append(t)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the neural network. We're using a model which can be used to perform image classification on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPretrained weights loaded into Resnet\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cifar10_info = get_dataset_info(\"imagenet\")\n",
    "model = get_model(\"resnet18\", pretrained=True, dataset_info=cifar10_info)\n",
    "image = torch.randn(64, 3, 224, 224)\n",
    "\n",
    "opt_model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can inspect the runtime of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model: 3.0053 s\n",
      "Optimized model: 2.1805 s\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "n = 5\n",
    "\n",
    "model.to(device)\n",
    "opt_model.to(device)\n",
    "avg_t = time_model(model, n=n, device=device)\n",
    "opt_avg_t = time_model(opt_model, n=n, device=device)\n",
    "print(f\"Original model: {avg_t:.4f} s\")\n",
    "print(f\"Optimized model: {opt_avg_t:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ùìQuestion** We can see, very likely, maybe it varies on your machine, that the runtime of the optimized model is actually slower. There are a few reasons for this, but these are left as an exercise for the reader to investigate.\n",
    "\n",
    "The core idea about `torch.compile` is that it is an automatic optimization tool. There are actually three main building blocks in `torch.compile`:\n",
    "\n",
    "- TorchDynamo: `torch._dynamo` actually captures the PyTorch graph through CPython (https://github.com/pytorch/pytorch/tree/main/torch/_dynamo).\n",
    "- TorchInductor: `torch._inductor` (https://github.com/pytorch/pytorch/tree/main/torch/_inductor) can be seen as the backend of `torch.compile`, which is responsible for providing the kernel implementation of different operators in the PyTorch graph, it actually mainly uses OpenAI's Triton for implementing these kernels (https://github.com/pytorch/pytorch/tree/main/torch/_inductor/kernel).\n",
    "- AOT Autograd: This allows you to capture the whole graph, including the backward pass, ahead of time!\n",
    "\n",
    "TorchInductor actually also makes use of the `fx.graph` to pattern match code, as shown [here](https://github.com/pytorch/pytorch/tree/main/torch/_inductor/fx_passes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fused kernels are advantageous as they can reduce both the number of memory accesses and number of kernel launches. This can be particularly useful when you have a lot of small operations that can be fused together.\n",
    "\n",
    "### A simple example\n",
    "\n",
    "Assuming we have a simple module consiting of a linear layer and a relu activation function,\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y1 = self.linear(x)\n",
    "        y2 = self.relu(y1)\n",
    "        return y2\n",
    "```\n",
    "\n",
    "Note that both the linear layer (`torch.nn.functional.linear`) and ReLU layer (`torch.nn.functional.relu`) are built-in PyTorch operations. Here is what happens when this module runs on a GPU:\n",
    "\n",
    "- The kernel for the linear layer is launched on the GPU: \n",
    "  - Tiles of the input tensor `x` and weights `W` are **loaded from GPU's global memory** to shared memory\n",
    "  - Matrix multiplication runs with the `x` and `W` tiles, and the partial results are accumulated in registers until the entire output tile is completed.\n",
    "  - The output tile is **written back** to the global memory (`y1`).\n",
    "- Similarly, the kernel for the ReLU layer is then launched on the GPU:\n",
    "  - Tiles of `y1` is **loaded from the global memory**.\n",
    "  - ReLU is performed on `y1` tiles, \n",
    "  - The results are **written back to the global memory**.\n",
    "\n",
    "Everytime we call a kernel, we have to load the data from the global memory, and later write the results back. Because of the limited bandwidth of the global memory, one way to improve the performance is to reduce the number of memory accesses. This is where fused kernels come in. A fused kernel of linear and relu works like this:\n",
    "\n",
    "- When the kernel for the fused operation is launched on the GPU:\n",
    "  - Tiles of the input tensor `x` and weights `W` are **loaded from GPU's global memory** to shared memory\n",
    "  - The matrix multiplications runs on with `x` and `W` tiles, and the partial results are accumulated in registers until the entire output tile is completed.\n",
    "  - *ReLU is performed on the output tile*\n",
    "  - The results are **written back to the global memory**.\n",
    "\n",
    "Fusing these two kernels saves the one read and one write between the GPU's (relatively) slow global memory and fast cache memory, especially considering that ReLU (relu(x)=max(0, x)) is a memory-bouded operation. \n",
    "\n",
    "\n",
    "### SDPA example\n",
    "The following example demonstrates a fused kernel to optimize the performance of the scaled dot product attention (SDPA). SDPA denotes the attention operations in Transformer, consisting of two GEMMs with a softmax operation in between. PyTorch 2.0+ offers a fused kernel for SDPA.\n",
    "\n",
    "Here we show the naive implementation of SDPA and also call the fused kernel for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        scale_factor = 1 / math.sqrt(query.size(-1))\n",
    "        score = query @ key.transpose(-2, -1) * scale_factor\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = attn @ value\n",
    "        return context\n",
    "\n",
    "\n",
    "class ScaledDotProductAttentionFused(torch.nn.Module):\n",
    "    def forward(self, query, key, value):\n",
    "        return F.scaled_dot_product_attention(query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first implementation makes use of a number of functions to compute the scaled dot product attention. The second implementation uses a fused kernel provided by Pytorch to compute the scaled dot product attention.\n",
    "\n",
    "Now we can do a very simple test to check their functional equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float16) tensor(1., dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "query = torch.ones(32, 8, 128, 64, dtype=torch.float16, device=device)\n",
    "key = torch.ones(32, 8, 128, 64, dtype=torch.float16, device=device)\n",
    "value = torch.ones(32, 8, 128, 64, dtype=torch.float16, device=device)\n",
    "\n",
    "y1 = ScaledDotProductAttention()(query, key, value)\n",
    "y2 = ScaledDotProductAttentionFused()(query, key, value)\n",
    "print(y1[0, 0, 0, 0], y2[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method behind fused SPDA is detailed in the [FlashAttention paper](https://arxiv.org/abs/2205.14135). FlashAttention loads Q, K, and V tiles from global memory, and computes the attention scores and the weighted sum of values in a single kernel. Note that the softmax is a row reduction operation, thus FlashAttention use a trick called \"online-softmax\", which records row max and row sum of partial results and scales the partial results before accumulation. Online-softmax allows the fusion of the GEMM-softmax-GEMM operation into a single kernel, saving the memory bandwidth and improving the performance losslessly. FlashAttention also optimizes the backward propagation computation of the attention layer. If you are interested in the details, you can check the paper series of FlashAttention, and [Triton implementation of FlashAttention](https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html) (Triton provides a Python-based programming environment for writing DNN compute kernels on GPU hardware without having to write CUDA code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Kernels\n",
    "\n",
    "Custom kernels are a powerful way to optimize the performance of your code. \n",
    "\n",
    "When you run a PyTorch built-in function, e.g., `torch.matmul`, PyTorch will call the corresponding kernel function wrapped in its backend ATen to execute the operation. These built-in kernels may come from different libraries, such as Intel MKL, NVIDIA cuBLAS, or PyTorch's own implementation. PyTorch provides a lot of built-in kernels for different operations. However, sometimes you may want to write your own custom kernel to support a specific operation that PyTorch does not natively provide.\n",
    "\n",
    "### Quantization and Dequantization\n",
    "In the following, we show *a custom kernel for dequantization*. Dequantization is the process of converting quantized values back to floating-point values. \n",
    "\n",
    "- Here is an example of 4-bit fixed-point numbers with radix point placed at the most significant bit (0b.0000). Quantizing an FP32 number to 4-bit fixed-point means finding the closest 4-bit fixed-point number. This process introduces quantization error, e.g., 0.0621687 will be rounded to 0.0625 (with an error of 0.000103313), but a previously 32-bit number can now be represented with only 4 bits, and potentially accelerate the computation if the hardware supports 4-bit fixed-point operations.\n",
    "\n",
    "  ```bash\n",
    "  | FP32       | 4-bit fixed-point        |\n",
    "  | ---------- | ------------------------ |\n",
    "  | 0.0        | 0.0    (0b0000)          |\n",
    "  | 0.0625     | 0.0625 (0b0001)          |\n",
    "  | 0.125      | 0.125  (0b0010)          |\n",
    "  | 0.1875     | 0.1875 (0b0011)          |\n",
    "  | 0.25       | 0.25   (0b0100)          |\n",
    "  | ...        | ...                      |\n",
    "  ```\n",
    "\n",
    "- Dequantization is the reverse process of quantization. It converts the quantized values back to floating-point values (without fixing the quantization error).\n",
    "\n",
    "GPU is still a common choice of running DNN workloads. DNN Quantization methods for GPUs can be generally categorized into two types:\n",
    "(a) Weight-only quantization, (b) Weight-and-activation quantization. The kernel in this tutorial is for the former, which only quantizes weights to save GPU memory.\n",
    "\n",
    "### MXINT Format\n",
    "\n",
    "MXINT is a number format of vectors between floating-point and fixed-point, proposed by Microsoft ([ref](https://www.microsoft.com/en-us/research/publication/with-shared-microexponents-a-little-shifting-goes-a-long-way/)) and standardized by the industry for AI facility ([ref](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)).\n",
    "\n",
    "MXINT takes the following form with `group_size` as the number of mantissas in a group sharing the same 8-bit exponent:\n",
    "\n",
    "```text\n",
    "Exp |- Mantissa 1\n",
    "    |- Mantissa 2\n",
    "    |- ...\n",
    "    |- Mantissa group_size\n",
    "```\n",
    "The mantissa in MXINT is signed fixed-point with the radix point placed after the 2nd leftmost bit, and exp is an 8-bit unsigned integer biased by 127.\n",
    "\n",
    "```text\n",
    "Exp: eeee_eeee\n",
    "Mantissa (8-bit in this example): si.ii_iiii\n",
    "```\n",
    "\n",
    "For example, an example of an MXINT4 (4 denotes the mantissa bitwidth) vector with `group_size=4` looks like this:\n",
    "\n",
    "```text\n",
    "EXP = 0b0111_1111\n",
    "MANTISSA = [0b11.00]\n",
    "           [0b01.00]\n",
    "           [0b00.10]\n",
    "           [0b00.01]    \n",
    "``` \n",
    "\n",
    "The four mantissas share the same exponent.\n",
    "\n",
    "To dequantize this MXINT4 vector, we scale each fixed-point value by the shared exponent:\n",
    "\n",
    "- The decimal value of EXP is E = 2^(EXP-127) = 2^(127-127) = 1\n",
    "- Then we scale each signed fixed-point value by E, i.e., `E .* MANTISSA`\n",
    "```text\n",
    "                         [0b11.00]        [-1.0 ]    [-1.0]\n",
    "    E .* MANTISSA = 1 .* [0b01.00] = 1 .* [ 1.0 ]  = [ 1.0]\n",
    "                         [0b00.10]        [ 0.5 ]    [ 0.5]\n",
    "                         [0b00.11]        [ 0.25]    [ 0.25]\n",
    "```\n",
    "\n",
    "The effective bitwidth of this MXINT4 vector is (4 + 8/4) = 6 bits. If we increase the group size to 32, the effective bitwidth will be (4 + 8 / 32) = 4.25 bits. Compared to 4-bit fixed-point number that only represents 16 values, MXINT4 with group_size = 32 can represent 2^8 * 16 = 4096 values, reducing the quantization error.\n",
    "\n",
    "‚ùì **Question**: How does MXINT8 benefit custom hardware if both the activation and weights in a linear layer are quantized to MXINT8?\n",
    "\n",
    "Now We show a custom kernel for dequantizing MXINT8 numbers, and show how it saves GPU memory with minimal loss of model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Kernel for MXINT8 Dequantization.\n",
    "\n",
    "#### Preamble\n",
    "**Why dequantization kernel?** \n",
    "\n",
    "We target saving GPU memory by quantizing weights to approximated 8-bit numbers. This means model weights are stored in the form of MXINT8 in GPU global memory. When running a specific layer, the GPU loads the MXINT8 weights, dequantizes them to high-precision (BFloat16 this in kernel) and writes back for the following computation. Once computation of this layer is done, the dequantized weights are discarded. This saves GPU memory by storing weights in a more compact form.\n",
    "\n",
    "\n",
    "**The following tutorial is more involved**\n",
    "- CUDA-capable GPU is required.\n",
    "- If developing on your own Windows laptop, WSL2 is highly recommended.\n",
    "- The dev env needs setup with PyTorch, CUDA, and CMake.\n",
    "- The kernel requires basic knowledge of C++, CUDA programming and CUTLASS CUTE.\n",
    "  - CUTLASS is a CUDA template library for GEMM. CUTE is a part of CUTLASS for data/thread layout transformation.\n",
    "  - CUTLASS repo: https://github.com/NVIDIA/cutlass\n",
    "  - CUTE-related docs: https://github.com/NVIDIA/cutlass/tree/main/media/docs/cute\n",
    "\n",
    "**MASE-CUDA** \n",
    "- The source codes of MXINT8 dequantization kernel can be found in [MASE-CUDA](https://github.com/DeepWok/mase-cuda).\n",
    "- To setup the dev environment for custom kernels, please refer to the [README.md](https://github.com/DeepWok/mase-cuda/tree/master) and [this file](https://github.com/DeepWok/mase-cuda/blob/master/docs/beginner.md). This dev env is for building the kernel and Python package wheel. Once the wheel is built, you may install it in the mase environment and use it to quantize models.\n",
    "\n",
    "#### MXINT8 Dequantization Kernel\n",
    "\n",
    "The following kernels dequantize MXINT8 data to [BFloat16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format).\n",
    "\n",
    "##### CPU kernel\n",
    "Usually before implementing CUDA kernels, we need to check the correctness of our understanding. This can be done by writing a CPU or Python version of the algorithm, which is easy to debug. [`mase_cuda::mxint8::dequantize::dequantize1d_host`](https://github.com/DeepWok/mase-cuda/blob/master/src/csrc/mxint/dequantize.cuh) is the C++ version of the dequantization algorithm, and [`mase_cuda.mxint8.dequantize.dequantize1d_simulated`](https://github.com/DeepWok/mase-cuda/blob/master/src/mase_cuda/mxint8/dequantize.py) is the PyTorch-simulated version of the algorithm. Here is the core part of the C++ version:\n",
    "\n",
    "```cpp\n",
    "    for (int i = 0; i < M; ++i) {\n",
    "        auto sign = static_cast<uint16_t>(hX[i] & 0x80) << 8;\n",
    "        auto exp = static_cast<uint16_t>(hScales[i / group_size]) << 7;\n",
    "        auto mantissa_abs = abs(hX[i]);\n",
    "        auto frac = static_cast<uint16_t>((mantissa_abs & 0x3F) << 1);\n",
    "        auto out = cutlass::bfloat16_t::bitcast(sign | exp | frac);\n",
    "        auto dont_need_abs = bool(mantissa_abs & 0x40);\n",
    "        auto bias = cutlass::bfloat16_t::bitcast(sign | exp | uint16_t(0));\n",
    "        y[i] = dont_need_abs ? out : out - bias;\n",
    "    }\n",
    "```\n",
    "\n",
    "For each mantissa element, the loop \n",
    "- splits the sign bit from the mantissa, upcasts it to 16-bit and shifts it to the correct position.\n",
    "- upcasts the scale to 16-bit and shifts it to the correct position.\n",
    "- converts the int8 mantissa to BFloat16's fraction part. \n",
    "\n",
    "‚ùì **Question**: What is the purpose of the `dont_need_abs` variable and the `bias` variable? Note that unlike IEEE Floating-Point, MXINT has no implicit leading bit for the mantissa.\n",
    "\n",
    "##### GPU kernel\n",
    "The CUDA kernel codes can be found at [`mase_cuda::mxint8::dequantize:dequantize1d_device`](https://github.com/DeepWok/mase-cuda/blob/master/src/csrc/mxint/dequantize.cuh). This kernel treats 1D data as a matrix of shape `[num_groups, group_size]` using `group_tiler`. Then it loads tiles of MXINT8 data from global memory to shared memory, dequantizes them using the algorithm above, and write the dequantized BFloat16 number back to global memory. This kernel leverages CUTE for partitioning data for copy, and partitioning threads in a threadblock for computation. Besides, it creates predication masks to avoid out-of-bound accesses.\n",
    "\n",
    "‚ùì **Question (Challenging)**: How does `cta_tiler` partition the data for copy? You may find the documentation of `local_tile` in CUTE helpful ([ref](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md))\n",
    "\n",
    "‚ùì **Question (Challenging)**: How does `layout_sX` partition the threads in a threadblock for computation? You may find the documentation of `local_partition` in CUTE helpful ([ref](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md))\n",
    "\n",
    "---\n",
    "\n",
    "üìù **C++/CUDA Build**: The following tutorial requires setting up a C++ & CUDA dev env. **If you do not have a CUDA-enabled machine, or you find it difficult to set up such an env, you may use Colab instead: Open [lab4_software_custom_kernel_colab_minimal.ipynb](./lab4_software_custom_kernel_colab_minimal.ipynb) in your Colab and run it there.**\n",
    "\n",
    "##### Testing the kernel\n",
    "\n",
    "After implementing the CUDA kernel, we can compare its output to the CPU version to check the correctness. A simple test to check the correctness of the CUDA kernel can be found [here](https://github.com/DeepWok/mase-cuda/blob/master/test/cu/mxint/dequantize/test_mxint8_dequantize1d.cu). We first generate random MXINT8 data, dequantize them using the CPU version, and then compare the results with the GPU version.\n",
    "\n",
    "You may run the following just command to build and run the test executable. Before that, you may need to change the `TORCH_CUDA_ARCH_LIST` variable in the [justfile](https://github.com/DeepWok/mase-cuda/blob/master/justfile) to match your GPU's [compute capability](https://developer.nvidia.com/cuda-gpus).\n",
    "```bash\n",
    "# build the test executable\n",
    "$ just --set CU_BUILD_TARGETS test_mxint8_dequantize1d build-cu-test\n",
    "# run the test executable\n",
    "$ ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d \n",
    "Usage: ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d [m] [group_size] [is_random]\n",
    "m=4096, group_size=128, num_groups=32, is_random=0\n",
    "PASSED\n",
    "# or you can specify the arguments\n",
    "$ ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d 2048 32 1\n",
    "Usage: ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d [m] [group_size] [is_random]\n",
    "m=2048, group_size=32, num_groups=64, is_random=1\n",
    "PASSED\n",
    "```\n",
    "\n",
    "##### Binding the kernel to PyTorch \n",
    "\n",
    "Many DeepLearning packages run on Python, and PyTorch is one of the most popular ones. To use the CUDA kernel in PyTorch, we need to \n",
    "- Wrap the kernels with LibTorch API in C++. The wrapper function `mase_cuda::mxint8::dequantize::dequantize1d` can be found [here](https://github.com/DeepWok/mase-cuda/blob/master/src/csrc/mxint/dequantize.cuh). This wrapper takes PyTorch Tensors, and calls the CPU kernel or the CUDA kernel based on the device type of the input tensors, and returns the dequantized BFloat16 Tensors.\n",
    "- Build the wrapper as an extension module (named `mase_cuda_ext` in this case) of `mase_cuda`. These steps uses PyBind11 and PyTorch C++ Extension. Here are the related files:\n",
    "  - [bind.cu](https://github.com/DeepWok/mase-cuda/blob/master/src/csrc/bind.cu)\n",
    "  - [setup.py](https://github.com/DeepWok/mase-cuda/blob/master/setup.py)\n",
    "- Build `mase_cuda` package! You can build the package by running the following command:\n",
    "  ```bash\n",
    "  # the wheel file will be generated in the dist folder\n",
    "  $ just-build-py\n",
    "  # install the wheel file\n",
    "  $ pip install dist/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl\n",
    "  ```\n",
    "\n",
    "#### Playing with the kernel\n",
    "\n",
    "The dequantization is now available in Python package `mase_cuda_ext`, and the main package `mase_cuda` has a Python wrapper over it.\n",
    "\n",
    "##### Latency profiling: GPU vs CPU\n",
    "\n",
    "Function [`test_ext_dequantize1d_latency`](https://github.com/DeepWok/mase-cuda/blob/master/test/py/mxint8/dequantize/test_dequantize1d.py) compares the latency of the CPU version and the GPU version of the dequantization kernel. This test case can be triggered by running the following command:\n",
    "\n",
    "üìù **Note** Remember to reduce the element number `num_elements` in `test_ext_dequantize1d_latency` before running it as too many elements may run out of CPU/GPU memory!!!\n",
    "\n",
    "```bash\n",
    "$ just --set PYTEST_PATTERN test_ext_dequantize1d_latency test-py-pattern \n",
    "```\n",
    "\n",
    "Here is an example output of the test case:\n",
    "\n",
    "```text\n",
    "test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency\n",
    "----------------------------------------------------------------------------- live log call ------------------------------------------------------------------------------\n",
    "INFO     test_dequantize1d:test_dequantize1d.py:203\n",
    "+-----------+------------+------------------------+------------------------+---------------------+\n",
    "|     m     | group_size |      latency_cpu       |      latency_gpu       |     GPU speedup     |\n",
    "+-----------+------------+------------------------+------------------------+---------------------+\n",
    "|   1024    |     8      | 9.584426879882813e-06  | 3.673119996674359e-05  | 0.2609342163762836  |\n",
    "|   1024    |     16     |  4.9591064453125e-06   | 1.1745599983260035e-05 | 0.4222097170327847  |\n",
    "|   1024    |     32     | 3.993511199951172e-06  | 1.1219199933111667e-05 | 0.3559532964703628  |\n",
    "|   1024    |     64     | 3.826618194580078e-06  | 1.118720006197691e-05  | 0.3420532549146055  |\n",
    "|   1024    |    128     | 3.933906555175781e-06  | 1.1347199836745858e-05 | 0.34668522734891255 |\n",
    "|   1024    |    256     | 3.921985626220703e-06  | 1.0571199934929609e-05 | 0.3710066643675507  |\n",
    "|   1024    |    512     | 3.7789344787597655e-06 | 1.0598399955779315e-05 | 0.35655707413637566 |\n",
    "|  2097152  |     8      |  0.00988675355911255   | 1.6137600084766744e-05 |  612.653276024931   |\n",
    "|  2097152  |     16     |  0.00951402187347412   | 1.6275200061500074e-05 |  584.571731070765   |\n",
    "...\n",
    "| 234881024 |     8      |   1.0949357390403747   | 0.0018697983980178834  |  585.5902648120154  |\n",
    "| 234881024 |     16     |   1.0813238978385926   | 0.0017637632012367248  |  613.0777062818775  |\n",
    "| 234881024 |     32     |   1.0706217408180236   | 0.0017168064057826995  |  623.6123870529959  |\n",
    "| 234881024 |     64     |   1.0728538513183594   | 0.0017273023962974547  |  621.1152451464588  |\n",
    "| 234881024 |    128     |   1.062897527217865    | 0.0017135215997695922  |  620.3000460343113  |\n",
    "| 234881024 |    256     |   1.0750348925590516   | 0.0017249839961528779  |  623.2144152969729  |\n",
    "| 234881024 |    512     |   1.0646217107772826   | 0.0011905567944049836  |  894.2216917164034  |\n",
    "+-----------+------------+------------------------+------------------------+---------------------+\n",
    "PASSED                                                                                                                                                             [100%]\n",
    "```\n",
    "\n",
    "GPU is much faster than CPU if the data size is large enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saved GPU memory\n",
    "\n",
    "Let's use the dequantization kernel in a real model. `mase_cuda` has a custom layer `mase_cuda.mxint8.linear.QLinearPacked` for demostration (source code [here](https://github.com/DeepWok/mase-cuda/blob/master/src/mase_cuda/mxint8/linear.py)).\n",
    "\n",
    "We can compress the linear layers in a model with `QLinearPacked` and inspect the GPU memory usage. Install the built mase-cuda wheel in mase env (**not the dev env for kernel building**) and run the following codes using Python in mase env .\n",
    "\n",
    "\n",
    "- First let's just run a simple sentiment analysis model in FP32. **Please copy the code below to a new Python file and run it instead of running it in the notebook.** This example runs a Deberta model which classifies the emotion of a sentence into one of the 6 categories: `anger`, `fear`, `joy`, `love`, `sadness`, `surprise`.\n",
    "\n",
    "    ```python\n",
    "    import torch\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    \n",
    "    model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
    "    # if you meet OOM error, try this smaller model, but the quantization effect may not be obvious later\n",
    "    # model_name = \"AnkitAI/deberta-v3-small-base-emotions-classifier\" \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).cuda().eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
    "    \n",
    "    \n",
    "    # Example usage\n",
    "    @torch.no_grad()\n",
    "    def predict_emotion(model, tokenizer, text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        predictions = label2emotion[predictions.item()]\n",
    "        top3_values, top3_indices = torch.topk(logits, 3)\n",
    "        top3_values = top3_values.cpu().tolist()\n",
    "        top3_indices = top3_indices.cpu().tolist()\n",
    "        return predictions, (top3_values, top3_indices)\n",
    "    \n",
    "    \n",
    "    text = \"I'm so happy with the results!\"\n",
    "    emotion, top3 = predict_emotion(model, tokenizer, text)\n",
    "    \n",
    "    print(\"Index to Emotion Mapping:\", label2emotion)\n",
    "    print(\"Input text:\", text)\n",
    "    print(\"Detected Emotion:\", emotion)\n",
    "    print(f\"top3 logits: {top3[0]}, top3 indices: {top3[1]}\")\n",
    "    ```\n",
    "\n",
    "    This code will output the detected emotion of the input text and the top 3 logits and their indices.\n",
    "  \n",
    "  \n",
    "- Then let's compress the model with MXINT8 and inspect the GPU memory usage. **Please copy the code below to a new Python file and run it instead of running it in the notebook.**\n",
    "\n",
    "    ```python\n",
    "    import torch\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    \n",
    "    from mase_cuda.mxint8.linear import QLinearPacked\n",
    "    \n",
    "    init_memory = torch.cuda.memory_allocated()  # in bytes\n",
    "    model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float32).cuda().eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
    "    \n",
    "    mxint8_group_size = 32\n",
    "    assert model.config.hidden_size % mxint8_group_size == 0\n",
    "    assert model.config.intermediate_size % mxint8_group_size == 0\n",
    "    \n",
    "    text = \"I'm so happy with the results!\"\n",
    "    \n",
    "    \n",
    "    # Example usage\n",
    "    @torch.no_grad()\n",
    "    def predict_emotion(model, tokenizer, text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        predictions = label2emotion[predictions.item()]\n",
    "        top3_values, top3_indices = torch.topk(logits, 3)\n",
    "        top3_values = top3_values.cpu().tolist()\n",
    "        top3_indices = top3_indices.cpu().tolist()\n",
    "        return predictions, (top3_values, top3_indices)\n",
    "    \n",
    "    \n",
    "    # check the GPU memory usage of FP32 model\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    emotion_fp32, top3_fp32 = predict_emotion(model, tokenizer, text)\n",
    "    peak_memory_fp32 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
    "    \n",
    "    \n",
    "    def set_layer_by_name(module: torch.nn.Module, name: str, new_layer: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Replace a layer (`new_layer`) in a model (`module`) by its `name`.\n",
    "        \"\"\"\n",
    "        levels = name.split(\".\")\n",
    "        if len(levels) > 1:\n",
    "            mod_ = module\n",
    "            for l_idx in range(len(levels) - 1):\n",
    "                if levels[l_idx].isdigit():\n",
    "                    mod_ = mod_[int(levels[l_idx])]\n",
    "                else:\n",
    "                    mod_ = getattr(mod_, levels[l_idx])\n",
    "            setattr(mod_, levels[-1], new_layer)\n",
    "        else:\n",
    "            setattr(module, name, new_layer)\n",
    "    \n",
    "    \n",
    "    for layer_name, layer in model.named_modules():\n",
    "        if not isinstance(layer, torch.nn.Linear):\n",
    "            continue\n",
    "        if \"classifier\" in layer_name:\n",
    "            continue\n",
    "        layer.cuda()\n",
    "        layer_q = QLinearPacked.build_from_linear(layer, group_size=mxint8_group_size)\n",
    "        set_layer_by_name(model, layer_name, layer_q)\n",
    "        del layer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    emotion_mxint8, top3_mxint8 = predict_emotion(model, tokenizer, text)\n",
    "    peak_memory_mxint8 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
    "    \n",
    "    print(f\"FP32 model peak memory: {peak_memory_fp32/1024**2:.4f} MB\")\n",
    "    print(f\"PF32 prediction: {emotion_fp32}\")\n",
    "    print(f\"FP32 top3 logits: {top3_fp32[0]}, indices: {top3_fp32[1]}\")\n",
    "    \n",
    "    print(f\"MXINT8 model peak memory: {peak_memory_mxint8/1024**2:.4f} MB\")\n",
    "    print(f\"MXINT8 prediction: {emotion_mxint8}\")\n",
    "    print(f\"MXINT8 top3 logits: {top3_mxint8[0]}, indices: {top3_mxint8[1]}\")\n",
    "    ```\n",
    "    \n",
    "    Here is an example output of the code:\n",
    "    \n",
    "    ```text\n",
    "    FP32 model peak memory: 2906.1997 MB\n",
    "    PF32 prediction: joy\n",
    "    FP32 top3 logits: [[7.345229148864746, -1.485020399093628, -1.6403968334197998]], indices: [[1, 4, 0]]\n",
    "    MXINT8 model peak memory: 976.1616 MB\n",
    "    MXINT8 prediction: joy\n",
    "    MXINT8 top3 logits: [[7.350158214569092, -1.488326072692871, -1.6497581005096436]], indices: [[1, 4, 0]]\n",
    "    ```\n",
    "    \n",
    "    The MXINT8 **saves GPU memory by 66.4%** without affecting the final model prediction (\"joy\" in this case). Besides, the output logits are very close to the FP32 model.\n",
    "    \n",
    "    ‚ùì **Question**: Why the saved GPU memory is not exactly (32 - (4+8/32))/32 = 86.7%?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
