{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://deepwok.github.io/\">\n",
    "    <img src=\"../imgs/deepwok.png\" alt=\"Logo\" width=\"160\" height=\"160\">\n",
    "  </a>\n",
    "\n",
    "  <h1 align=\"center\">Lab 4 for Advanced Deep Learning Systems (ADLS) - Software Stream</h1>\n",
    "\n",
    "  <p align=\"center\">\n",
    "    ELEC70109/EE9-AML3-10/EE9-AO25\n",
    "    <br />\n",
    "\t\tWritten by\n",
    "    <a href=\"https://aaron-zhao123.github.io/\">Aaron Zhao, Pedro Gimenes </a>\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General introduction\n",
    "\n",
    "In this lab, you will learn how to optimize performance for torch based models. \n",
    "\n",
    "We will cover the following topics:\n",
    "1. Automatic performance tuning using existing high-level flows (eg. `torch.compile`), understand its main building blocks and how it works.\n",
    "2. Manual performance enhancement with techniques such as kernel fusion. \n",
    "3. Manual performance enhancement with low-level custom kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.compile\n",
    "\n",
    "`torch.compile` makes PyTorch models run faster by optimizing the model and the input data. It is a just-in-time compiler that optimizes the model and the input data for the specific hardware. It is a part of the PyTorch ecosystem.\n",
    "\n",
    "JIT compilation is a technique that converts the Python code into machine code at runtime. This technique is used to improve the performance of the Python code. JIT compilation is used in many programming languages, including Python, Java, and C#. JIT compiler typically continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation or recompilation would outweigh the overhead of compiling that code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to use `torch.compile` to optimize the performance of some arbitrary functions and also a model. The usage of `torch.compile` is very simple. You just need to add `torch.compile` as a decorator before the function or model you want to optimize, or you wrap it on top of the function or model.\n",
    "\n",
    "We first instantiate the helper functions below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from chop.models import get_model\n",
    "from chop.dataset import get_dataset_info\n",
    "\n",
    "def timed_gpu(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "def timed_cpu(fn):\n",
    "    start = time.time()\n",
    "    result = fn()\n",
    "    return result, time.time() - start\n",
    "\n",
    "def get_data():\n",
    "    return torch.randn(128, 3, 224, 224)\n",
    "\n",
    "def time_model(fn, n=1000, device='cpu'):\n",
    "    times = []\n",
    "    data = get_data().to(device)\n",
    "    for _ in range(n):\n",
    "        if device == 'cpu':\n",
    "            _, t = timed_cpu(lambda: fn(data.cpu()))\n",
    "        else:\n",
    "            _, t = timed_gpu(lambda: fn(data))\n",
    "        times.append(t)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the neural network. We're using a model which can be used to perform image classification on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"classification\"\n",
    "model = get_model(\"resnet18\", pretrained=True, num_classes=1000)\n",
    "image = torch.randn(64, 3, 224, 224)\n",
    "\n",
    "opt_model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can inspect the runtime of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model: 2.6594 s\n",
      "Optimized model: 3.1050 s\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "n = 5\n",
    "\n",
    "model.to(device)\n",
    "opt_model.to(device)\n",
    "avg_t = time_model(model, n=n, device=device)\n",
    "opt_avg_t = time_model(opt_model, n=n, device=device)\n",
    "print(f\"Original model: {avg_t:.4f} s\")\n",
    "print(f\"Optimized model: {opt_avg_t:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, very likely, maybe it varies on your machine, that the runtime of the optimized model is actually slower. There are a few reasons for this, but these are left as an exercise for the reader to investigate.\n",
    "\n",
    "The core idea about `torch.compile` is that it is an automatic optimization tool. There are actually three main building blocks in `torch.compile`:\n",
    "\n",
    "- TorchDynamo: `torch._dynamo` actually captures the PyTorch graph through CPython (https://github.com/pytorch/pytorch/tree/main/torch/_dynamo).\n",
    "- TorchInductor: `torch._inductor` (https://github.com/pytorch/pytorch/tree/main/torch/_inductor) can be seen as the backend of `torch.compile`, which is responsible for providing the kernel implementation of different operators in the PyTorch graph, it actually mainly uses OpenAI's Triton for implementing these kernels (https://github.com/pytorch/pytorch/tree/main/torch/_inductor/kernel).\n",
    "- AOT Autograd: This allows you to capture the whole graph, including the backward pass, ahead of time!\n",
    "\n",
    "TorchInductor actually also makes use of the `fx.graph` to pattern match code, as shown [here](https://github.com/pytorch/pytorch/tree/main/torch/_inductor/fx_passes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a fused kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fused kernels are advantageous as they can reduce both the number of memory accesses and number of kernel launches. This can be particularly useful when you have a lot of small operations that can be fused together.\n",
    "\n",
    "The following example demonstrates how to use a fused kernel to optimize the performance of the scaled dot product attention (SDPA). The two different implementations are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        scale_factor = 1 / math.sqrt(query.size(-1))\n",
    "        score = query @ key.transpose(-2, -1) / scale_factor\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = attn @ value\n",
    "        return context\n",
    "\n",
    "class ScaledDotProductAttentionFused(torch.nn.Module):\n",
    "    def forward(self, query, key, value):\n",
    "        return F.scaled_dot_product_attention(query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first implementation makes use of a number of functions to compute the scaled dot product attention. The second implementation uses a fused kernel provided by Pytorch to compute the scaled dot product attention.\n",
    "\n",
    "Now we can do a very simple test to check their functional equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float16) tensor(1., dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "query = torch.ones(32, 8, 128, 64, dtype=torch.float16, device=device)\n",
    "key = torch.ones(32, 8, 128, 64, dtype=torch.float16, device=device)\n",
    "value = torch.ones(32, 8, 128, 64, dtype=torch.float16, device=device)\n",
    "\n",
    "y1 = ScaledDotProductAttention()(query, key, value)\n",
    "y2 = ScaledDotProductAttentionFused()(query, key, value)\n",
    "print(y1[0,0,0,0], y2[0,0,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
