{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff1bS2_YZ3hB"
      },
      "source": [
        "# Custom Kernel Tutorial\n",
        "\n",
        "## Env Setup in Colab\n",
        "\n",
        "Check if Colab is connected to a NViDIA Tesla T4 GPU, if not, change Colab runtime to this GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBGkK1R2NVyT",
        "outputId": "0235ff29-b1be-4788-c2ba-aac07b6e0566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jan 20 22:26:51 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('❌ Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B66afpunzTQp"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VUhihX3iOYF9",
        "outputId": "0cce8cea-7048-4ba2-f768-22a735938dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tox\n",
            "  Downloading tox-4.23.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting rust-just\n",
            "  Downloading rust_just-1.38.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: cachetools>=5.5 in /usr/local/lib/python3.11/dist-packages (from tox) (5.5.0)\n",
            "Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.11/dist-packages (from tox) (5.2.0)\n",
            "Collecting colorama>=0.4.6 (from tox)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from tox) (3.16.1)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.11/dist-packages (from tox) (24.2)\n",
            "Requirement already satisfied: platformdirs>=4.3.6 in /usr/local/lib/python3.11/dist-packages (from tox) (4.3.6)\n",
            "Requirement already satisfied: pluggy>=1.5 in /usr/local/lib/python3.11/dist-packages (from tox) (1.5.0)\n",
            "Collecting pyproject-api>=1.8 (from tox)\n",
            "  Downloading pyproject_api-1.8.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting virtualenv>=20.26.6 (from tox)\n",
            "  Downloading virtualenv-20.29.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.26.6->tox)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading tox-4.23.2-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.8/166.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rust_just-1.38.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading pyproject_api-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading virtualenv-20.29.1-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv, rust-just, pyproject-api, ninja, colorama, tox\n",
            "Successfully installed colorama-0.4.6 distlib-0.3.9 ninja-1.11.1.3 pyproject-api-1.8.0 rust-just-1.38.0 tox-4.23.2 virtualenv-20.29.1\n",
            "Collecting build\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build) (24.2)\n",
            "Collecting pyproject_hooks (from build)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pyproject_hooks, build\n",
            "Successfully installed build-1.2.2.post1 pyproject_hooks-1.2.0\n",
            "Cloning into 'mase-cuda'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (270/270), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 270 (delta 91), reused 266 (delta 88), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (270/270), 62.99 KiB | 1.21 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Submodule 'submodules/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'submodules/cutlass'\n",
            "Submodule 'submodules/nlohmann_json' (https://github.com/nlohmann/json.git) registered for path 'submodules/nlohmann_json'\n",
            "Cloning into '/content/mase-cuda/submodules/cutlass'...\n",
            "remote: Enumerating objects: 31028, done.        \n",
            "remote: Counting objects: 100% (2163/2163), done.        \n",
            "remote: Compressing objects: 100% (1616/1616), done.        \n",
            "remote: Total 31028 (delta 1138), reused 547 (delta 547), pack-reused 28865 (from 5)        \n",
            "Receiving objects: 100% (31028/31028), 49.79 MiB | 20.47 MiB/s, done.\n",
            "Resolving deltas: 100% (22276/22276), done.\n",
            "Cloning into '/content/mase-cuda/submodules/nlohmann_json'...\n",
            "remote: Enumerating objects: 47223, done.        \n",
            "remote: Counting objects: 100% (567/567), done.        \n",
            "remote: Compressing objects: 100% (207/207), done.        \n",
            "remote: Total 47223 (delta 263), reused 389 (delta 138), pack-reused 46656 (from 3)        \n",
            "Receiving objects: 100% (47223/47223), 200.00 MiB | 23.00 MiB/s, done.\n",
            "Resolving deltas: 100% (28132/28132), done.\n",
            "Submodule path 'submodules/cutlass': checked out '08101d9d0ca68fbdd4ed8833a9fa66dc3948b77d'\n",
            "Submodule path 'submodules/nlohmann_json': checked out 'ac8b22180db393d56f5f2954eb353967fad254e3'\n",
            "/content/mase-cuda\n"
          ]
        }
      ],
      "source": [
        "! pip install tox ninja torch numpy scipy rust-just pytest transformers\n",
        "! pip install -U build\n",
        "\n",
        "git_token = \"YOUR_GIT_TOKEN\"\n",
        "! git clone --recurse-submodules https://{git_token}@github.com/DeepWok/mase-cuda.git\n",
        "%cd mase-cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJ6j3wRaoc2"
      },
      "source": [
        "## Build & Run C++ Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fIMUKJWLJo10",
        "outputId": "683256bd-d09c-4d29-d234-eebc1185b02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m# python\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/dist ]; then rm -r /content/mase-cuda/dist; fi\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/src/mase_cuda.egg-info ]; then rm -r /content/mase-cuda/src/mase_cuda.egg-info; fi\u001b[0m\n",
            "\u001b[1m# all\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/build ]; then rm -r /content/mase-cuda/build; fi\u001b[0m\n",
            "\u001b[1mecho $(which cmake)\u001b[0m\n",
            "/usr/local/bin/cmake\n",
            "\u001b[1mcmake -D BUILD_TESTING=ON -D CUDA_ARCHITECTURES=native -B build -S .\u001b[0m\n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140 with host compiler GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- CUDA_ARCHITECTURES: native\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.11.11\") found components: Interpreter Development Development.Module Development.Embed\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.2\") \n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.2.140\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Caffe2: CUDA detected: 12.2\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.2\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.11.11\") found components: Interpreter\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\n",
            "  Failed to compute shorthash for libnvrtc.so\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/lib/python3.11/dist-packages/cmake/data/share/cmake-3.31/Modules/FindPackageHandleStandardArgs.cmake:441 (message):\n",
            "  The package name passed to `find_package_handle_standard_args` (nvtx3) does\n",
            "  not match the name of the calling package (Caffe2).  This can lead to\n",
            "  problems in calling code that expects `find_package` result variables\n",
            "  (e.g., `_FOUND`) to follow a certain pattern.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could NOT find nvtx3 (missing: nvtx3_dir) \n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\n",
            "  Cannot find NVTX3, find old NVTX instead\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
            "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
            "-- USE_CUDSS is set to 0. Compiling without cuDSS support\n",
            "-- USE_CUFILE is set to 0. Compiling without cuFile support\n",
            "-- Autodetected CUDA architecture(s):  7.5\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
            "  static library kineto_LIBRARY-NOTFOUND not found.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch: /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so\n",
            "-- Configuring done (10.1s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/mase-cuda/build\n",
            "\u001b[1mif [ -z  ]; then cmake --build /content/mase-cuda/build -j 8 ; else cmake --build /content/mase-cuda/build --target  -j 8 ; fi\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CUDA object test/cu/mxint/dequantize/CMakeFiles/test_mxint8_dequantize1d.dir/test_mxint8_dequantize1d.cu.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CUDA executable test_mxint8_dequantize1d\u001b[0m\n",
            "[100%] Built target test_mxint8_dequantize1d\n"
          ]
        }
      ],
      "source": [
        "!just build-cu-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BajynrmayrY"
      },
      "source": [
        "Run test executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIxj1uQGZQcj",
        "outputId": "93ab35d7-3ee5-442f-c336-ceef7e40b927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage: ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d [m] [group_size] [is_random]\n",
            "m=4096, group_size=128, num_groups=32, is_random=0\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "! ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdO3h_06a99Q"
      },
      "source": [
        "## Build & Try mase-cuda Package\n",
        "\n",
        "The building process can be slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v01rAwvkZaf6",
        "outputId": "641143dd-dd63-49df-973f-696b1fd5bb90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m* Creating isolated environment: venv+pip...\u001b[0m\n",
            "\u001b[1m* Installing packages in isolated environment:\u001b[0m\n",
            "  - numpy\n",
            "  - setuptools\n",
            "  - torch\n",
            "\u001b[1m* Getting build dependencies for wheel...\u001b[0m\n",
            "running egg_info\n",
            "creating src/mase_cuda.egg-info\n",
            "writing src/mase_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to src/mase_cuda.egg-info/dependency_links.txt\n",
            "writing requirements to src/mase_cuda.egg-info/requires.txt\n",
            "writing top-level names to src/mase_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "\u001b[1m* Building wheel...\u001b[0m\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/__init__.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/constants.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/utils.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/quantize.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/dequantize.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/__init__.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/linear.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda/tools\n",
            "copying src/mase_cuda/tools/bin_repr.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/tools\n",
            "running egg_info\n",
            "writing src/mase_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to src/mase_cuda.egg-info/dependency_links.txt\n",
            "writing requirements to src/mase_cuda.egg-info/requires.txt\n",
            "writing top-level names to src/mase_cuda.egg-info/top_level.txt\n",
            "reading manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "creating build/lib.linux-x86_64-cpython-311/csrc\n",
            "copying src/csrc/bind.cu -> build/lib.linux-x86_64-cpython-311/csrc\n",
            "running build_ext\n",
            "/tmp/build-env-8v6ulgqx/lib/python3.11/site-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/tmp/build-env-8v6ulgqx/lib/python3.11/site-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'mase_cuda_ext' extension\n",
            "creating /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc\n",
            "Emitting ninja build file /content/mase-cuda/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "Compiling objects...\n",
            "Using envvar MAX_JOBS (8) as the number of workers...\n",
            "[1/1] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o.d -I/content/mase-cuda/submodules/cutlass/include -I/content/mase-cuda/submodules/cutlass/tools/util/include -I/tmp/build-env-8v6ulgqx/lib/python3.11/site-packages/torch/include -I/tmp/build-env-8v6ulgqx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/build-env-8v6ulgqx/lib/python3.11/site-packages/torch/include/TH -I/tmp/build-env-8v6ulgqx/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda/include -I/tmp/build-env-8v6ulgqx/include -I/usr/include/python3.11 -c -c /content/mase-cuda/src/csrc/bind.cu -o /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DNDEBUG -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=mase_cuda_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75\n",
            "x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o -L/tmp/build-env-8v6ulgqx/lib/python3.11/site-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lcuda -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/mase_cuda_ext.cpython-311-x86_64-linux-gnu.so\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/quantize.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/dequantize.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/__init__.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/linear.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/__init__.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda/tools\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/tools/bin_repr.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/tools\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/constants.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/utils.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/csrc\n",
            "copying build/lib.linux-x86_64-cpython-311/csrc/bind.cu -> build/bdist.linux-x86_64/wheel/./csrc\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda_ext.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "running install_egg_info\n",
            "Copying src/mase_cuda.egg-info to build/bdist.linux-x86_64/wheel/./mase_cuda-0.0.1-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda-0.0.1.dist-info/WHEEL\n",
            "creating '/content/mase-cuda/dist/.tmp-frq7obcz/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'mase_cuda_ext.cpython-311-x86_64-linux-gnu.so'\n",
            "adding 'csrc/bind.cu'\n",
            "adding 'mase_cuda/__init__.py'\n",
            "adding 'mase_cuda/constants.py'\n",
            "adding 'mase_cuda/utils.py'\n",
            "adding 'mase_cuda/mxint8/__init__.py'\n",
            "adding 'mase_cuda/mxint8/dequantize.py'\n",
            "adding 'mase_cuda/mxint8/linear.py'\n",
            "adding 'mase_cuda/mxint8/quantize.py'\n",
            "adding 'mase_cuda/tools/bin_repr.py'\n",
            "adding 'mase_cuda-0.0.1.dist-info/METADATA'\n",
            "adding 'mase_cuda-0.0.1.dist-info/WHEEL'\n",
            "adding 'mase_cuda-0.0.1.dist-info/top_level.txt'\n",
            "adding 'mase_cuda-0.0.1.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[1m\u001b[92mSuccessfully built \u001b[4mmase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl\u001b[0m\u001b[1m\u001b[92m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! TORCH_CUDA_ARCH_LIST=\"7.5\" MAX_JOBS=$(nproc --all) python -m build --wheel ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YjEO4j4zrNs"
      },
      "source": [
        "Install the mase-cuda wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CGJhk1mcOPaS",
        "outputId": "10d1b926-02ad-4f1e-b965-a7ff29189904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ./dist/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mase-cuda==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mase-cuda==0.0.1) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mase-cuda==0.0.1) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mase-cuda==0.0.1) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mase-cuda==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mase-cuda==0.0.1) (3.0.2)\n",
            "Installing collected packages: mase-cuda\n",
            "Successfully installed mase-cuda-0.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install ./dist/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_1PsZ4qzybk"
      },
      "source": [
        "Profile dequantization latency (CPU vs GPU). This is slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSWT8qbcwBu8",
        "outputId": "29eab330-5fc0-43c1-e733-ea01a96da5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/mase-cuda\n",
            "configfile: tox.ini\n",
            "plugins: typeguard-4.4.1, anyio-3.7.1\n",
            "collected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency \n",
            "\u001b[1m------------------------------------------ live log call -------------------------------------------\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m test_dequantize1d:test_dequantize1d.py:203 \n",
            "+-----------+------------+------------------------+------------------------+---------------------+\n",
            "|     m     | group_size |      latency_cpu       |      latency_gpu       |     GPU speedup     |\n",
            "+-----------+------------+------------------------+------------------------+---------------------+\n",
            "|   1024    |     8      | 0.00033932924270629883 | 7.334719887003302e-05  |  4.626342218024857  |\n",
            "|   1024    |     16     | 1.4746189117431641e-05 | 2.7062400057911873e-05 | 0.5448958365065811  |\n",
            "|   1024    |     32     | 9.465217590332032e-06  | 2.721760021522641e-05  | 0.34776091629991984 |\n",
            "|   1024    |     64     | 9.655952453613281e-06  | 2.572159990668297e-05  | 0.3754024822967749  |\n",
            "|   1024    |    128     | 9.310245513916015e-06  | 2.440159972757101e-05  | 0.38154242417953055 |\n",
            "|   1024    |    256     | 9.274482727050781e-06  | 2.5841599982231854e-05 | 0.35889738767830637 |\n",
            "|   1024    |    512     | 1.1217594146728515e-05 | 2.6467199716717006e-05 | 0.4238300336564637  |\n",
            "|  2097152  |     8      |  0.022749853134155274  | 0.0001553824007511139  | 146.41203266382269  |\n",
            "|  2097152  |     16     |  0.020895063877105713  | 0.00015470880120992662 |  135.0606023296173  |\n",
            "|  2097152  |     32     |  0.019620335102081297  | 0.00015356480032205583 |  127.7658360570493  |\n",
            "|  2097152  |     64     |  0.02061176300048828   | 0.00015346399843692778 | 134.31008712417665  |\n",
            "|  2097152  |    128     |  0.018138813972473144  | 0.00015343520119786265 |  118.2180740199389  |\n",
            "|  2097152  |    256     |  0.019102632999420166  | 0.00015405760034918785 | 123.99669315971447  |\n",
            "|  2097152  |    512     |  0.01792299747467041   | 0.00015459040105342864 |  115.9386181324154  |\n",
            "| 16777216  |     8      |  0.17930004596710206   | 0.0022754432141780852  |  78.79785566605193  |\n",
            "| 16777216  |     16     |  0.17303448915481567   | 0.0015832624018192291  | 109.28983657793707  |\n",
            "| 16777216  |     32     |   0.1565511107444763   | 0.0015850912153720855  |  98.76473304896045  |\n",
            "| 16777216  |     64     |  0.15572370290756227   | 0.0011739712059497833  | 132.64695259844675  |\n",
            "| 16777216  |    128     |  0.16187801361083984   | 0.0011729903995990755  | 138.00455115930126  |\n",
            "| 16777216  |    256     |  0.16296688318252564   |  0.001174132800102234  | 138.79765829583826  |\n",
            "| 16777216  |    512     |  0.16297812461853028   | 0.0011730975985527038  | 138.92972317018015  |\n",
            "| 58720256  |     8      |   0.5400641202926636   |  0.005536414456367493  |  97.54763205480936  |\n",
            "| 58720256  |     16     |   0.5767450928688049   |  0.005567404747009277  | 103.59316756674164  |\n",
            "| 58720256  |     32     |   0.5682603597640992   |  0.005591809558868408  | 101.62369690556767  |\n",
            "| 58720256  |     64     |   0.5779216408729553   |  0.005634153628349304  | 102.57470402742193  |\n",
            "| 58720256  |    128     |   0.6114006638526917   |  0.004100708770751953  | 149.09633871428966  |\n",
            "| 58720256  |    256     |   0.6050710678100586   | 0.0041026783943176265  | 147.48196413545506  |\n",
            "| 58720256  |    512     |   0.6021418809890747   |  0.004105388736724853  | 146.67109976762984  |\n",
            "| 234881024 |     8      |   2.2362759590148924   |  0.01719512619972229   | 130.05289597996725  |\n",
            "| 234881024 |     16     |   2.2424280881881713   |  0.018469788742065427  | 121.41059757121003  |\n",
            "| 234881024 |     32     |   2.3151396751403808   |  0.01741295509338379   |  132.9550132487299  |\n",
            "| 234881024 |     64     |   2.380437362194061    |  0.018501527881622317  | 128.66166391363626  |\n",
            "| 234881024 |    128     |   2.476264750957489    |  0.01801117272377014   | 137.48492610308784  |\n",
            "| 234881024 |    256     |   2.5159724950790405   |  0.01852295684814453   |  135.8299603948529  |\n",
            "| 234881024 |    512     |   2.3864796996116637   |  0.012716859292984007  | 187.66266454865186  |\n",
            "+-----------+------------+------------------------+------------------------+---------------------+\n",
            "\u001b[32mPASSED\u001b[0m\u001b[32m                                                                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 474.22s (0:07:54)\u001b[0m\u001b[32m ===================================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pytest -v --log-cli-level INFO test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90MO9ajpz54_"
      },
      "source": [
        "### FP32 Deberta Demo\n",
        "Create the Deberta demo file and run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kvKtObTMw483"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > demo.py <<- EOF\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
        "# if you meet OOM error, try this smaller model, but the quantization effect may not be obvious later\n",
        "# model_name = \"AnkitAI/deberta-v3-small-base-emotions-classifier\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).cuda().eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
        "\n",
        "\n",
        "# Example usage\n",
        "@torch.no_grad()\n",
        "def predict_emotion(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = logits.argmax(dim=1)\n",
        "    predictions = label2emotion[predictions.item()]\n",
        "    top3_values, top3_indices = torch.topk(logits, 3)\n",
        "    top3_values = top3_values.cpu().tolist()\n",
        "    top3_indices = top3_indices.cpu().tolist()\n",
        "    return predictions, (top3_values, top3_indices)\n",
        "\n",
        "\n",
        "text = \"I'm so happy with the results!\"\n",
        "emotion, top3 = predict_emotion(model, tokenizer, text)\n",
        "\n",
        "print(\"Index to Emotion Mapping:\", label2emotion)\n",
        "print(\"Input text:\", text)\n",
        "print(\"Detected Emotion:\", emotion)\n",
        "print(f\"top3 logits: {top3[0]}, top3 indices: {top3[1]}\")\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4JU2y1wyZ9V",
        "outputId": "1185112c-46ca-431f-ce9c-51a3c2173413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rconfig.json:   0% 0.00/1.12k [00:00<?, ?B/s]\rconfig.json: 100% 1.12k/1.12k [00:00<00:00, 8.49MB/s]\n",
            "2025-01-20 22:40:44.039772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-20 22:40:44.060592: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-20 22:40:44.067121: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-20 22:40:44.082384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-20 22:40:45.611828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "model.safetensors: 100% 3.04G/3.04G [01:25<00:00, 35.4MB/s]\n",
            "tokenizer_config.json: 100% 1.31k/1.31k [00:00<00:00, 10.3MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 1.53MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.32MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 3.00MB/s]\n",
            "special_tokens_map.json: 100% 969/969 [00:00<00:00, 8.14MB/s]\n",
            "Index to Emotion Mapping: {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
            "Input text: I'm so happy with the results!\n",
            "Detected Emotion: joy\n",
            "top3 logits: [[7.3452301025390625, -1.4850208759307861, -1.6403967142105103]], top3 indices: [[1, 4, 0]]\n"
          ]
        }
      ],
      "source": [
        "! python demo.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XrSk9bfz_nZ"
      },
      "source": [
        "### MXINT8 Deberta\n",
        "Create the quantized Deberta and compare GPU memory usage with FP32 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hnd5jocy1x9",
        "outputId": "61565d8f-48db-4711-9f6e-65f216280934"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "bash: line 80: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "bash: line 1: new_layer: command not found\n",
            "bash: line 1: module: command not found\n",
            "bash: line 1: name: command not found\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cat > demo-q.py <<- EOF\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "from mase_cuda.mxint8.linear import QLinearPacked\n",
        "\n",
        "init_memory = torch.cuda.memory_allocated()  # in bytes\n",
        "model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float32).cuda().eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
        "\n",
        "mxint8_group_size = 32\n",
        "assert model.config.hidden_size % mxint8_group_size == 0\n",
        "assert model.config.intermediate_size % mxint8_group_size == 0\n",
        "\n",
        "text = \"I'm so happy with the results!\"\n",
        "\n",
        "\n",
        "# Example usage\n",
        "@torch.no_grad()\n",
        "def predict_emotion(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = logits.argmax(dim=1)\n",
        "    predictions = label2emotion[predictions.item()]\n",
        "    top3_values, top3_indices = torch.topk(logits, 3)\n",
        "    top3_values = top3_values.cpu().tolist()\n",
        "    top3_indices = top3_indices.cpu().tolist()\n",
        "    return predictions, (top3_values, top3_indices)\n",
        "\n",
        "\n",
        "# check the GPU memory usage of FP32 model\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "emotion_fp32, top3_fp32 = predict_emotion(model, tokenizer, text)\n",
        "peak_memory_fp32 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
        "\n",
        "\n",
        "def set_layer_by_name(module: torch.nn.Module, name: str, new_layer: torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Replace a layer (`new_layer`) in a model (`module`) by its `name`.\n",
        "    \"\"\"\n",
        "    levels = name.split(\".\")\n",
        "    if len(levels) > 1:\n",
        "        mod_ = module\n",
        "        for l_idx in range(len(levels) - 1):\n",
        "            if levels[l_idx].isdigit():\n",
        "                mod_ = mod_[int(levels[l_idx])]\n",
        "            else:\n",
        "                mod_ = getattr(mod_, levels[l_idx])\n",
        "        setattr(mod_, levels[-1], new_layer)\n",
        "    else:\n",
        "        setattr(module, name, new_layer)\n",
        "\n",
        "\n",
        "for layer_name, layer in model.named_modules():\n",
        "    if not isinstance(layer, torch.nn.Linear):\n",
        "        continue\n",
        "    if \"classifier\" in layer_name:\n",
        "        continue\n",
        "    layer.cuda()\n",
        "    layer_q = QLinearPacked.build_from_linear(layer, group_size=mxint8_group_size)\n",
        "    set_layer_by_name(model, layer_name, layer_q)\n",
        "    del layer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "emotion_mxint8, top3_mxint8 = predict_emotion(model, tokenizer, text)\n",
        "peak_memory_mxint8 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
        "\n",
        "print(f\"FP32 model peak memory: {peak_memory_fp32/1024**2:.4f} MB\")\n",
        "print(f\"PF32 prediction: {emotion_fp32}\")\n",
        "print(f\"FP32 top3 logits: {top3_fp32[0]}, indices: {top3_fp32[1]}\")\n",
        "\n",
        "print(f\"MXINT8 model peak memory: {peak_memory_mxint8/1024**2:.4f} MB\")\n",
        "print(f\"MXINT8 prediction: {emotion_mxint8}\")\n",
        "print(f\"MXINT8 top3 logits: {top3_mxint8[0]}, indices: {top3_mxint8[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGCCrlvbzDug",
        "outputId": "beed37b4-abc7-4f1d-fb0e-c015202789c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-01-20 22:42:28.139444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-20 22:42:28.160214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-20 22:42:28.166695: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-20 22:42:28.181280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-20 22:42:29.664303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "FP32 model peak memory: 2906.1997 MB\n",
            "PF32 prediction: joy\n",
            "FP32 top3 logits: [[7.3452301025390625, -1.4850208759307861, -1.6403967142105103]], indices: [[1, 4, 0]]\n",
            "MXINT8 model peak memory: 976.1616 MB\n",
            "MXINT8 prediction: joy\n",
            "MXINT8 top3 logits: [[7.350157737731934, -1.4883261919021606, -1.649757981300354]], indices: [[1, 4, 0]]\n"
          ]
        }
      ],
      "source": [
        "! python demo-q.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
