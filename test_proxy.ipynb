{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ansonhon/mase_project/machop\n"
     ]
    }
   ],
   "source": [
    "%cd machop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.predictors.utils.pruners.predictive import find_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 128)  # Corrected size\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 32 * 5 * 5)  # Corrected size\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize the network\n",
    "net = SimpleNet()\n",
    "\n",
    "# Create some random input data and targets\n",
    "inputs = torch.randn(64, 1, 28, 28)  # Assuming MNIST-like data\n",
    "targets = torch.randint(0, 10, (64,))  # 10 classes\n",
    "\n",
    "# Define a loss function\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "# Call the function with the provided code\n",
    "result = compute_epe_score(net, inputs, targets, loss_fn, split_data=1)\n",
    "# print(\"length of result: \", len(result))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import types\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Function to generate some dummy data\n",
    "def get_some_data(trainloader, num_batches, device):\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "    return images.to(device), labels.to(device)\n",
    "\n",
    "# Function to simulate a data loader\n",
    "class DummyDataLoader:\n",
    "    def __init__(self, data_shape):\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Simulate loading data\n",
    "        return torch.randn(*self.data_shape), torch.randint(0, 10, (self.data_shape[0],))\n",
    "\n",
    "# Define dummy parameters for the test case\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainloader = DummyDataLoader((4, 3, 32, 32))  # Dummy data loader with batch size 4 and 3-channel images\n",
    "dataload_info = (\"random\", 1, 10)  # Random data, 1 batch, 10 classes\n",
    "\n",
    "# Call the function with the provided code\n",
    "measure_values = find_measures_arrays(SimpleNet(), trainloader, dataload_info, device)\n",
    "\n",
    "# Print the results\n",
    "print(measure_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def forward_before_global_avg_pool(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "# Call the function with the provided code\n",
    "measure_values = find_measures(SimpleNet(), trainloader, dataload_info, device)\n",
    "\n",
    "# Print the results\n",
    "print(measure_values.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonhon/miniconda3/envs/mase/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flops': 0.659624, 'params': 0.062006, 'epe_nas': 7.356238641582316, 'fisher': 0.0014616213738918304, 'grad_norm': 0.4683944582939148, 'grasp': -0.20232538878917694, 'jacov': -4.012511299971813, 'l2_norm': 17.07454490661621, 'nwot': -inf, 'plain': 0.0012587300734594464, 'snip': 0.8029875159263611, 'synflow': 321288.7766803133, 'zen': 5.562592029571533}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def forward_before_global_avg_pool(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "# Dummy data loader class\n",
    "class DummyDataLoader:\n",
    "    def __init__(self, data_shape):\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Simulate loading data\n",
    "        return torch.randn(*self.data_shape), torch.randint(0, 10, (self.data_shape[0],))\n",
    "\n",
    "# Dummy loss function\n",
    "def dummy_loss_fn(output, target):\n",
    "    # Assuming output has shape [batch_size, num_classes] and target has shape [batch_size]\n",
    "    # Convert target to one-hot encoding to match the shape of output\n",
    "    target_one_hot = F.one_hot(target, num_classes=output.shape[1])\n",
    "    # Calculate mean squared error\n",
    "    return F.mse_loss(output, target_one_hot.float())\n",
    "\n",
    "# Define dummy parameters for the test case\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloader = DummyDataLoader((4, 3, 32, 32))  # Dummy data loader with batch size 4 and 3-channel images\n",
    "dataload_info = (\"random\", 1, 10)  # Random data, 1 batch, 10 \n",
    "# dataload_info = None\n",
    "measure_names = ['params']  # Only compute parameters count for this test\n",
    "\n",
    "# Call the function with the provided code\n",
    "measure_values = find_measures(SimpleNet(), dataloader, dataload_info, device, dummy_loss_fn, measure_names = ['epe_nas', 'fisher', 'grad_norm', 'grasp', 'jacov', 'l2_norm', 'nwot', 'plain', 'snip', 'synflow', 'zen', 'params', 'flops'])\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(measure_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralModel(len(measure_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_path = r'/home/ansonhon/mase_project/nas_results/model_state_dict.pt'\n",
    "model.load_state_dict(torch.load(pretrained_model_path))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.356238641582316,\n",
       " 0.0014616213738918304,\n",
       " 0.4683944582939148,\n",
       " -0.20232538878917694,\n",
       " -4.012511299971813,\n",
       " 17.07454490661621,\n",
       " -inf,\n",
       " 0.0012587300734594464,\n",
       " 0.8029875159263611,\n",
       " 321288.7766803133,\n",
       " 5.562592029571533,\n",
       " 0.062006,\n",
       " 0.659624]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_names = ['epe_nas', 'fisher', 'grad_norm', 'grasp', 'jacov', 'l2_norm', 'nwot', 'plain', 'snip', 'synflow', 'zen', 'params', 'flops']\n",
    "measure_values_list = [measure_values[s] for s in measure_names]\n",
    "measure_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_values_tensor = torch.tensor(measure_values_list, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7413184]\n"
     ]
    }
   ],
   "source": [
    "### Make prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(measure_values_tensor)\n",
    "\n",
    "prediction_numpy = prediction.numpy()\n",
    "print(prediction_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%./ch search --config ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2401)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,1,1]\n",
    "b = [2,2,2]\n",
    "a = torch.tensor([0.99,0.99,0.99])\n",
    "b = torch.tensor([0.5,0.5,0.5])\n",
    "loss = nn.MSELoss()\n",
    "loss(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Loss: 0.3633333444595337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonhon/miniconda3/envs/mase/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example ground truth and predicted values\n",
    "predictions = torch.tensor([1])  # Example predicted values\n",
    "targets = torch.tensor([0.4, 0.7, 0.2])  # Example ground truth values\n",
    "\n",
    "# Calculate the mean squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(predictions, targets)\n",
    "\n",
    "print(\"Mean Squared Error Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
