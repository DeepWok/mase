{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import types\n",
    "from ..p_utils import get_layer_metric_array, reshape_element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_forward_conv2d(self, x):\n",
    "    x = F.conv2d(\n",
    "        x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "    )\n",
    "    # intercept and store the activations after passing through 'hooked' identity op\n",
    "    self.act = self.dummy(x)\n",
    "    return self.act\n",
    "\n",
    "\n",
    "def fisher_forward_linear(self, x):\n",
    "    x = F.linear(x, self.weight, self.bias)\n",
    "    self.act = self.dummy(x)\n",
    "    return self.act\n",
    "\n",
    "\n",
    "# @measure(\"fisher\", bn=True, mode=\"channel\")\n",
    "def compute_fisher_per_weight(net, inputs, targets, loss_fn, mode, split_data=1):\n",
    "\n",
    "    device = inputs.device\n",
    "\n",
    "    if mode == \"param\":\n",
    "        raise ValueError(\"Fisher pruning does not support parameter pruning.\")\n",
    "\n",
    "    net.train()\n",
    "    all_hooks = []\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            # variables/op needed for fisher computation\n",
    "            layer.fisher = None\n",
    "            layer.act = 0.0\n",
    "            layer.dummy = nn.Identity()\n",
    "\n",
    "            # replace forward method of conv/linear\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                layer.forward = types.MethodType(fisher_forward_conv2d, layer)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.forward = types.MethodType(fisher_forward_linear, layer)\n",
    "\n",
    "            # function to call during backward pass (hooked on identity op at output of layer)\n",
    "            def hook_factory(layer):\n",
    "                def hook(module, grad_input, grad_output):\n",
    "                    act = layer.act.detach()\n",
    "                    grad = grad_output[0].detach()\n",
    "                    if len(act.shape) > 2:\n",
    "                        g_nk = torch.sum((act * grad), list(range(2, len(act.shape))))\n",
    "                    else:\n",
    "                        g_nk = act * grad\n",
    "                    del_k = g_nk.pow(2).mean(0).mul(0.5)\n",
    "                    if layer.fisher is None:\n",
    "                        layer.fisher = del_k\n",
    "                    else:\n",
    "                        layer.fisher += del_k\n",
    "                    del (\n",
    "                        layer.act\n",
    "                    )  # without deleting this, a nasty memory leak occurs! related: https://discuss.pytorch.org/t/memory-leak-when-using-forward-hook-and-backward-hook-simultaneously/27555\n",
    "\n",
    "                return hook\n",
    "\n",
    "            # register backward hook on identity fcn to compute fisher info\n",
    "            layer.dummy.register_backward_hook(hook_factory(layer))\n",
    "\n",
    "    N = inputs.shape[0]\n",
    "    for sp in range(split_data):\n",
    "        st = sp * N // split_data\n",
    "        en = (sp + 1) * N // split_data\n",
    "\n",
    "        net.zero_grad()\n",
    "        outputs = net(inputs[st:en])\n",
    "        loss = loss_fn(outputs, targets[st:en])\n",
    "        loss.backward()\n",
    "\n",
    "    # retrieve fisher info\n",
    "    def fisher(layer):\n",
    "        if layer.fisher is not None:\n",
    "            return torch.abs(layer.fisher.detach())\n",
    "        else:\n",
    "            return torch.zeros(layer.weight.shape[0])  # size=ch\n",
    "\n",
    "    grads_abs_ch = get_layer_metric_array(net, fisher, mode)\n",
    "\n",
    "    # broadcast channel value here to all parameters in that channel\n",
    "    # to be compatible with stuff downstream (which expects per-parameter metrics)\n",
    "    # TODO cleanup on the selectors/apply_prune_mask side (?)\n",
    "    shapes = get_layer_metric_array(net, lambda l: l.weight.shape[1:], mode)\n",
    "\n",
    "    grads_abs = reshape_elements(grads_abs_ch, shapes, device)\n",
    "\n",
    "    return grads_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ansonhon/mase_project/machop/naslib/predictors/utils/pruners/measures'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define dummy model\n",
    "class NeuralModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(13, 64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))  # 输出层的激活函数使用sigmoid\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
