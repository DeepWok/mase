# basics
model = "toy"
dataset = "toy-tiny"

# training
training_optimizer = "adamw"
learning_rate = 1e-5
max_epochs = 10
batch_size = 32
# torch lightning
task = "classification"
num_workers = 0
num_devices = 1
accelerator = "cpu"

[passes]

[passes.quantize]

[passes.quantize.default.config]
name = "block_fp"
weight_width = 7
weight_exponent_width = 8
weight_exponent_bias = 255 
weight_block_size = 8
data_in_width = 7
data_in_exponent_width = 8
data_in_exponent_bias = 255
data_in_block_size = 8
bias_width = 7
bias_exponent_width = 8
bias_exponent_bias = 255
bias_block_size = 8


# [passes.quantize.linear]
# [passes.quantize.linear.config]
# name = "block_fp"
# weight_width = 7
# weight_exponent_width = 8
# weight_exponent_bias = 255 
# weight_block_size = 8
# data_in_width = 7
# data_in_exponent_width = 8
# data_in_exponent_bias = 255
# data_in_block_size = 8
# bias_width = 7
# bias_exponent_width = 8
# bias_exponent_bias = 255
# bias_block_size = 8

[passes.quantize.add]
[passes.quantize.add.config]
name = "block_fp"
width = 7
block_size = 8

[passes.quantize.seq_blocks_0]
[passes.quantize.seq_blocks_0.config]
name = "integer"
weight_width = 12 
weight_frac_width =10
bias_width = 12 
bias_frac_width =10
data_in_width = 12
data_in_frac_width =10
block_size = 8