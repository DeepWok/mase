{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install basic dependencies for running NasBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# For meta-proxy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from einops import rearrange\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ansonhon/mase_project/machop/chop/NASLib\n"
     ]
    }
   ],
   "source": [
    "%cd machop/chop/NASLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e . --quiet\n",
    "%pip install ConfigSpace==0.6 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = cifar10\n",
      "search_space = nb201\n",
      "cifar10 exists\n"
     ]
    }
   ],
   "source": [
    "# For running cifar10\n",
    "!source scripts/bash_scripts/download_benchmarks.sh nb201 cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code for generating list for random sampling architecture from NASBench\n",
    "big_list = []\n",
    "while len(big_list) < 200:\n",
    "    small_rand_list = [int(np.random.rand()*5) for _ in range(6)]\n",
    "    if small_rand_list not in big_list:\n",
    "        big_list.append(small_rand_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%./ch` not found.\n"
     ]
    }
   ],
   "source": [
    "# After copying the list for specifying architectures into config file, we ran the following command:\n",
    "%./ch search --config configs/examples/search_nas.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(r'/home/ansonhon/mase_project/nas_results/scores_test2.npy',allow_pickle=True)\n",
    "val_accuracies = np.load(r'/home/ansonhon/mase_project/nas_results/val_acc_test2.npy', allow_pickle=True)\n",
    "val_accuracies /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(r'/home/ansonhon/mase_project/nas_results/scores_test3.npy',allow_pickle=True)\n",
    "val_accuracies = np.load(r'/home/ansonhon/mase_project/nas_results/val_acc_test3.npy',allow_pickle=True)\n",
    "val_accuracies /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = ['epe_nas', \n",
    "        'fisher', \n",
    "        'grad_norm', \n",
    "        'grasp', \n",
    "        'jacov', \n",
    "        'l2_norm', \n",
    "        'nwot', \n",
    "        'plain', \n",
    "        'snip', \n",
    "        'synflow', \n",
    "        'zen', \n",
    "        'flops', \n",
    "        'params']\n",
    "name = proxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Neural Network for meta-proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data into format for pytorch\n",
    "data_set = []\n",
    "for proxy_name in proxy:\n",
    "    data_set.append(scores.item().get(proxy_name))\n",
    "data_set = np.array(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = rearrange(data_set, 'a b -> b a')\n",
    "labels = val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "features = torch.Tensor(features)\n",
    "labels = torch.Tensor(labels)\n",
    "dataset = TensorDataset(features, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class neural_model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(neural_model, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, input_size)\n",
    "        self.linear2 = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(13, 64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))  # 输出层的激活函数使用sigmoid\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = NeuralModel()\n",
    "# try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(proxy)\n",
    "# model = neural_model(input_size)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaging proxies gives:  tensor(98.0233)\n",
      "average accruracy\n",
      "Prediction gives:  tensor([0.8495])\n",
      "actual accuracy:  0.8847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22320/4139946056.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_data = torch.tensor(features[i], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "i = int(np.random.rand()*200)\n",
    "with torch.no_grad():\n",
    "    new_data = torch.tensor(features[i], dtype=torch.float32)\n",
    "    prediction = model(features[i])\n",
    "    print(\"averaging proxies gives: \", torch.mean(features[i]))\n",
    "    print('average accruracy: ', )\n",
    "    print(\"Prediction gives: \", prediction)\n",
    "    print(\"actual accuracy: \", val_accuracies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(76.7071)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(features[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
