{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install basic dependencies for running NasBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# For meta-proxy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from einops import rearrange\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ansonhon/mase_project/machop/chop/NASLib\n"
     ]
    }
   ],
   "source": [
    "%cd machop/chop/NASLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e . --quiet\n",
    "%pip install ConfigSpace==0.6 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = cifar10\n",
      "search_space = nb201\n",
      "cifar10 exists\n"
     ]
    }
   ],
   "source": [
    "# For running cifar10\n",
    "!source scripts/bash_scripts/download_benchmarks.sh nb201 cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code for generating list for random sampling architecture from NASBench\n",
    "big_list = []\n",
    "while len(big_list) < 200:\n",
    "    small_rand_list = [int(np.random.rand()*5) for _ in range(6)]\n",
    "    if small_rand_list not in big_list:\n",
    "        big_list.append(small_rand_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%./ch` not found.\n"
     ]
    }
   ],
   "source": [
    "# After copying the list for specifying architectures into config file, we ran the following command:\n",
    "%./ch search --config configs/examples/search_nas.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(r'/home/ansonhon/mase_project/nas_results/scores_test2.npy',allow_pickle=True)\n",
    "val_accuracies = np.load(r'/home/ansonhon/mase_project/nas_results/val_acc_test2.npy', allow_pickle=True)\n",
    "val_accuracies /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(r'/home/ansonhon/mase_project/nas_results/scores_test3.npy',allow_pickle=True)\n",
    "val_accuracies = np.load(r'/home/ansonhon/mase_project/nas_results/val_acc_test3.npy',allow_pickle=True)\n",
    "val_accuracies /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = ['epe_nas', \n",
    "        'fisher', \n",
    "        'grad_norm', \n",
    "        'grasp', \n",
    "        'jacov', \n",
    "        'l2_norm', \n",
    "        'nwot', \n",
    "        'plain', \n",
    "        'snip', \n",
    "        'synflow', \n",
    "        'zen', \n",
    "        'flops', \n",
    "        'params']\n",
    "name = proxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Neural Network for meta-proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data into format for pytorch\n",
    "data_set = []\n",
    "for proxy_name in proxy:\n",
    "    data_set.append(scores.item().get(proxy_name))\n",
    "data_set = np.array(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = rearrange(data_set, 'a b -> b a')\n",
    "labels = val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "features = torch.Tensor(features)\n",
    "labels = torch.Tensor(labels)\n",
    "dataset = TensorDataset(features, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class neural_model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(neural_model, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, input_size)\n",
    "        self.linear2 = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(13, 64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))  # 输出层的激活函数使用sigmoid\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = NeuralModel()\n",
    "# try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(proxy)\n",
    "# model = neural_model(input_size)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.0162\n",
      "Epoch [20/1000], Loss: 0.0210\n",
      "Epoch [30/1000], Loss: 0.0513\n",
      "Epoch [40/1000], Loss: 0.0299\n",
      "Epoch [50/1000], Loss: 0.0208\n",
      "Epoch [60/1000], Loss: 0.0093\n",
      "Epoch [70/1000], Loss: 0.0091\n",
      "Epoch [80/1000], Loss: 0.0142\n",
      "Epoch [90/1000], Loss: 0.0521\n",
      "Epoch [100/1000], Loss: 0.0789\n",
      "Epoch [110/1000], Loss: 0.1117\n",
      "Epoch [120/1000], Loss: 0.0468\n",
      "Epoch [130/1000], Loss: 0.0492\n",
      "Epoch [140/1000], Loss: 0.0194\n",
      "Epoch [150/1000], Loss: 0.0141\n",
      "Epoch [160/1000], Loss: 0.0680\n",
      "Epoch [170/1000], Loss: 0.0147\n",
      "Epoch [180/1000], Loss: 0.0237\n",
      "Epoch [190/1000], Loss: 0.0535\n",
      "Epoch [200/1000], Loss: 0.0161\n",
      "Epoch [210/1000], Loss: 0.0119\n",
      "Epoch [220/1000], Loss: 0.0470\n",
      "Epoch [230/1000], Loss: 0.0197\n",
      "Epoch [240/1000], Loss: 0.0716\n",
      "Epoch [250/1000], Loss: 0.0255\n",
      "Epoch [260/1000], Loss: 0.0408\n",
      "Epoch [270/1000], Loss: 0.0708\n",
      "Epoch [280/1000], Loss: 0.0196\n",
      "Epoch [290/1000], Loss: 0.0242\n",
      "Epoch [300/1000], Loss: 0.0323\n",
      "Epoch [310/1000], Loss: 0.0152\n",
      "Epoch [320/1000], Loss: 0.0226\n",
      "Epoch [330/1000], Loss: 0.0364\n",
      "Epoch [340/1000], Loss: 0.0390\n",
      "Epoch [350/1000], Loss: 0.0378\n",
      "Epoch [360/1000], Loss: 0.0457\n",
      "Epoch [370/1000], Loss: 0.0308\n",
      "Epoch [380/1000], Loss: 0.0351\n",
      "Epoch [390/1000], Loss: 0.0048\n",
      "Epoch [400/1000], Loss: 0.0162\n",
      "Epoch [410/1000], Loss: 0.0169\n",
      "Epoch [420/1000], Loss: 0.1259\n",
      "Epoch [430/1000], Loss: 0.0621\n",
      "Epoch [440/1000], Loss: 0.0492\n",
      "Epoch [450/1000], Loss: 0.0160\n",
      "Epoch [460/1000], Loss: 0.0173\n",
      "Epoch [470/1000], Loss: 0.0228\n",
      "Epoch [480/1000], Loss: 0.0096\n",
      "Epoch [490/1000], Loss: 0.0158\n",
      "Epoch [500/1000], Loss: 0.0285\n",
      "Epoch [510/1000], Loss: 0.0273\n",
      "Epoch [520/1000], Loss: 0.0560\n",
      "Epoch [530/1000], Loss: 0.0273\n",
      "Epoch [540/1000], Loss: 0.0451\n",
      "Epoch [550/1000], Loss: 0.0119\n",
      "Epoch [560/1000], Loss: 0.0153\n",
      "Epoch [570/1000], Loss: 0.0180\n",
      "Epoch [580/1000], Loss: 0.0325\n",
      "Epoch [590/1000], Loss: 0.0425\n",
      "Epoch [600/1000], Loss: 0.0212\n",
      "Epoch [610/1000], Loss: 0.0253\n",
      "Epoch [620/1000], Loss: 0.0190\n",
      "Epoch [630/1000], Loss: 0.0138\n",
      "Epoch [640/1000], Loss: 0.0515\n",
      "Epoch [650/1000], Loss: 0.0144\n",
      "Epoch [660/1000], Loss: 0.0241\n",
      "Epoch [670/1000], Loss: 0.0271\n",
      "Epoch [680/1000], Loss: 0.0130\n",
      "Epoch [690/1000], Loss: 0.0130\n",
      "Epoch [700/1000], Loss: 0.0068\n",
      "Epoch [710/1000], Loss: 0.0212\n",
      "Epoch [720/1000], Loss: 0.0317\n",
      "Epoch [730/1000], Loss: 0.0203\n",
      "Epoch [740/1000], Loss: 0.0184\n",
      "Epoch [750/1000], Loss: 0.0555\n",
      "Epoch [760/1000], Loss: 0.0160\n",
      "Epoch [770/1000], Loss: 0.0166\n",
      "Epoch [780/1000], Loss: 0.0363\n",
      "Epoch [790/1000], Loss: 0.0187\n",
      "Epoch [800/1000], Loss: 0.0106\n",
      "Epoch [810/1000], Loss: 0.0104\n",
      "Epoch [820/1000], Loss: 0.0167\n",
      "Epoch [830/1000], Loss: 0.0203\n",
      "Epoch [840/1000], Loss: 0.0523\n",
      "Epoch [850/1000], Loss: 0.0244\n",
      "Epoch [860/1000], Loss: 0.0207\n",
      "Epoch [870/1000], Loss: 0.0198\n",
      "Epoch [880/1000], Loss: 0.1109\n",
      "Epoch [890/1000], Loss: 0.0188\n",
      "Epoch [900/1000], Loss: 0.0104\n",
      "Epoch [910/1000], Loss: 0.0255\n",
      "Epoch [920/1000], Loss: 0.0422\n",
      "Epoch [930/1000], Loss: 0.0224\n",
      "Epoch [940/1000], Loss: 0.0211\n",
      "Epoch [950/1000], Loss: 0.0150\n",
      "Epoch [960/1000], Loss: 0.0109\n",
      "Epoch [970/1000], Loss: 0.0229\n",
      "Epoch [980/1000], Loss: 0.0702\n",
      "Epoch [990/1000], Loss: 0.0528\n",
      "Epoch [1000/1000], Loss: 0.0274\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaging proxies gives:  tensor(98.0233)\n",
      "average accruracy\n",
      "Prediction gives:  tensor([0.8495])\n",
      "actual accuracy:  0.8847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22320/4139946056.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_data = torch.tensor(features[i], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "i = int(np.random.rand()*200)\n",
    "with torch.no_grad():\n",
    "    new_data = torch.tensor(features[i], dtype=torch.float32)\n",
    "    prediction = model(features[i])\n",
    "    print(\"averaging proxies gives: \", torch.mean(features[i]))\n",
    "    print('average accruracy: ', )\n",
    "    print(\"Prediction gives: \", prediction)\n",
    "    print(\"actual accuracy: \", val_accuracies[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(76.7071)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(features[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
